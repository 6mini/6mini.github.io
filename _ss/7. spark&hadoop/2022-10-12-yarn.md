---
title: "[빅데이터 처리 입문] YARN(Yet Another Recource Negotiator)이란?"
description: 맵리듀스(MapReduce) 1의 특징과 문제점. YARN의 등장 및 특징, 아키텍쳐. 얀 컴포넌트 비교 및 각 컴포넌트의 특징. 얀의 동작 방식
categories:
 - Spark & Hadoop
tags: [YARN, 맵리듀스]
mathjax: enable
---

- 하둡(Hadoop) 2 부터 도입된 리소스 매니저(Resource Manager)의 역할을 맡고 있는 얀(YARN)에 대해 알아볼 것이다.
- 얀을 통해서 기존의 맵리듀스(MapReduce)를 처리할 수 있는 한계를 벗어나, 다양한 분산 애플리케이션을 실행하고 관리가 가능해졌다.

# 맵리듀스(MapReduce) 1

![image](https://user-images.githubusercontent.com/79494088/195232103-7b527e68-e9b5-4f53-bfd6-4a3dd68043dd.png)

- 맵리듀스 1의 아키텍쳐는 위 이미지와 같다.
- 맵리듀스 1에서는 마스터 노드(Master Node)의 역할을 잡 트래커(JobTracker)가, 워커 노드(Worker Node)는 테스크 트래커(TaskTracker)가 수행한다.
- 클라이언트(Client)가 잡(Job)을 잡 트래커에 제출하면, 자원을 할당하여 테스크 트래커가 작업을 수행할 수 있도록 해준다.
<br>그러면 테스크 트래커는 테스크를 실행하고, 작업 상황을 잡 트래커에 전송한다.

## 문제점

![image](https://user-images.githubusercontent.com/79494088/195232660-78378a7e-e9f6-45f5-a9de-a611b4f1da6b.png)

- 맵리듀스 1의 문제점으로는, 잡 트래커 역할이 과중되어있는 점을 꼽을 수 있다.
- 잡 트래커는 모든 맵리듀스 잡의 자원을 할당하는 리소스 매니징 역할과, 스케쥴링 작업을 수행하면서 각각의 작업까지 모니터링한다.
<br>그로 인해 잡 트래커의 부담이 커 클러스터를 확장하는데 문제점이 존재한다.
- 클러스터를 확장하는데 병목이 되었다. 클러스터의 규모가 4,000대만 넘어가도 잡 트래커에 부하가 걸렸다.
- 신뢰성과 가용성에 문제가 존재했다. 잡 트래커에 장애가 발생하면, 모든 잡이 정상적으로 동작하지 않았다.
- 맵리듀스로 해결할 수 없는 문제가 존재했다. 하둡에서 필요한 추가 요구사항(하둡 클러스터 이용률을 높이거나, 보안과 관련된 부분 등)이 존재했다.

# YARN(Yet Another Recource Negotiator)의 등장
- 하둡 2에서 얀은 하둡의 문제점을 개선하기 위해 나왔다. 직역하자면 또 다른 리소스 협상가라는 의미이다.
- 얀을 통해서 클러스터 규모를 10,000대 이상 사용할 수 있는 환경이 제공되었다.

## 특징
- 잡 트래커의 두 가지 중요한 부분의 책임을 분리하였다.
    - 리소스 매니저(ResourceManager)를 통해 새로운 컴포넌트를 처리하였다. 원래 맵 슬롯(Map Slot)과 리듀스 슬롯(Reduce Slot)으로 자원을 관리했는데, 리소스 매니저는 CPU, 메모리, 디스크 등으로 리소스 컨테이너라는 단위로 추상화하여 자원을 배분해준다.
    - 애플리케이션 마스터(ApplicationMaster)
- 클러스터 이용률을 개선하였다.
- 원래 맵리듀스만 실행할 수 있었지만, 이제 다른 애플리케이션도 실행이 가능하다. 다양한 프로그래밍 모델을 실행할 수 있다. 대표적으로 스파크(Spark)나 플링크(Flink)가 얀에서 실행될 수 있다.
- 또한 기존 맵리듀스와 호환성도 지원이 된다.

![image](https://user-images.githubusercontent.com/79494088/195235847-8bc6839b-b046-43b2-bcfd-5b5489dffeff.png)

- 위 이미지는 얀을 잘 표현해준다. 맵리듀스 외 다른 프로그래밍 모델을 구현 할 수 있게 발전되었다.

# 얀의 아키텍처(YARN Architecture)

![image](https://user-images.githubusercontent.com/79494088/195236561-319b1787-99c6-442e-b7f7-011bd05b61ae.png)

## 리소스 매니저(ResourceManager)
- 모든 클러스터의 자원을 중재하는 역할을 한다.
- 플러그인 가능한 스케쥴러(Scheduler)와 클러스터 사용자의 잡을 관리하는 애플리캐이션 매니저(Application Manager)라는 두 부분으로 이루어져있다.

## 노드 매니저(NodeManager)
- 하둡 클러스터(Cluster)의 각 노드들을 관리한다.
- 리소스 매니저에게 노드의 상태를 공유하고, 애플리케이션 컨테이너(Container)의 라이프 사이클을 관리 및 감독한다.
- 개별 컨테이너의 사용률을 모니터링하고, 로그 관리를 수행한다.

## 애플리케이션 마스터(ApplicationMaster)
- 리소스 매니저와 자원을 협력하여 테스크를 실행하고 모니터링한다. 리소스 매니저에게 주기적으로 하트비트(Heartbeat)를 전송해서 애플리케이션의 상태를 갱신한다.

## 컨테이너(Container)
- 하둡에서의 컨테이너는 단일 노드를 CPU, 코어, 램, 디스크 등 물리적인 리소스의 단위를 이야기한다. 하나의 노드에 여러 컨테이너가 존재할 수 있다. 이 말인 즉슨, 하나 노드는 여러 컨테이너의 구성이 될 수 있다. 노드 매니저는 이런 컨테이너들을 관리 및 감독하고 리소스 매니저는 스케쥴링하는 역할을 수행한다.

# 맵리듀스 1과 얀 컴포넌트 비교

MapReduce 1|YARN
===|===
JobTracker|ResourceManager, ApplicationMaster, TimelineServer
TaskTracker|NodeManager
Slot|Container

- 잡 트래커는 완료된 잡에 대한 이력을 저장하는 역할도 가졌는데, 얀에서는 타임라인 서버(TimelineServer)로 애플리케이션의 이력을 저장한다.


# 얀 컴포넌트의 특징

## 리소스 매니저()
- 클러스터 리소스를 중재하는 마스터 역할을 수행한다.
- 주요 컴포넌트로 스케쥴러 및 애플리케이션 매니저가 존재한다.

### 스케쥴러(Sceduler)
- 다양한 애플리케이션 자원을 할당한다. 메모리, CPU 등의 자원 요구사항에 따라 다르게 스케쥴링을 수행한다.
- 리소스 매니저 스케쥴러는 플러그인이 가능한 스케쥴러 인데, 아래의 종류 중에 선택할 수 있다.
    - FIFO: 하둡1의 잡 트래커에서 제공하던 스케쥴러이다. 작업 큐에 먼저 들어온 작업에 대해 먼저 실행하며, 잡의 우선순위 범위는 제공하지 않는다. 작은 워크로드에서는 잘 동작하지만, 대용량에서는 문제가 발생할 수 있다.
    - Capacity: 관리자가 하나 이상의 큐를 처리 용량에 맞게 미리 결정된 비율로 구성한다. 각 큐마다 처리를 위한 최소한의 양을 보장할 수 있다.
    - Fair: 모든 애플리케이션이 시간이 지남에 따라 자원을 평균적으로 균등히 분배해 자원을 애플리케이션에 할당한다.

### 애플리케이션 매니저
- 다수 애플리케이션의 유지를 책임진다.
- 애플리케이션 마스터를 실행하기 위한 첫번째 컨테이너를 설정해주는 역할을 수행한다.
- 컨테이너 설정에 실패하면 애플리케이션 마스터의 컨테이너를 재실행해주는 역할도 수행한다.

## 노드 매니저
- 하둡 클러스터의 노드를 관리한다.
- 시작할 때 리소스 매니저에 등록되고, 노드 상태를 하트비트로 보낸다.
- 노드 매니저의 목적은 리소스 매니저가 노드 매니저에 할당된 애플리케이션 컨테이너를 관리하기 위함이다. 컨테이너 할당을 확인한 후 컨테이너의 자원 사항을 모니터링한다.

## 애플리케이션 마스터
- 클러스터에 애플리케이션 실행을 조정하는 프로세스를 수행한다.
- 애플리케이션은 사용자가 제출한 단일 작업을 의미한다. 고로 각 애플리케이션마다 고유의 애플리케이션 마스터가 존재한다.
- 리소스 매니저와 자원을 협상하고 노드 매니저와 함께 동작하며, 테스크를 실행시키고 모니터링한다.
- 애플리케이션 마스터가 실행되면, 주기적으로 리소스 매니저에게 하트비트를 전송한다. 하트비트를 전송하여 형태를 확인하고 자원의 요구사항 갱신한다.

## 컨테이너
- 단일 노드의 자원을 의미한다.
- 노드 매니저는 컨테이너를 관리 및 감독하며, 리소스 매니저는 컨테이너를 스캐쥴링한다.
- 얀에서는 자원의 할당을 위해 더 큰 유연성을 제공하고자하여, 동적 할당이 가능한 컨테이너의 형태로 요청을 받아서 처리한다.


# 얀의 동작 방식

![image](https://user-images.githubusercontent.com/79494088/195240231-24960015-ee21-41a9-a9da-12570a44e698.png)

- 클라이언트로부터 각 애플리케이션에 제출이 요청되면, 리소스 매니저의 애플리케이션 매니저가 받아 최초에 애플리케이션 마스터를 위한 컨테이너 할당을 요청한다.
- 애플리케이션 마스터는 애플리케이션을 실행하기 위한 컨테이너를 스케쥴러에게 요구하고, 스케쥴러는 애플리케이션 실행을 스케쥴링 하면서 노드의 상태에 따라 컨테이너를 활성해주게 된다.
- 워커 노드에서 컨테이너를 생성하는 것은 각각의 노드 매니저가 담당한다.
- 컨테이너는 리소스를 사용해서 어플리케이션을 실행한다. 애플리케이션 마스터는 실행 상태를 모니터링하면서 컨테이너의 상태 추적하고, 실행이 완료되면 완료가 되었음을 알린다.

# 참조
[Hadoop 2.0 and YARN Architecture](https://bigdataanalyticsnews.com/hadoop-2-0-yarn-architecture/)