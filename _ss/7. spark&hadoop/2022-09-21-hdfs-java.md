---
title: "[빅데이터 처리 입문] HDFS Java API 실습"
description: 
categories:
 - Spark & Hadoop
tags: []
mathjax: enable
---

# HDFS Java API 실습

## 사전 준비
- 하둡 클러스터의 동작을 확인한다.
    - `jps`

### Maven 설치

```s
$ wget https://dlcdn.apache.org/maven/maven-3/3.8.6/binaries/apache-maven-3.8.6-bin.tar.gz
$ tar zxvf apache-maven-3.8.6-bin.tar.gz
```

- 위 코드로 설치 한 뒤 환경 변수를 등록한다.

```s
$ vim ~/.zshrc

export MAVEN_HOME=/path/to/apache-maven-3.8.6
export PATH=$PATH:$MAVEN_HOME/bin
```

## 빌드

![스크린샷 2022-09-21 11 24 14](https://user-images.githubusercontent.com/79494088/191406764-554cf7cb-2007-4530-85bb-df6cb8bdb3a2.png)

- VSC에서 `Maven for Java`를 인스톨한다.

![스크린샷 2022-09-21 11 24 55](https://user-images.githubusercontent.com/79494088/191406765-e10572b2-5f25-4bb8-931c-b9b34c562ed8.png)

- `Create Maven Project` -> `maven-archetype-quickstart` -> `1.4` 로 프로젝트를 생성한다.

![스크린샷 2022-09-21 11 30 12](https://user-images.githubusercontent.com/79494088/191406766-aacc786e-be09-4438-92b9-498e3d3e3e56.png)

- 버전을 `1.0.0`으로 입력한다.

![스크린샷 2022-09-21 11 30 37](https://user-images.githubusercontent.com/79494088/191406743-8e241f18-531f-42b5-b7a3-3e7f880df85d.png)

- 빌드가 성공하고 지정한 프로젝트 폴더로 들어가 `pom.xml`을 수정한다.

```xml
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <groupId>com.hadoop.hadoop</groupId>
    <artifactId>hdfs</artifactId>
    <version>1.0</version>

    <properties>
        <maven.compiler.source>8</maven.compiler.source>
        <maven.compiler.target>8</maven.compiler.target>
        <hadoop.version>3.3.2</hadoop.version>
    </properties>

    <dependencies>
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-client</artifactId>
            <version>${hadoop.version}</version>
        </dependency>
    </dependencies>

</project>
```

- `src/main/java/com/hadoop`에 아래와 같은 자바 파일을 생성한다.

```java
// CopyFromLocal.java
package com.hadoop.hadoop;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IOUtils;

import java.io.*;
import java.net.URI;

public class CopyFromLocal {
    public static void main(String[] args) throws IOException {
        String localSrc = args[0];
        String dst = args[1];

        InputStream in = new BufferedInputStream(new FileInputStream(localSrc));

        Configuration conf = new Configuration();
        FileSystem fs = FileSystem.get(URI.create(dst), conf);
        OutputStream out = fs.create(new Path(dst));

        IOUtils.copyBytes(in, out, 4096, true);
    }
}

//DeleteFile.java
package com.hadoop.hadoop;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;

import java.io.IOException;
import java.net.URI;

public class DeleteFile {
    public static void main(String[] args) throws IOException {
        String uri = args[0];

        Configuration conf = new Configuration();
        FileSystem fs = FileSystem.get(URI.create(uri), conf);

        Path path = new Path(uri);
        if (fs.exists(path)) {
            fs.delete(new Path(uri), false);
        }
    }
}

//FileSystemPrint.java
package com.hadoop.hadoop;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IOUtils;

import java.io.IOException;
import java.io.InputStream;
import java.net.URI;

public class FileSystemPrint {
    public static void main(String[] args) throws IOException {
        String uri = args[0];
        Configuration conf = new Configuration();
        FileSystem fs = FileSystem.get(URI.create(uri), conf);
        try (InputStream in = fs.open(new Path(uri))) {
            IOUtils.copyBytes(in, System.out, 4096, false);
        }
    }
}

// ListStatus.java
package com.hadoop.hadoop;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileStatus;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.FileUtil;
import org.apache.hadoop.fs.Path;

import java.io.IOException;
import java.net.URI;

public class ListStatus {
    public static void main(String[] args) throws IOException {
        String uri = args[0];
        Configuration conf = new Configuration();
        FileSystem fs = FileSystem.get(URI.create(uri), conf);

        Path path = new Path(uri);
        FileStatus[] status = fs.listStatus(path);
        Path[] listedPaths = FileUtil.stat2Paths(status);
        for (Path p : listedPaths) {
            System.out.println(p);
        }
    }
}
```

- `mvn package` 명령어를 이용하여 빌드를 진행한다.

## 실행

### `FileSystemPrint`
- HDFS에 존재하는 파일을 stdout으로 출력한다.

#### 파일 업로드

```s
$ hadoop fs -put /path/to/hadoop/pom.xml /user/hadoop
$ hadoop fs -ls /user/hadoop
```

```s
# hdfs://localhost:9000/와 같이 스킴 지정가능
hadoop jar target/hdfs-1.0-SNAPSHOT.jar com.hadoop.hadoop.FileSystemPrint /user/hadoop/input/LICENSE.txt
```

### `ListStatus`
- 디렉토리 파일 목록을 조회한다.

```s
hadoop jar target/hdfs-1.0-SNAPSHOT.jar com.hadoop.hadoop.ListStatus /user/hadoop/
```

### `CopyFromLocal`
- 로컬 파일을 HDFS에 복사한다

```s
hadoop jar target/hdfs-1.0-SNAPSHOT.jar com.hadoop.hadoop.CopyFromLocal ./pom.xml /user/hadoop/input/pom.xml
hadoop fs -ls /user/hadoop/input
```

### `DeleteFile`
- 파일을 삭제한다.

```s
hadoop jar target/hdfs-1.0-SNAPSHOT.jar com.hadoop.hadoop.DeleteFile /user/hadoop/input/pom.xml
hadoop fs -ls /user/hadoop/input
```