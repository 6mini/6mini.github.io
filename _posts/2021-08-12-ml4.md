---
title: '[Linear Models] 로지스틱 회귀(Logistic Regression)'
description: 훈련/검증/테스트(train/validate/test) 데이터를 분리하는 이유를 명확히 이해, 분류(Classification) 문제와 회귀문제의 차이점을 파악하고 문제에 맞는 모델 사용, 로지스틱 회귀(Logistic regression)을 이해
categories:
 - Machine Learning
tags: [Machine Learning, Linear Models, Logistic Regression, 로지스틱회귀]
mathjax: enable
# 0️⃣1️⃣2️⃣3️⃣4️⃣5️⃣6️⃣7️⃣8️⃣9️⃣🔟
---

# 1️⃣ 훈련/검증/테스트(train/validate/test)
- 캐글은 보통 데이터셋을 훈련/테스트로 나누어 제공한다.
- 테스트데이터는 특성의 개수가 하나 적다.

```py
import pandas as pd
train = pd.read_csv('https://ds-lecture-data.s3.ap-northeast-2.amazonaws.com/titanic/train.csv')
test = pd.read_csv('https://ds-lecture-data.s3.ap-northeast-2.amazonaws.com/titanic/test.csv')

print("train features: ", train.shape[1])
print("test features: ", test.shape[1])

'''
train features:  12
test features:  11
'''

# 없는 타겟 확인
print("target col: ", train.columns.difference(test.columns)[0])

# target col:  Survived
```

- 테스트 데이터에서는 타겟 정보를 제외해 놓았다.
- 이유
  - 모델의 일반화 성능을 올바르게 측정하기 위해서
- 검증셋이 필요한 이유
  - 훈련세트로 모델을 한 번에 완전하게 학습시키기 어렵기 때문이다.
  - 훈련세트로 다르게 튜닝된 여러 모델을 학습 후 어떤 모델이 학습이 잘 되었는지 검증하고 선택하는 과정이 필요하다.
- 훈련/검증세트로 좋은 모델을 만들어 낸 후 최종적으로 테스트세트에서 단 한번의 예측 테스트를 진행한다.
- 최종테스트 결과가 마음에 들지 않는다고 모델을 또 수정한다면 그 모델을 테스트세트에 과적합하여 일반화 성능이 떨어진다.

## 3개의 세트로 나누는 것은 머신러닝에서 매우 중요
- 훈련데이터 : 모델을 Fit 하는데 사용
- 검증데이터 : 예측 모델을 선택하기 위해 예측의 오류를 측정할 때 사용
- 테스트데이터 : 일반화 오류를 평가하기 위해 선택된 모델에 한하여 마지막에 한 번만 사용
  - 훈련이나 검증과정에서 사용하지 않도록 주의 
  - 테스트데이터가 유출(leak) 되어 훈련/검증과정에 사영되면 모델을 잘못 평가하게 된다.

### 모델검증

![스크린샷 2021-08-12 14 02 22](https://user-images.githubusercontent.com/79494088/129140750-8bd1c4fc-f41c-4f43-987b-ebbbb095878f.png)

- 학습 모델 개발 시, 모델 선택(Model Selection) 수행해야 한다.
- 이 때 하이퍼파라미터(HyperParameter) 튜닝을 하게 되는데 튜닝의 효과를 확인하기 위해서 검증세트가 필요하다.
  - [하이퍼파라미터](https://gooopy.tistory.com/75) : 연구자가 수정할 수 있는 값(학습률, Optimizer, 활성화 함수, 손실 함수 등 다양한 인자)
- 테스트 세트로는 하이퍼파라미터 튜닝을 절대로 하면 안된다.
- 데이터가 많은 경우, 세가지로 나누면 되지만, 데이터수가 적은 경우 K-fold 교차검증을 진행할 수 있다. (이 때도 테스트 세트는 미리 떼 놓아야 함)

### 캐글의 데이터셋 나누기
- 훈련데이터를 훈련/검증셋으로 나눈다.
- Sklearn의 train_test_split 사용

```py
from sklearn.model_selection import train_test_split
train, val = train_test_split(train, random_state=2)
print("train shape: ", train.shape)
print("val shape: ", val.shape)

'''
train shape:  (668, 12)
val shape:  (223, 12)
'''
```

# 2️⃣ 분류(Classification)문제
- 분류문제는 회귀문제와 다른 기준으로 기준모델을 설정
- 다수 클래스를 기준모델로 정하는 방법
  - 회귀문제 : 타겟 변수의 평균값
  - 분류문제 : 타겟 변수에서 가장 빈번하게 나타나는 범주
  - 시계열 : 어떤 시점을 기준으로 이전 시간의 데이터
- 분류문제에서는 타겟 변수가 편중된 범주비율을 가지는 경우가 많다.
  - 편중 된 다수 모델을 쓰지 않고 적은 모델을 사용하는 경우 착각이 일어날 수 있다.
  - 만약 다수 모델의 비율이 90%일 경우 정확도도 90%가 될 수 있는데, 그것보다 좋은 모델을 만들기 위해 노력해야 한다.

## 타겟 범주 비율 확인

```py
# 타겟을 정합니다
# survived => 0 = No, 1 = Yes
target = 'Survived'

# 타겟 데이터 범주의 비율을 확인
y_train = train[target]
y_train.value_counts(normalize=True) # 범주 비율 확인

'''
0    0.625749
1    0.374251
Name: Survived, dtype: float64
'''

import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
sns.countplot(x=y_train); # 시각화로도 확인
```

![스크린샷 2021-08-12 14 13 05](https://user-images.githubusercontent.com/79494088/129141629-7eb9e01c-4515-498f-8506-1412dbe8e830.png)

## 범주 0(majority class)으로 예측 수행

```py
# mode(): 가장 높은 벨류 값 반환
major = y_train.mode()[0]

# 타겟 샘플 수 만큼 0이 담긴 리스트를 만듭니다. 기준모델로 예측
y_pred = [major] * len(y_train)
```

## 분류의 평가지표(evaluation metrics)
- 회귀 평가지표를 분류에 사용하면 안된다. 반대도 마찬가지
- 분류문제에서는 정확도를 평가지표로 사용한다.
  - Accuracy = $\frac{올바르게 예측한 수} {전체 예측 수}$ = $\frac{TP + TN} {P + N}$

```py
from sklearn.metrics import accuracy_score
print("training accuracy: ", accuracy_score(y_train, y_pred)) # 기준 모델의 정확도 산출

# training accuracy:  0.625748502994012
# 최다 클래스의 빈도가 정확도가 된다.

y_val = val[target] 
y_pred = [major] * len(y_val)
print("validation accuracy: ", accuracy_score(y_val, y_pred)) # 검증세트에서의 정확도 확인

# validation accuracy:  0.5874439461883408
```

# 3️⃣ 로지스틱 회귀(Logistic Regression)

## 먼저 선형회귀모델 사용

```py
from sklearn.linear_model import LinearRegression

linear_model = LinearRegression()

# 숫자형 특성
features = ['Pclass', 'Age', 'Fare']
X_train = train[features]
X_val = val[features]

# Age, Cabin의 결측치를 평균 값으로 채우기
from sklearn.impute import SimpleImputer # 심플하게 결측치들을 평균값으로 채워줌

## default, imputing 'mean' value
imputer = SimpleImputer() 
X_train_imputed = imputer.fit_transform(X_train)
X_val_imputed = imputer.transform(X_val)

# 학습
linear_model.fit(X_train_imputed, y_train)

# 예측
pred = linear_model.predict(X_val_imputed)

# 회귀계수 수치 확인
pd.Series(linear_model.coef_, features)

'''
Pclass   -0.203810
Age      -0.007513
Fare      0.000819
dtype: float64
'''
# Pclass의 경우 높을 수록(2, 3등석) 생존률이 떨어짐
# Age의 경우 많을 수록 생존률이 떨어짐
# Fare의 경우는 수치가 작지만 높을 수록 생존률이 올라감
```

### 가상의 테스트 케이스

```py
test_case = [[1, 5, 600]] # 1등급의 5살 나이에 비싼요금 : 무조건 생존할 것 같다
linear_model.predict(test_case) 

# array([1.28916042]) : 1이 넘었다.
```

- 회귀모델이기 때문에 타겟변수값이 음수에서 양수까지 나타나는데 생존인지 아닌지 분명하게 결과를 알 수 없다.
- 게다가 회귀이기 때문에 분류모델에 사용하는 평가지표를 사용할 수 없다.

## 로지스틱 회귀모델
- 로지스틱회귀를 사용하면 타겟변수의 범주로 0과 1을 사용할 수 있으며 각 범주의 예측 확률값을 얻을 수 있다.
- 로지스틱 회귀모델의 식

{% raw %} $$\large P(X)={\frac {1}{1+e^{-(\beta _{0}+\beta _{1}X_{1}+\cdots +\beta _{p}X_{p})}}}$$
$$ 0 \leq P(X) \leq 1$$ {% endraw %}

- 로지스틱 회귀는 로지스틱 함수, 시그모이드 함수 형태로 표현

![스크린샷 2021-08-12 14 27 15](https://user-images.githubusercontent.com/79494088/129142837-26e4ce15-9827-4713-a4e4-95d57f6b2f19.png)

- 기준값은 0.5
- 결과적으로 관측치가 특정 글래스에 속할 확률값으로 계산된다.
- 분류문제에서는 확률값을 사용하여 분류하는데, 확률값이 정해진 기준값 보다 크면 1 아니면 0이라고 예측한다.

### Logit transformation
- 로지스틱회귀의 계수는 비선형 함수 내에 있어서 직관적으로 해석하기 어려운데, 오즈(Odds)를 사용하면 선형결합 형태로 변환 가능해 보다 쉽게 해석이 가능하다. 
- 오즈는 실패확률에 대한 성공확률의 비인데 예를들어 오즈값이 4면, 성공확률이 실패확률의 4배라는 뜻이다.
- 분류문제에서는 클래스 1 확률에 대한 클래스 0 확률의 비라고 해석하면 된다. 
  - $Odds = \large \frac{p}{1-p}$, 
  - p = 성공확률, 1-p = 실패확률
  - p = 1 일때 odds = $\infty$
  - p = 0 일때 odds = 0

  {% raw %} $$\large ln(Odds) = ln(\frac{p}{1-p}) = ln(\frac{\frac {1}{1+e^{-(\beta _{0}+\beta _{1}X_{1}+\cdots +\beta _{p}X_{p})}}}{1 - \frac {1}{1+e^{-(\beta _{0}+\beta _{1}X_{1}+\cdots +\beta _{p}X_{p})}}}) = \normalsize \beta _{0}+\beta _{1}X_{1}+\cdots +\beta _{p}X_{p}$$ {% endraw %}

- 로짓변환(Logit transformation) : 오즈에 로그를 취해 변환하는 것
- 로짓변환을 통해 비선형형태인 로지스틱함수형태를 선형형태로 만들어 회귀계수의 의미를 해석하기 쉽게 하는데, 특성 X의 증가에 따라 로짓(ln(oddx))가 얼마나 증가o감소 했다고 해석할 수 있다.<br>(odds 확률로 해석을 하려면 exp(계수) = p 를 계산해서 특성 1단위 증가당 확률이 p배 증가한다고 해석을 할 수 있다.)
- 기존 로지스틱형태의 y 값은 0~1의 범위를 가졌다면 로짓은 - ∞  ~  ∞  범위를 가진다.

![스크린샷 2021-08-12 14 37 35](https://user-images.githubusercontent.com/79494088/129143684-d9045577-bc51-404b-9c38-a2ef8346e027.png)

# 4️⃣ 로지스틱회귀 vs 선형회귀

```py
from sklearn.linear_model import LogisticRegression

logistic = LogisticRegression()
logistic.fit(X_train_imputed, y_train)

print('검증세트 정확도', logistic.score(X_val_imputed, y_val)) # 분류 정확도 리턴

# 검증세트 정확도 0.7130044843049327
# 기준모델보다 정확도가 높게 나왔다.

pred = logistic.predict(X_val_imputed) # 예측 결과 확인
pred

'''
array([1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
       0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
       1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
       0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
       0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
       0, 1, 0])
'''

logistic.predict(test_case) # 위에서 만든 테스트 케이스 사용

logistic.predict_proba(test_case) # 클래스에 속할 확률값 확인

# array([[0.01749669, 0.98250331]])

print(features)
print(logistic.coef_) # 로지스틱회귀의 계수 확인

'''
['Pclass', 'Age', 'Fare']
[[-0.90248227 -0.03581619  0.00447486]]
'''

# 선형회귀분석과 비교했을 때 회귀계수 수치는 변했지만 방향은 같음을 볼 수 있다.

```

## 타이타닉 데이터 사용 모델
- 모델에 적합하기 전 데이터 변환 수행
  - OneHotEncoder
  - SimpleImputer
  - StandardScaler

```py
train.columns

'''
Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',
       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],
      dtype='object')
'''

train['Ticket'].value_counts() # 사용가능한 범주 확인

'''
347088              6
S.O.C. 14879        5
CA. 2343            5
382652              5
3101295             5
                   ..
STON/O2. 3101271    1
2693                1
345572              1
SCO/W 1585          1
315086              1
Name: Ticket, Length: 539, dtype: int64
'''
```

- 사용 가능한 모든 변수를 선택 : `['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']`
- `PassengerId`, `Name`, `Cabin`, `Ticket`를 사용하지 않는 이유
  - 아이디나 이름 같은 특성을 가진 것들은 샘플별로 모두 다르기 때문에 일반화를 하기위한 것에 도움이 되지 않음
  - 캐빈은 범주의 종류 너무 많고 결측치가 많다.
  - 티켓도 범주 종류가 너무 많다.
- 사용하고자하는 특성에 대해 vaue_counts로 범주를 확인해야 한다.

```py
from category_encoders import OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler

features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']
target = 'Survived'

X_train = train[features]
y_train = train[target]

X_val = val[features]
y_val = val[target]


# 원핫인코딩
encoder = OneHotEncoder(use_cat_names=True)
X_train_encoded = encoder.fit_transform(X_train) 
X_val_encoded = encoder.transform(X_val) # fit 안하는 이유 : 검증셋에서 어떠한 범주가 부족하면 문제가 생길 수 있다.


# 결측치 평균으로 변환
imputer = SimpleImputer(strategy='mean')
X_train_imputed = imputer.fit_transform(X_train_encoded)
X_val_imputed = imputer.transform(X_val_encoded)


# 특성값들을 표준화
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_imputed)
X_val_scaled = scaler.transform(X_val_imputed)


X_train_scaled.T[0].mean(), X_train_scaled.T[0].std()
# 평균은 0, 표준편차는 1로 표준화


model = LogisticRegression(random_state=1)
model.fit(X_train_scaled, y_train) # LogisticRegression(random_state=1)


# 정확도 확인
y_pred = model.predict(X_val_scaled)
accuracy_score(y_val, y_pred) # 0.7892376681614349


# 계수 확인
coefficients = pd.Series(model.coef_[0], X_train_encoded.columns)
coefficients
'''
Pclass         -0.915833
Sex_female      0.662095
Sex_male       -0.662095
Age            -0.559957
SibSp          -0.406466
Parch          -0.015897
Fare            0.078016
Embarked_S     -0.094939
Embarked_Q      0.007684
Embarked_C      0.077224
Embarked_nan    0.188837
dtype: float64
'''

# 시각화
coefficients.sort_values().plot.barh();
```

![스크린샷 2021-08-12 14 51 50](https://user-images.githubusercontent.com/79494088/129144978-e0ffe0e3-e956-4542-b98b-932711ca2475.png)

```py
model.intercept_ # array([-0.71320882])

# 절편(intercept)은 마이너스로 모든 특성이 0인 경우 생존하지 못할 가능성이 높다고 알려주긴 하지만 
# 사실 관측할 수 없는 예시로 해석이 크게 유용하지 않다.
```

### 모델을 테스트세트에 적용 후 캐글에 제출

```py
X_test = test[features]
X_test_encoded = encoder.transform(X_test)
X_test_imputed = imputer.transform(X_test_encoded)
X_test_scaled = scaler.transform(X_test_imputed)

y_pred_test = model.predict(X_test_scaled)


submission = test[['PassengerId']].copy()
submission['Survived'] = y_pred_test
submission
```

![스크린샷 2021-08-12 14 54 37](https://user-images.githubusercontent.com/79494088/129145231-53659c4b-b08e-4d13-b846-77128d4e7a3d.png)

```py
submission.to_csv('submission_titanic.csv', index=False)
```