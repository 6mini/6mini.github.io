---
title: '[Applied Predictive Modeling] Feature Importances'
description: íŠ¹ì„± ì¤‘ìš”ë„ ê³„ì‚° ë°©ë²•ë“¤(Permutation importances, Feature importance, ...)ì˜ ì´í•´ì™€ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ í•´ì„ ë° íŠ¹ì„± ì„ íƒ í™œìš©, Gradient Boosting ì´í•´ ë° xgboost ì´ìš© ëª¨ë¸ ìƒì„±
categories:
 - Machine Learning
tags: [Machine Learning, Applied Predictive Modeling, Feature Importances, Permutation Importances, Gradient boosting, xgboost, catboost, íŠ¹ì„± ì¤‘ìš”ë„]
mathjax: enable
# 0ï¸âƒ£1ï¸âƒ£2ï¸âƒ£3ï¸âƒ£4ï¸âƒ£5ï¸âƒ£6ï¸âƒ£7ï¸âƒ£8ï¸âƒ£9ï¸âƒ£ğŸ”Ÿ
---

# 1ï¸âƒ£ Prepare Data
- Import H1N1 Data

```py
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

# íƒ€ê²Ÿìƒì„±
target = 'vacc_seas_f'
# í•™ìŠµë°ì´í„° features, labels ë¥¼ ë³‘í•©
train = pd.merge(pd.read_csv('https://ds-lecture-data.s3.ap-northeast-2.amazonaws.com/vacc_flu/train.csv'), 
                 pd.read_csv('https://ds-lecture-data.s3.ap-northeast-2.amazonaws.com/vacc_flu/train_labels.csv')[target], left_index=True, right_index=True)
test = pd.read_csv('https://ds-lecture-data.s3.ap-northeast-2.amazonaws.com/vacc_flu/test.csv')



# 80/20 ë¹„ìœ¨ í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ë¦¬
train, val = train_test_split(train, train_size=0.80, test_size=0.20, 
                              stratify=train[target], random_state=2)

def engineer(df):
    """íŠ¹ì„±ì„ ì—”ì§€ë‹ˆì–´ë§ í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤."""
    
    # ìƒˆë¡œìš´ íŠ¹ì„± ìƒì„±
    behaviorals = [col for col in df.columns if 'behavioral' in col] 
    df['behaviorals'] = df[behaviorals].sum(axis=1)
    
    dels = [col for col in df.columns if ('employment' in col or 'h1n1' in col)]
    df.drop(columns=dels, inplace=True)
        
    return df

train = engineer(train)
val = engineer(val)
test = engineer(test)


X_train = train.drop(columns=target)
y_train = train[target]
X_val = val.drop(columns=target)
y_val = val[target]
X_test = test


from category_encoders import OrdinalEncoder
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.pipeline import make_pipeline

pipe = make_pipeline(
    OrdinalEncoder(),
    SimpleImputer(), 
    RandomForestClassifier(n_estimators=100, random_state=2, n_jobs=-1)
)


pipe
'''
Pipeline(steps=[('ordinalencoder', OrdinalEncoder()),
                ('simpleimputer', SimpleImputer()),
                ('randomforestclassifier',
                 RandomForestClassifier(n_jobs=-1, random_state=2))])
'''


from sklearn.metrics import classification_report
# train í•™ìŠµ, ê²€ì¦ì…‹ ì •í™•ë„
pipe.fit(X_train, y_train)
print('ê²€ì¦ ì •í™•ë„', pipe.score(X_val, y_val))

print(classification_report(y_val, pipe.predict(X_val)))
'''
ê²€ì¦ ì •í™•ë„ 0.7526983750444787
              precision    recall  f1-score   support

           0       0.76      0.80      0.78      4608
           1       0.74      0.70      0.72      3823

    accuracy                           0.75      8431
   macro avg       0.75      0.75      0.75      8431
weighted avg       0.75      0.75      0.75      8431

'''
```


# 2ï¸âƒ£ Permutation Importances(ìˆœì—´ ì¤‘ìš”ë„)
- ëª¨ë¸ í•´ì„ê³¼ Feature selectionì„ ìœ„í•´ Permutation Importances(ìˆœì—´ì¤‘ìš”ë„)ë¥¼ ê³„ì‚°í•œë‹¤.
- ê¸°ë³¸ Feature importanceëŠ” ë¹ ë¥´ì§€ë§Œ íŠ¹ì„± ì¢…ë¥˜ì— ë”°ë¼ ë¶€ì •í™•í•œ ê²°ê³¼ê°€ ë‚˜ì˜¬ ìˆ˜ ìˆì–´ ì£¼ì˜ê°€ í•„ìš”í•˜ë‹¤.
- **Permutation importanceë¥¼ ì‚¬ìš©í•˜ë©´ ë”ìš± ì •í™•í•œ ê³„ì‚°ì´ ê°€ëŠ¥**í•˜ë‹¤.

## íŠ¹ì„± ì¤‘ìš”ë„ ê³„ì‚° ë°©ë²• 3ê°€ì§€

### Feature Importances(Mean decrease impurity, MDI)
- sklearn tree ê¸°ë°˜ ë¶„ë¥˜ê¸°ì—ì„œ defaultë¡œ ì‚¬ìš©ë˜ëŠ” Feature importanceëŠ” ì†ë„ëŠ” ë¹ ë¥´ì§€ë§Œ ê²°ê³¼ë¥¼ ì£¼ì˜í•´ì•¼ í•œë‹¤.
- ê° íŠ¹ì„±ì„ ëª¨ë“  íŠ¸ë¦¬ì— ëŒ€í•´ í‰ê· ë¶ˆìˆœë„ê°ì†Œ(Mean Decrease Impurity)ë¥¼ ê³„ì‚°í•œ ê°’ì´ë‹¤.
  - ìƒìœ„ë…¸ë“œì—ì„œ í•˜ìœ„ë…¸ë“œì˜ ê°€ì¤‘í‰ê·  ë¶ˆìˆœë„ ì°¨ë¥¼ ê³„ì‚°í•œ ê²ƒì´ë‹¤.
- ë¶ˆìˆœë„ê°ì†Œ(Impurity Decrease)
  - $\displaystyle \frac{N_t}{N}$ * (impurity - $\displaystyle\frac{N_{tR}}{N_t}$ * right_impurity - $\displaystyle\frac{N_{tL}}{N_t}$ * left_impurity)
  - $N$: ì „ì²´ ê´€ì¸¡ì¹˜ ìˆ˜, $N_t$: í˜„ì¬ ë…¸ë“œ tì— ì¡´ì¬í•˜ëŠ” ê´€ì¸¡ì¹˜ ìˆ˜
  - $N_{tL}$, $N_{tR}$: ë…¸ë“œ t ì™¼ìª½(L)/ì˜¤ë¥¸ìª½(R) ìì‹ë…¸ë“œì— ì¡´ì¬í•˜ëŠ” ê´€ì¸¡ì¹˜ ìˆ˜
  - ë§Œì•½ `sample_weight`ê°€ ì£¼ì–´ì§„ë‹¤ë©´, $N$, $N_t$, $N_{tR}$, $N_{tL}$ëŠ” ê°€ì¤‘í•©
  - Warning : impurity-based feature importances can be misleading for High cardinality features (many unique values).
  - **ë†’ì€ Cadinalityì— ëŒ€í•´ Groupì´ í¸í–¥ë˜ì–´ ê³¼ì í•©ì„ ì¼ìœ¼í‚¤ê³  ë¶ˆìˆœë„ê°€ ë†’ê²Œ ë‚˜ì˜¤ëŠ” ì˜¤ë¥˜ê°€ ë‚˜ì„œ ì˜ëª»ëœ í•´ì„ì„ í•˜ê¸° ì‰½ë‹¤.**

```py
# íŠ¹ì„± ì¤‘ìš”ë„
rf = pipe.named_steps['randomforestclassifier']
importances = pd.Series(rf.feature_importances_, X_train.columns)

%matplotlib inline
import matplotlib.pyplot as plt

n = 20
plt.figure(figsize=(10,n/2))
plt.title(f'Top {n} features')
importances.sort_values()[-n:].plot.barh();
```

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-26 10 30 10](https://user-images.githubusercontent.com/79494088/130885453-21bf31ee-c8ae-448f-84b9-b23423dffa34.png)

- ë‹¤ë¥¸ íŠ¹ì„±ì— ë¹„í•´ ë¹„êµì  High cardinalityì¸ 51ê°œì˜ Categoryë¡œ êµ¬ì„± ëœ `state`ëŠ” ìœ ì˜í•´ì„œ ë´ì•¼í•œë‹¤.
- Tree êµ¬ì„± ì¤‘ ë¶„ê¸°ì— ì´ìš©ë  í™•ë¥ ì´ ë†’ì•„ ê³¼ì í•© ìœ„í—˜ì´ ìˆë‹¤.

```py
len(X_train['state'].value_counts())
'''
51
'''
```

### Drop-Column Importance
- ì´ë¡ ì ìœ¼ë¡œ ê°€ì¥ ì¢‹ì•„ ë³´ì´ì§€ë§Œ, ë§¤ íŠ¹ì„±ì„ dropí•œ í›„ fitì„ ë‹¤ì‹œ í•´ì•¼í•˜ê¸° ë•Œë¬¸ì— ëŠë¦¬ë‹¤ëŠ” ë‹¨ì ì´ ìˆë‹¤.
- íŠ¹ì„±ì´ nê°œ ì¡´ì¬í•  ë•Œ n + 1ë²ˆ í•™ìŠµì´ í•„ìš”í•˜ë‹¤.

```py
column  = 'opinion_seas_risk'

# opinion_h1n1_risk ì—†ì´ fit
pipe = make_pipeline(
    OrdinalEncoder(), 
    SimpleImputer(), 
    RandomForestClassifier(n_estimators=100, random_state=2, n_jobs=-1)
)
pipe.fit(X_train.drop(columns=column), y_train)
score_without = pipe.score(X_val.drop(columns=column), y_val)
print(f'ê²€ì¦ ì •í™•ë„ ({column} ì œì™¸): {score_without}')

# opinion_h1n1_risk í¬í•¨ í›„ ë‹¤ì‹œ í•™ìŠµ
pipe = make_pipeline(
    OrdinalEncoder(), 
    SimpleImputer(), 
    RandomForestClassifier(n_estimators=100, random_state=2, n_jobs=-1)
)
pipe.fit(X_train, y_train)
score_with = pipe.score(X_val, y_val)
print(f'ê²€ì¦ ì •í™•ë„ ({column} í¬í•¨): {score_with}')

# opinion_h1n1_risk í¬í•¨ ì „ í›„ ì •í™•ë„ ì°¨ì´ ê³„ì‚°
print(f'{column}ì˜ Drop-Column ì¤‘ìš”ë„: {score_with - score_without}')
'''
ê²€ì¦ ì •í™•ë„ (opinion_seas_risk ì œì™¸): 0.733127742853754
ê²€ì¦ ì •í™•ë„ (opinion_seas_risk í¬í•¨): 0.7526983750444787
opinion_seas_riskì˜ Drop-Column ì¤‘ìš”ë„: 0.019570632190724635
'''
```

### ìˆœì—´ì¤‘ìš”ë„(Permutation Importance, Mean Decrease Accuracy, MDA)
- Permutation ImportanceëŠ” ê¸°ë³¸ Feature Importanceì™€ Drop-column Importanceì˜ ì¤‘ê°„ì— ìœ„ì¹˜í•˜ëŠ” íŠ¹ì§•ì´ ìˆë‹¤.
- Importance ì¸¡ì •ì€ **ê´€ì‹¬ìˆëŠ” íŠ¹ì„±ì—ë§Œ ë¬´ì‘ìœ„ë¡œ Noiseë¥¼ ì£¼ê³  ì˜ˆì¸¡í–ˆì„ ë•Œ Confusion Matrix(Acc, F1, $R^2$ ë“±)ê°€ ì–¼ë§ˆë‚˜ ê°ì†Œí•˜ëŠ”ì§€ ì¸¡ì •**í•œë‹¤.
- Drop-column importanceë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•´ Retrainingì„ í•´ì•¼í–ˆë‹¤ë©´, Permutation improtanceëŠ” Val dataì—ì„œ ê° íŠ¹ì„±ì„ ì œê±°í•˜ì§€ ì•Šê³  íŠ¹ì„±ê°’ì— ë¬´ì‘ìœ„ë¡œ Noiseë¥¼ ì£¼ì–´ ê¸°ì¡´ ì •ë³´ë¥¼ ì œê±°í•˜ì—¬ íŠ¹ì„±ì´ ê¸°ì¡´ì— í•˜ë˜ ì—­í• ì„ í•˜ì§€ ëª»í•˜ê²Œ í•˜ê³  ì„±ëŠ¥ì„ ì¸¡ì •í•œë‹¤.
- ì´ë•Œ Noiseë¥¼ ì£¼ëŠ” ê°„ë‹¨í•œ ë°©ë²•ì´ ê·¸ íŠ¹ì„±ê°’ë“¤ì„ Sample ë‚´ì—ì„œ Shuffle or Permutation í•˜ëŠ” ê²ƒì´ë‹¤.
- ì£¼ë¡œ ì“°ê²Œ ëœë‹¤.

```py
# eli5 ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© ìˆœì—´ ì¤‘ìš”ë„ ê³„ì‚°
from sklearn.pipeline import Pipeline
# encoder, imputerë¥¼ preprocessingìœ¼ë¡œ ë¬¶ê³  eli5 permutation ê³„ì‚°ì— ì‚¬ìš©
pipe = Pipeline([
    ('preprocessing', make_pipeline(OrdinalEncoder(), SimpleImputer())),
    ('rf', RandomForestClassifier(n_estimators=100, random_state=2, n_jobs=-1)) 
])


# pipeline ìƒì„± í™•ì¸
pipe.named_steps
'''
{'preprocessing': Pipeline(steps=[('ordinalencoder', OrdinalEncoder()),
                 ('simpleimputer', SimpleImputer())]),
 'rf': RandomForestClassifier(n_jobs=-1, random_state=2)}
'''


pipe.fit(X_train, y_train)
print('ê²€ì¦ ì •í™•ë„: ', pipe.score(X_val, y_val))
'''
ê²€ì¦ ì •í™•ë„:  0.7526983750444787
'''


import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

import eli5
from eli5.sklearn import PermutationImportance

# permuter ì •ì˜
permuter = PermutationImportance(
    pipe.named_steps['rf'], # model
    scoring='accuracy', # metric
    n_iter=5, # ë‹¤ë¥¸ random seedë¥¼ ì‚¬ìš©í•˜ì—¬ 5ë²ˆ ë°˜ë³µ
    random_state=2
)

# permuter ê³„ì‚°ì€ preprocessing ëœ X_val ì‚¬ìš©
X_val_transformed = pipe.named_steps['preprocessing'].transform(X_val)

# ì‹¤ì œë¡œ fit ì˜ë¯¸ë³´ë‹¤ ìŠ¤ì½”ì–´ ë‹¤ì‹œ ê³„ì‚°í•˜ëŠ” ì‘ì—…
permuter.fit(X_val_transformed, y_val);


feature_names = X_val.columns.tolist()
pd.Series(permuter.feature_importances_, feature_names).sort_values()
'''
n_adult_r                     -0.003511
hhs_region                    -0.003108
census_region                 -0.003084
behavioral_face_mask          -0.003060
sex_i                         -0.002942
state                         -0.002918
behavioral_wash_hands         -0.002657
n_people_r                    -0.002562
behavioral_large_gatherings   -0.002538
behavioral_antiviral_meds     -0.002420
behavioral_avoidance          -0.002159
behavioral_outside_home       -0.002064
behavioral_touch_face         -0.001755
chronic_med_condition         -0.001613
census_msa                    -0.001542
behaviorals                   -0.001471
child_under_6_months          -0.001400
marital                       -0.001328
rent_own_r                    -0.000712
inc_pov                       -0.000474
raceeth4_i                    -0.000213
household_children            -0.000119
education_comp                 0.001234
health_insurance               0.002775
health_worker                  0.003060
opinion_seas_sick_from_vacc    0.004792
agegrp                         0.007733
opinion_seas_risk              0.041039
opinion_seas_vacc_effective    0.043814
doctor_recc_seasonal           0.071427
dtype: float64
'''


# íŠ¹ì„±ë³„ score í™•ì¸
eli5.show_weights(
    permuter, 
    top=None, # top n ì§€ì • ê°€ëŠ¥, None ì¼ ê²½ìš° ëª¨ë“  íŠ¹ì„± 
    feature_names=feature_names # list í˜•ì‹ìœ¼ë¡œ ë„£ì–´ì•¼ í•©ë‹ˆë‹¤
)
```

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-26 10 46 57](https://user-images.githubusercontent.com/79494088/130886833-8ed7a86e-f862-4871-88d5-f55543824801.png)

#### Feature Selection
- Improtanceë¥¼ ì´ìš©í•˜ì—¬ Feature selection í•œë‹¤.
- Importanceê°€ ìŒìˆ˜ì¸ íŠ¹ì„±ì€ ì œì™¸í•´ë„ ì„±ëŠ¥ì— ì˜í–¥ì´ ì—†ìœ¼ë©° ëª¨ë¸í•™ìŠµ ì†ë„ê°€ ê°œì„ ëœë‹¤.

```py
print('íŠ¹ì„± ì‚­ì œ ì „:', X_train.shape, X_val.shape)
'''
íŠ¹ì„± ì‚­ì œ ì „: (33723, 30) (8431, 30)
'''


minimum_importance = 0.001
mask = permuter.feature_importances_ > minimum_importance
features = X_train.columns[mask]
X_train_selected = X_train[features]
X_val_selected = X_val[features]
print('íŠ¹ì„± ì‚­ì œ í›„:', X_train_selected.shape, X_val_selected.shape)
'''
íŠ¹ì„± ì‚­ì œ í›„: (33723, 8) (8431, 8)
'''


# pipeline ë‹¤ì‹œ ì •ì˜
pipe = Pipeline([
    ('preprocessing', make_pipeline(OrdinalEncoder(), SimpleImputer())),
    ('rf', RandomForestClassifier(n_estimators=100, random_state=2, n_jobs=-1)) 
], verbose=1)

pipe.fit(X_train_selected, y_train);
'''
[Pipeline] ..... (step 1 of 2) Processing preprocessing, total=   0.1s
[Pipeline] ................ (step 2 of 2) Processing rf, total=   0.4s
'''


print('ê²€ì¦ ì •í™•ë„: ', pipe.score(X_val_selected, y_val))
'''
ê²€ì¦ ì •í™•ë„:  0.7513936662317637
'''


# ìˆœì—´ ì¤‘ìš”ë„ì˜ í‰ê·  ê°ì†Œê°’ê³¼ ê·¸ í‘œì¤€í¸ì°¨ì˜ ì°¨ê°€ ì–‘ìˆ˜ì¸ íŠ¹ì§• í™•ì¸
permuter.feature_importances_ - permuter.feature_importances_std_ > 0
'''
array([False, False, False, False, False, False, False,  True, False,
       False,  True,  True,  True,  True,  True,  True, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False])
'''
```

# 3ï¸âƒ£ Boosting(xgboost for gradient boosting)
- Classifier ë¬¸ì œì—ì„œ Tree Ensemble Modelì„ ë§ì´ ì‚¬ìš©í•œë‹¤.
- Tree Ensemble Modeldì€ RandomForestë‚˜ Gradiant Boosting Modelì„ ì´ì•¼ê¸° í•˜ë©° ì—¬ëŸ¬ ë¬¸ì œì—ì„œ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤.
- Tree Modelì€ non-linear, non-monotonic ê´€ê³„, íŠ¹ì„±ê°„ ìƒí˜¸ì‘ìš©ì´ ì¡´ì¬í•˜ëŠ” ë°ì´í„° í•™ìŠµì— ì ìš©í•˜ê¸° ì¢‹ë‹¤.
- í•œ Treeë¥¼ ê¹Šê²Œ í•™ìŠµì‹œí‚¤ë©´ ê³¼ì í•©ì„ ì¼ìœ¼í‚¤ê¸° ì‰½ê¸° ë•Œë¬¸ì— Bagging(RandomForest)ë‚˜ Boosting Ensemble Modelì„ ì‚¬ìš©í•´ ê³¼ì í•©ì„ í”¼í•œë‹¤.
- RandomForestì˜ ì¥ì ì€ Hyperparameterì— ìƒëŒ€ì ìœ¼ë¡œ ëœ ë¯¼ê°í•œ ê²ƒì¸ë°, Gradiant Boostingì˜ ê²½ìš° Hyperparameterì˜ settingì— ë”°ë¼ RandomForestë³´ë‹¤ ì¢‹ì€ ì˜ˆì¸¡ ì„±ëŠ¥ì„ ë³´ì—¬ì¤€ë‹¤.

## Boostingê³¼ Baggingì˜ ì°¨ì´ì 
- ê°€ì¥ í° ì°¨ì´ëŠ” RandomForestì˜ ê²½ìš° ê° Treeë¥¼ ë…ë¦½ì ìœ¼ë¡œ ë§Œë“¤ì§€ë§Œ Boostingì€ ë§Œë“¤ì–´ì§€ëŠ” Treeê°€ ì´ì „ì— ë§Œë“¤ì–´ì§„ Treeì— ì˜í–¥ì„ ë°›ëŠ”ë‹¤ëŠ” ê²ƒì´ë‹¤.
- **AdaBoostëŠ” ê° Tree(Weak learners)ê°€ ë§Œë“¤ì–´ì§ˆ ë•Œ ì˜ëª» ë¶„ë¥˜ë˜ëŠ” ê´€ì¸¡ì¹˜ì— Weightì„ ì¤€ë‹¤.**
- ë‹¤ìŒ Treeê°€ ë§Œë“¤ì–´ì§ˆ ë•Œ ì´ì „ì— ì˜ëª» ë¶„ë¥˜ëœ ê´€ì¸¡ì¹˜ê°€ ë” ë§ì´ Samplingë˜ê²Œ í•˜ì—¬ ê·¸ ê´€ì¸¡ì¹˜ë¥¼ ë¶„ë¥˜í•˜ëŠ”ë° ë” ì´ˆì ì„ ë§ì¶˜ë‹¤.

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-26 13 27 24](https://user-images.githubusercontent.com/79494088/130900557-ddab475d-d933-4ff8-a968-cc81d88dc990.png)

- AdaBoost Algorithm
  1. ëª¨ë“  ê´€ì¸¡ì¹˜ì˜ Weightì„ ë™ì¼í•˜ê²Œ Settingí•œë‹¤.
  2. ê´€ì¸¡ì¹˜ ë³µì›ì¶”ì¶œí•˜ì—¬ Leak learners $Dn$ì„ í•™ìŠµí•˜ê³  +,- ë¶„ë¥˜í•œë‹¤.
  3. ì˜ëª» ë¶„ë¥˜ëœ ê´€ì¸¡ì¹˜ì— Weightì„ ë¶€ì—¬í•´ ë‹¤ìŒ ê³¼ì •ì—ì„œ Samplingì´ ì˜ë˜ê²Œ í•œë‹¤.
  4. 2-3 ê³¼ì •ì„ $n$íšŒ ë°˜ë³µí•œë‹¤.
  5. ë¶„ë¥˜ê¸°($D1, D2, D3$)ë¥¼ ê²°í•©í•˜ì—¬ ìµœì¢… ì˜ˆì¸¡ì„ ìˆ˜í–‰í•œë‹¤.

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-26 13 27 52](https://user-images.githubusercontent.com/79494088/130900605-d102c6fb-6ce8-484d-b16f-e13ed5a6d95c.png)

- Final learner **H(x)** ëŠ” **Weak learners**($h_t$)ì˜ **ê°€ì¤‘($\alpha$)í•©**ìœ¼ë¡œ ë§Œë“¤ì–´ì§„ë‹¤.
  - ì—¬ê¸°ì„œ $\alpha_t$ ê°€ í¬ë©´ $e_t$ê°€ ì‘ë‹¤ëŠ” ê²ƒìœ¼ë¡œ ë¶„ë¥˜ê¸° $h_t$ ì„±ëŠ¥ì´ ì¢‹ë‹¤ëŠ” ê²ƒ
  - ì—¬ê¸°ì„œ $\alpha_t$ ê°€ ì‘ìœ¼ë©´ $e_t$ê°€ í¬ë‹¤ëŠ” ê²ƒìœ¼ë¡œ ë¶„ë¥˜ê¸° $h_t$ ì„±ëŠ¥ì´ ì•ˆì¢‹ë‹¤ëŠ” ëœ»

## Gradiant Boosting
- Regressor, Classifier ëª¨ë‘ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-26 13 32 26](https://user-images.githubusercontent.com/79494088/130901041-4fac166e-4b55-427a-8910-2cbca4d065ac.png)

- GradiantëŠ” Adaì™€ ìœ ì‚¬í•˜ì§€ë§Œ Loss functionì„ ìµœì†Œí™” í•˜ëŠ” ë°©ë²•ì—ì„œ ì°¨ì´ê°€ ë‚œë‹¤.
- Gradiantì—ì„œëŠ” Sampleì˜ Weightì„ ì¡°ì •í•˜ëŠ” ëŒ€ì‹  **Residual(ì”ì°¨)ì„ í•™ìŠµ**í•œë‹¤.
- **ì”ì°¨ê°€ ë” í° ë°ì´í„°ë¥¼ í•™ìŠµí•˜ëŠ” íš¨ê³¼**ê°€ ìˆë‹¤.

### Python libraries for Gradient Boosting
- [scikit-learn Gradient Tree Boosting](https://scikit-learn.org/stable/modules/ensemble.html#gradient-boosting) â€” ìƒëŒ€ì ìœ¼ë¡œ ì†ë„ê°€ ëŠë¦¬ë‹¤.
  - Anaconda: already installed
  - Google Colab: already installed
- [xgboost](https://xgboost.readthedocs.io/en/latest/) â€”Â ê²°ì¸¡ê°’ì„ ìˆ˜ìš©í•˜ë©°, [monotonic constraints](https://xiaoxiaowang87.github.io/monotonicity_constraint/)ë¥¼ ê°•ì œí•œë‹¤.
  - Anaconda, Mac/Linux: `conda install -c conda-forge xgboost`
  - Windows: `conda install -c anaconda py-xgboost`
  - Google Colab: already installed
- [LightGBM](https://lightgbm.readthedocs.io/en/latest/) â€”Â ê²°ì¸¡ê°’ì„ ìˆ˜ìš©í•˜ë©°, [monotonic constraints](https://blog.datadive.net/monotonicity-constraints-in-machine-learning/)ë¥¼ ê°•ì œí•œë‹¤.
  - Anaconda: `conda install -c conda-forge lightgbm`
  - Google Colab: already installed
- [CatBoost](https://catboost.ai/) â€”Â ê²°ì¸¡ê°’ì„ ìˆ˜ìš©í•˜ë©°, [categorical features](https://catboost.ai/docs/concepts/algorithm-main-stages_cat-to-numberic.html)ë¥¼ ì „ì²˜ë¦¬ ì—†ì´ ì‚¬ìš©í•œë‹¤.
  - Anaconda: `conda install -c conda-forge catboost`
  - Google Colab: `pip install catboost`

### XGBoost
```py
from xgboost import XGBClassifier

pipe = make_pipeline(
    OrdinalEncoder(),
    SimpleImputer(strategy='median'),
    XGBClassifier(n_estimators=200
                  , random_state=2
                  , n_jobs=-1
                  , max_depth=7
                  , learning_rate=0.2
                 )
)

pipe.fit(X_train, y_train);

from sklearn.metrics import accuracy_score
y_pred = pipe.predict(X_val)
print('ê²€ì¦ ì •í™•ë„: ', accuracy_score(y_val, y_pred))

print(classification_report(y_pred, y_val))
'''
ê²€ì¦ ì •í™•ë„:  0.7601707982445736
              precision    recall  f1-score   support

           0       0.79      0.77      0.78      4724
           1       0.72      0.74      0.73      3707

    accuracy                           0.76      8431
   macro avg       0.76      0.76      0.76      8431
weighted avg       0.76      0.76      0.76      8431
'''
```

- XGBoostëŠ” RandomForestë³´ë‹¤ Hyperparameter settingì— ë¯¼ê°í•˜ë‹¤.

#### Early Stopping
- `n_estimators`ë¥¼ ìµœì í™” í•˜ê¸° ìœ„í•´ GridSeachCVë‚˜ forë¬¸ ëŒ€ì‹  Early stoppingì„ ì‚¬ìš©í•œë‹¤.
- `n_iterations`ê°€ ë°˜ë³µìˆ˜ë¼ í• ë•Œ Early stoppingì„ ì‚¬ìš©í•˜ë©´ `n_interation` ë§Œí¼ì˜ Treeë¥¼ í•™ìŠµí•˜ë©´ ëœë‹¤.
- GridSearchCVë‚˜ forë¬¸ì„ ì‚¬ìš©í•˜ë©´ `sum(range(1,n_rounds+1))`ë§Œí¼ì˜ Treeë¥¼ í•™ìŠµí•´ì•¼ í•˜ê¸° ë•Œë¬¸ì—(`max_depth`ë‚˜ `learning_rate`ê°€ ìˆìœ¼ë©´ ë”í•˜ë‹¤.) Early stoppingì„ í™œìš©í•˜ëŠ” ê²Œ íš¨ê³¼ì ì´ë‹¤.
- Target imbalance ì‹œ Classì— Weigtì„ ì£¼ê¸° ìœ„í•´ Ratioë¥¼ ê³„ì‚°í•œë‹¤.

```py
encoder = OrdinalEncoder()
X_train_encoded = encoder.fit_transform(X_train) # í•™ìŠµë°ì´í„°
X_val_encoded = encoder.transform(X_val) # ê²€ì¦ë°ì´í„°

model = XGBClassifier(
    n_estimators=1000,  # <= 1000 íŠ¸ë¦¬ë¡œ ì„¤ì •í–ˆì§€ë§Œ, early stopping ì— ë”°ë¼ ì¡°ì ˆë¨
    max_depth=7,        # default=3, high cardinality íŠ¹ì„±ì„ ìœ„í•´ ê¸°ë³¸ë³´ë‹¤ ë†’ì„
    learning_rate=0.2,
    #scale_pos_weight=ratio, # imbalance ë°ì´í„° ì¼ ê²½ìš° ë¹„ìœ¨ì ìš©
    n_jobs=-1
)

eval_set = [(X_train_encoded, y_train), 
            (X_val_encoded, y_val)]

model.fit(X_train_encoded, y_train, 
          eval_set=eval_set,
          eval_metric='error', # (wrong cases) / # (all cases)
          early_stopping_rounds=50
         ) # 50 rounds ë™ì•ˆ ìŠ¤ì½”ì–´ì˜ ê°œì„ ì´ ì—†ìœ¼ë©´ ë©ˆì¶¤
'''
[0]	validation_0-error:0.23978	validation_1-error:0.24801
Multiple eval metrics have been passed: 'validation_1-error' will be used for early stopping.

Will train until validation_1-error hasn't improved in 50 rounds.
[1]	validation_0-error:0.23509	validation_1-error:0.24564
[2]	validation_0-error:0.22940	validation_1-error:0.24268
[3]	validation_0-error:0.22830	validation_1-error:0.24173
[4]	validation_0-error:0.22575	validation_1-error:0.23900
[5]	validation_0-error:0.22409	validation_1-error:0.23817
[6]	validation_0-error:0.22258	validation_1-error:0.23663
[7]	validation_0-error:0.22098	validation_1-error:0.23627
[8]	validation_0-error:0.22089	validation_1-error:0.23734
[9]	validation_0-error:0.21893	validation_1-error:0.23627
[10]	validation_0-error:0.21691	validation_1-error:0.23485
[11]	validation_0-error:0.21626	validation_1-error:0.23508
[12]	validation_0-error:0.21419	validation_1-error:0.23437
[13]	validation_0-error:0.21261	validation_1-error:0.23331
[14]	validation_0-error:0.21101	validation_1-error:0.23354
[15]	validation_0-error:0.21009	validation_1-error:0.23212
[16]	validation_0-error:0.20888	validation_1-error:0.23224
[17]	validation_0-error:0.20769	validation_1-error:0.23295
[18]	validation_0-error:0.20627	validation_1-error:0.23236
[19]	validation_0-error:0.20473	validation_1-error:0.23271
[20]	validation_0-error:0.20393	validation_1-error:0.23200
[21]	validation_0-error:0.20298	validation_1-error:0.23188
[22]	validation_0-error:0.20179	validation_1-error:0.23105
[23]	validation_0-error:0.20061	validation_1-error:0.23129
[24]	validation_0-error:0.19954	validation_1-error:0.23117
[25]	validation_0-error:0.19865	validation_1-error:0.23046
[26]	validation_0-error:0.19758	validation_1-error:0.23176
[27]	validation_0-error:0.19660	validation_1-error:0.23188
[28]	validation_0-error:0.19524	validation_1-error:0.23141
[29]	validation_0-error:0.19503	validation_1-error:0.23129
[30]	validation_0-error:0.19370	validation_1-error:0.23271
[31]	validation_0-error:0.19293	validation_1-error:0.23236
[32]	validation_0-error:0.19180	validation_1-error:0.23259
[33]	validation_0-error:0.19014	validation_1-error:0.23070
[34]	validation_0-error:0.18892	validation_1-error:0.23034
[35]	validation_0-error:0.18862	validation_1-error:0.22951
[36]	validation_0-error:0.18753	validation_1-error:0.23034
[37]	validation_0-error:0.18622	validation_1-error:0.23153
[38]	validation_0-error:0.18524	validation_1-error:0.23034
[39]	validation_0-error:0.18471	validation_1-error:0.23046
[40]	validation_0-error:0.18379	validation_1-error:0.22987
[41]	validation_0-error:0.18344	validation_1-error:0.23022
[42]	validation_0-error:0.18266	validation_1-error:0.23283
[43]	validation_0-error:0.18222	validation_1-error:0.23354
[44]	validation_0-error:0.18115	validation_1-error:0.23390
[45]	validation_0-error:0.17991	validation_1-error:0.23366
[46]	validation_0-error:0.17955	validation_1-error:0.23437
[47]	validation_0-error:0.17842	validation_1-error:0.23402
[48]	validation_0-error:0.17777	validation_1-error:0.23307
[49]	validation_0-error:0.17700	validation_1-error:0.23283
[50]	validation_0-error:0.17629	validation_1-error:0.23402
[51]	validation_0-error:0.17513	validation_1-error:0.23449
[52]	validation_0-error:0.17442	validation_1-error:0.23437
[53]	validation_0-error:0.17442	validation_1-error:0.23461
[54]	validation_0-error:0.17294	validation_1-error:0.23331
[55]	validation_0-error:0.17214	validation_1-error:0.23414
[56]	validation_0-error:0.17184	validation_1-error:0.23414
[57]	validation_0-error:0.17128	validation_1-error:0.23508
[58]	validation_0-error:0.16983	validation_1-error:0.23461
[59]	validation_0-error:0.16890	validation_1-error:0.23402
[60]	validation_0-error:0.16819	validation_1-error:0.23378
[61]	validation_0-error:0.16760	validation_1-error:0.23425
[62]	validation_0-error:0.16686	validation_1-error:0.23508
[63]	validation_0-error:0.16597	validation_1-error:0.23366
[64]	validation_0-error:0.16526	validation_1-error:0.23366
[65]	validation_0-error:0.16413	validation_1-error:0.23366
[66]	validation_0-error:0.16336	validation_1-error:0.23366
[67]	validation_0-error:0.16289	validation_1-error:0.23354
[68]	validation_0-error:0.16259	validation_1-error:0.23390
[69]	validation_0-error:0.16223	validation_1-error:0.23342
[70]	validation_0-error:0.16194	validation_1-error:0.23259
[71]	validation_0-error:0.16123	validation_1-error:0.23283
[72]	validation_0-error:0.16060	validation_1-error:0.23402
[73]	validation_0-error:0.15980	validation_1-error:0.23342
[74]	validation_0-error:0.15939	validation_1-error:0.23283
[75]	validation_0-error:0.15900	validation_1-error:0.23342
[76]	validation_0-error:0.15820	validation_1-error:0.23295
[77]	validation_0-error:0.15749	validation_1-error:0.23354
[78]	validation_0-error:0.15672	validation_1-error:0.23319
[79]	validation_0-error:0.15589	validation_1-error:0.23307
[80]	validation_0-error:0.15559	validation_1-error:0.23247
[81]	validation_0-error:0.15458	validation_1-error:0.23283
[82]	validation_0-error:0.15408	validation_1-error:0.23319
[83]	validation_0-error:0.15369	validation_1-error:0.23271
[84]	validation_0-error:0.15283	validation_1-error:0.23295
[85]	validation_0-error:0.15233	validation_1-error:0.23354
Stopping. Best iteration:
[35]	validation_0-error:0.18862	validation_1-error:0.22951

XGBClassifier(base_score=0.5, booster=None, colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
              importance_type='gain', interaction_constraints=None,
              learning_rate=0.2, max_delta_step=0, max_depth=7,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              n_estimators=1000, n_jobs=-1, num_parallel_tree=1, random_state=0,
              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
              tree_method=None, validate_parameters=False, verbosity=None)
'''

results = model.evals_result()
train_error = results['validation_0']['error']
val_error = results['validation_1']['error']

epoch = range(1, len(train_error)+1)
plt.plot(epoch, train_error, label='Train')
plt.plot(epoch, val_error, label='Validation')
plt.ylabel('Classification Error')
plt.xlabel('Model Complexity (n_estimators)')
plt.ylim((0.15, 0.25)) # Zoom in
plt.legend();
```

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-26 14 15 21](https://user-images.githubusercontent.com/79494088/130904888-7f7bfd1c-5417-4767-9584-e812cd64bbf6.png)

```py
print('ê²€ì¦ ì •í™•ë„', model.score(X_val_encoded, y_val))

print(classification_report(y_val, model.predict(X_val_encoded)))
'''
ê²€ì¦ ì •í™•ë„ 0.7704898588542285
              precision    recall  f1-score   support

           0       0.78      0.81      0.79      4608
           1       0.76      0.73      0.74      3823

    accuracy                           0.77      8431
   macro avg       0.77      0.77      0.77      8431
weighted avg       0.77      0.77      0.77      8431
'''
```

### Hyperparameter Tuning

#### Random Forest
- max_depth (ë†’ì€ê°’ì—ì„œ ê°ì†Œì‹œí‚¤ë©° íŠœë‹, ë„ˆë¬´ ê¹Šì–´ì§€ë©´ ê³¼ì í•©)
- n_estimators (ì ì„ê²½ìš° ê³¼ì†Œì í•©, ë†’ì„ê²½ìš° ê¸´ í•™ìŠµì‹œê°„)
- min_samples_leaf (ê³¼ì í•©ì¼ê²½ìš° ë†’ì„)
- max_features (ì¤„ì¼ ìˆ˜ë¡ ë‹¤ì–‘í•œ íŠ¸ë¦¬ìƒì„±, ë†’ì´ë©´ ê°™ì€ íŠ¹ì„±ì„ ì‚¬ìš©í•˜ëŠ” íŠ¸ë¦¬ê°€ ë§ì•„ì ¸ ë‹¤ì–‘ì„±ì´ ê°ì†Œ)
- class_weight (imbalanced í´ë˜ìŠ¤ì¸ ê²½ìš° ì‹œë„)

#### XGBoost
- learning_rate (í•™ìŠµ ì†ë„, ë†’ì„ê²½ìš° ê³¼ì í•© ìœ„í—˜)
- max_depth (ë‚®ì€ê°’ì—ì„œ ì¦ê°€ì‹œí‚¤ë©° íŠœë‹, ë„ˆë¬´ ê¹Šì–´ì§€ë©´ ê³¼ì í•©ìœ„í—˜, -1 ì„¤ì •ì‹œ ì œí•œ ì—†ì´ ë¶„ê¸°, íŠ¹ì„±ì´ ë§ì„ ìˆ˜ë¡ ê¹Šê²Œ ì„¤ì •)
- n_estimators (íŠ¸ë¦¬ ìˆ˜, ë„ˆë¬´ í¬ê²Œ ì£¼ë©´ ê¸´ í•™ìŠµì‹œê°„, early_stopping_roundsì™€ ê°™ì´ ì‚¬ìš©)
- scale_pos_weight (imbalanced ë¬¸ì œì¸ ê²½ìš° ì ìš©ì‹œë„)

## ì°¸ê³ ìë£Œ

- [Olson 2017](https://arxiv.org/abs/1708.05070)
- [Feature Importance in Decision Trees](https://sefiks.com/2020/04/06/feature-importance-in-decision-trees/)
- Permutation Importance
    - [Kaggle: Machine Learning Explainability](https://www.kaggle.com/dansbecker/permutation-importance)
    - [Christoph Molnar: Interpretable Machine Learning](https://christophm.github.io/interpretable-ml-book/feature-importance.html)
    - [Selecting good features, Part 3, Random Forests](https://blog.datadive.net/selecting-good-features-part-iii-random-forests/)
    - [Permutation Importance vs Random Forest Feature Importance (MDI)](https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance.html) 
    - [Beware Default Random Forest Importances](https://explained.ai/rf-importance/index.html)
    - [eli5.sklearn.PermutationImportance](https://eli5.readthedocs.io/en/latest/autodocs/sklearn.html#eli5.sklearn.permutation_importance.PermutationImportance)
    - [eli5.show_weights](https://eli5.readthedocs.io/en/latest/autodocs/eli5.html#eli5.show_weights)
    - [scikit-learn user guide, `scoring` parameter](https://scikit-learn.org/stable/modules/model_evaluation.html#the-scoring-parameter-defining-model-evaluation-rules)
- Boosting
    - [Understanding AdaBoost](https://towardsdatascience.com/understanding-adaboost-2f94f22d5bfe)
    - [Study of AdaBoost and Gradient Boosting Algorithms for Predictive Analytics](https://link.springer.com/chapter/10.1007/978-981-15-0633-8_22)
    - [Gradient Boosting, DataCamp](https://campus.datacamp.com/courses/machine-learning-with-tree-based-models-in-python/boosting?ex=5)
    - [Gradient Boost Part 2: Regression Details](https://youtu.be/2xudPOBz-vs)
    - [Gradient Boost Part 3: Classification](https://youtu.be/jxuNLH5dXCs)
    - [Avoid Overfitting By Early Stopping With XGBoost In Python](https://machinelearningmastery.com/avoid-overfitting-by-early-stopping-with-xgboost-in-python/)
    - [Notes on parameter tuning](https://xgboost.readthedocs.io/en/latest/tutorials/param_tuning.html)
    - [Parameters documentation](https://xgboost.readthedocs.io/en/latest/parameter.html)