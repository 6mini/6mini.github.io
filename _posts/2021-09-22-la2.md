---
title: '[Linear Algebra] Intermediate Linear Algebra'
description: 분산과 공분산의 차이와 상관계수의 목적, 벡터의 직교, 단위 벡터, Span, Basis, Rank, Gaussian Elemination, Linear Projection 등의 설명
categories:
 - Fundamentals to DS
tags: [Linear Algebra, 분산, 공분산, 상관계수, 단위벡터, Span, Basis, Rank, Gaussian Elemination, Linear Projection]
mathjax: enable
# 0️⃣1️⃣2️⃣3️⃣4️⃣5️⃣6️⃣7️⃣8️⃣9️⃣🔟
---

# 분산(Varicance)
- 분산 : 데이터가 얼마나 퍼져있는지를 측정하는 방법
- 각 값들의 평균으로부터 차이의 제곱 평균이다.
- 분산을 구하기 위해서 일반적으로 평균을 먼저 계산해야한다.

```py
# 랜덤한 50개의 정수를 포함하는 2 변수 설정
variance_one = []
variance_two = []
for x in range(50):
  variance_one.append(random.randint(25,75))
  variance_two.append(random.randint(0,100))
  
variance_data = {'v1': variance_one, 'v2': variance_two}

variance_df = pd.DataFrame(variance_data)
variance_df['zeros'] = pd.Series(list(np.zeros(50)))

variance_df.head()
```

![스크린샷 2021-08-21 21 26 37](https://user-images.githubusercontent.com/79494088/130321706-3cc584a6-86c2-46d6-9789-00fd0e52bcd3.png)

```py
# scatter plot

plt.scatter(variance_df.v1, variance_df.zeros)
plt.xlim(0,100)
plt.title("Plot 1")
plt.show()

plt.scatter(variance_df.v2, variance_df.zeros)
plt.xlim(0,100)
plt.title("Plot 2")
plt.show()
```

![스크린샷 2021-08-21 21 27 14](https://user-images.githubusercontent.com/79494088/130321726-cc6e80da-8e3b-4a90-bbac-137da2cff6c7.png)

- Scatter Plot의 두 데이터를 통해 벌어져있는 정도의 차이를 쉽게 확인할 수 있다.
- $\overline{X}$ 는 평균, $N$ 은 관측의 수(샘플의 수)
- $v$ 혹은 분산은 일반적으로 소문자 v로 표기되며 필요에 따라 $\sigma^{2}$로 표기한다.

$$
v = \frac{\sum{(X_{i} - \overline{X})^{2}} }{N}
$$

```py
# 평균
v1_mean = variance_df.v1.mean()
print("v1 mean: ", v1_mean)
v2_mean = variance_df.v2.mean()
print("v2 mean: ", v2_mean)

# 각 데이터로부터 평균까지의 거리
variance_df['v1_distance'] = variance_df.v1-v1_mean
variance_df['v2_distance'] = variance_df.v2-v2_mean

variance_df.head()

# 제곱 
variance_df['v1_squared_distance'] = variance_df.v1_distance**2
variance_df['v2_squared_distance'] = variance_df.v2_distance**2

# 제곱을 통해서 음수를 양수로 (거리의 크기) 바꿀 수 있습니다.
variance_df.head()
```

![스크린샷 2021-08-21 21 45 24](https://user-images.githubusercontent.com/79494088/130322214-68d879bb-aff2-43f1-be41-d5408556305b.png)

```py
# 더하고 나눔
observations = len(variance_df)
print("Number of Observations: ", observations)

Variance_One = variance_df.v1_squared_distance.sum()/observations
Variance_Two = variance_df.v2_squared_distance.sum()/observations

print("Variance One: ", Variance_One)
print("Variance Two: ", Variance_Two)
'''
Number of Observations:  50
Variance One:  212.17640000000003
Variance Two:  764.3844
'''
```

- Random Number를 생성할 때, v1은 `25 ~ 75`까지 `50`범위를 / v2는 `0 ~ 100`까지 `100`범위를 바탕으로 2배 정도 차이나게 생성했지만, **분산의 차이는 4배정도 차이**가 난다.

## Variance 쉽게 계산하기

```py
print(variance_df.v1.var(ddof = 1))
print(variance_df.v2.var(ddof = 1))
'''
216.50653061224492
779.9840816326531
'''
```

- 앞서 계산했던 결과와 조금 다르다.
  - 그 이유는 분산을 계산하는 방법이 모집단이냐 혹은 샘플이냐에 따라서 달라지기 때문이다.
- **모집단의 분산** $\sigma^{2}$ : 모집단의 **PARAMETER** (aspect, property, attribute, etc)
- The **샘플의 분산** $s^{2}$ : 샘플의 **STATISTIC** (estimated attribute)
- 샘플 분산 $s^{2}$ 는 모집단 분산 $\sigma^{2}$의 추정치다.
- 일반적으로, **샘플**의 분산을 계산 할 때 $N-1$로 나누어야 한다.
- 앞서 우리가 데이터를 통해 계산했던 방식은 **모집단의 분산**이다.
- 그렇기 때문에 자유도를 0 으로 설정하는 경우, 동일한 값을 얻을 수 있다.

```py
print(variance_df.v1.var(ddof = 0))
print(variance_df.v2.var(ddof = 0))
'''
212.17640000000003
764.3844
'''
```

# Standard Deviation
- 표준편차 : 분산의 값에 $\sqrt()$를 씌운 것이다.

## 분산/표준편차
- 표준편차는 분산이 평균값에 비해서 스케일의 문제가 있어서 이를 해결하기 위해 제곱된 스케일을 낮춘 방법이다.
- 이는 많은 통계분석 프로세스에서 표준편차를 사용하여 게산하는 이유 중 하나다.

```py
print(variance_df.v1.std(ddof=0)) # std 에 주의
print(variance_df.v2.std(ddof=0))
'''
14.566276119859873
27.647502599692437
'''
```

# 공분산(Covariance)
- 공분산(Covariance) : 1개 변수 값이 변화할 때 다른 변수가 어떤 연관성을 나타내며 변하는지를 측정하는 것

![스크린샷 2021-08-21 21 57 26](https://user-images.githubusercontent.com/79494088/130322521-9fa56c84-abb6-435e-8fcf-d18121e1cac4.png)

- 첫번째 그래프 : 음의 공분산 값을 갖는다.(nagative)
- 두번째 그래프 : 양 변수의 높고 낮음에 대해 관련성을 알 수 없다. 0에 가까운 공분산 값을 갖는다.
- 세번재 그래프 : 양 변수간의 공분산 값은 양의 값이다.(positive)

## 공분산 이해하기
- 큰 값의 공분산은 두 변수간의 큰 연관성을 나타낸다.
- 만약 변수들이 다른 스케일을 가지고 있다면 공분산은 실제 변수의 연관성에 관계 없이 영향을 받게 될 것이다.
- 두 변수가 연관성이 적더라도 큰 스케일을 가지고 있다면, 연관성이 높지만 스케일이 작은 변수들에 비해서 높은 공분산 값을 갖게 될 것이다.

```py
a = b = np.arange(5, 50, 5)
c = d = np.arange(10,100,10)

fake_data = {"a": a, "b": b, "c": c, "d": d}

df = pd.DataFrame(fake_data)

plt.scatter(df.a, df.b)
plt.xlim(0,100)
plt.ylim(0,100)
plt.show()

plt.scatter(df.c, df.d)
plt.xlim(0,100)
plt.ylim(0,100)
plt.show()
```

![스크린샷 2021-08-21 22 01 32](https://user-images.githubusercontent.com/79494088/130322622-3ce36c3c-162c-4068-892b-f5dbf46cce6b.png)

## Variance covariance matrix

```py
# 공분산 계산
df.cov()
```

![스크린샷 2021-08-21 22 03 13](https://user-images.githubusercontent.com/79494088/130322682-f5c40162-26fc-4011-b667-35a7f2a99427.png)

- 이러한 matrix를 variance-covariance matrix 라고 표현하며, 대각선 부분은 공분산이 아닌, 분산을 표현한다.
- 두 데이터셋(a-b, c-d)은 동일한 연관성을 갖고 있지만 ($x = y$), 계산된 공분산의 값은 매우 다르다.

## Correlation coefficient
- 분산에서 표준편차를 사용했던 것처럼, 공분산의 스케일을 조정할 수 있다.
- 공분산을 두 변수의 표준편차로 각각 나눠주면 스케일을 조정할 수 있으며 그것을 **상관계수**라고 부른다.
- 상관계수는 -1에서 1까지로 정해진 범위 안의 값만을 가지며 선형연관성이 없는 경우 0에 근접하게 된다.
- 대부분의 경우, **상관계수**는 공분산에 비해 더 좋은 지표로 사용된다.
  - 공분산은 이론상 모든 값을 가질 수 있지만, 상관계수는 `-1 ~ 1`사이로 정해져 비교하기 쉽다.
  - 공분산은 항상 스케일, 단위를 포함하고 있지만, 상관계수는 이에 영향을 받지 않는다.
  - 상관계수는 데이터의 평균 혹은 분산의 크기에 영향을 받지 않는다.
- 상관계수는 일반적으로 소문자 $r$로 표현된다.

$$cor(X,Y) = r = \frac{cov(X,Y)}{\sigma_{X}\sigma_{Y}} $$

```py
# correlation 계산
df.corr()
```

![스크린샷 2021-08-21 22 22 48](https://user-images.githubusercontent.com/79494088/130323150-8ec28e0f-561a-48bc-a51b-ebe6a2033eef.png)

- 상관계수가 1이라는 것은 한 변수가 다른 변수에 대해 완벽한 양의 선형 관계를 갖고 있다는 것을 의미한다.

![스크린샷 2021-08-21 22 31 06](https://user-images.githubusercontent.com/79494088/130323378-7a4ff648-3571-46e9-998a-595928d1ccb2.png)

## Spearman Correlation
- 상관계수(correlation coefficient)는 Pearson correlation이라 부르며 이는 데이터로부터 분산과 같은 통계치를 계산할 수 있을 때 사용가능하다.
- 만약 데이터가 numeric이 아니라, categorical이라면 spearman correlation coefficien는 값들에 대해 순서 혹은 rank를 매기고, 그를 바탕으로 correlation을 측정하는 Non-parametiric한 방식이다.

# Orthogonality
- Orthogonality : 벡터 혹은 매트릭스가 서로 수직으로 있는 상태

![스크린샷 2021-08-21 22 37 29](https://user-images.githubusercontent.com/79494088/130323527-794089b9-0045-4a4c-a293-377e6d643806.png)

- 좌표상에 있는 거의 모든 벡터는 다른 벡터와 상관이 아주 작게라도 있다.
- 하나가 증가할 때, 다른 하도 증가하는 경향을 파악하는 것이 공분산이다.
- 딱 하나, 수직인 벡터만 상관관계가 전혀 없다.
- 데이터에 선형관계가 있는 것과 상관관계가 없는 것이 어떤 의미를 가질까?

## 벡터의 orthogonality 확인
- 임의의 두 벡터의 내적값이 0이라면 서로 수직으로 배치되어 있다.

```py
vector_1 = [0, 5]
vector_2 = [5, 0]

plt.arrow(0, 0, vector_1[0], vector_1[1], head_width = .1, head_length = .1, color ='#d63031')
plt.arrow(0, 0, vector_2[0], vector_2[1], head_width = .1, head_length = .1, color ='#00b894')
plt.xlim(-1, 7)          
plt.ylim(-1, 7)
plt.title("Orthogonal Vectors")
plt.show()
```

![스크린샷 2021-08-21 22 46 05](https://user-images.githubusercontent.com/79494088/130323770-513e6098-30f5-4495-ab6b-1cac10f4de14.png)

- 그래프상으로 명확하게 벡터들이 서로 수직하다.

$$
a = \begin{bmatrix} 0 & 5\end{bmatrix}
\qquad
b = \begin{bmatrix} 5 & 0\end{bmatrix}
\\
a \cdot b = (0)(5) + (5)(0) = 0
$$

- 내적값은 0이다.

# 단위 벡터(Unit Vectors)
- 선형대수에서 단위벡터란 '단위길이(1)'을 갖는 모든 벡터이다.
  - $v$ = [1, 2, 2] 
  - $\vert\vert v\vert\vert$ = $\sqrt{1^2 + 2^2 + 2^2}$ = 3
  - $\hat{v}$ = 1 / $\vert\vert v\vert\vert$ $\cdot$ $v$ = $1 \over 3$ $\cdot$ [1, 2, 2] = [$1 \over 3$, $2 \over 3$, $2 \over 3$]
  - $\vert\vert\hat{v}\vert\vert$ = 1

---

- 각각 1, 2, 3차원의 단위 벡터 입니다.
  - $\mathbb{R}$ unit vector: $\hat{i} = \begin{bmatrix} 1 \end{bmatrix}$
  - $\mathbb{R}^2$ unit vectors: $\hat{i} = \begin{bmatrix} 1 \\ 0 \end{bmatrix}$, $\hat{j} = \begin{bmatrix} 0 \\ 1 \end{bmatrix}$
  - $\mathbb{R}^3$ unit vectors: $\hat{i} = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}$, $\hat{j} = \begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix}$,  $\hat{k} = \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}$

```py
  # 축 설정

plt.xlim(-1,2)          
plt.ylim(-1,2)

# 단위 벡터
i_hat = [1,0]
j_hat = [0,1]

# 축 고정 
plt.gca().set_aspect('equal')

# 벡터 그리기
plt.arrow(0, 0, i_hat[0], i_hat[1], linewidth = 3, head_width = .05, head_length = .05, color = '#d63031')
plt.arrow(0, 0, j_hat[0], j_hat[1], linewidth = 3, head_width = .05, head_length = .05, color = '#0984e3')
plt.title("basis vectors in R^2")
plt.show()
```
![스크린샷 2021-08-21 22 54 03](https://user-images.githubusercontent.com/79494088/130323950-822e9699-a058-4127-9897-fec12c6f78b9.png)

## 벡터를 단위 벡터의 조합으로 표기
- 모든 벡터는 단위 벡터의 선형 조합으로 표기된다.
- v = [5, 30] = [1, 0] * 5 + [0, 1] * 30 = 5 $\cdot$ $\hat{i}$ + 30 $\cdot$ $\hat{j}$
- 위의 예시처럼, $\mathbb{R}^2$내에 있는 임의의 벡터를 단위 벡터의 조합으로 표기 할 수 있다.

# Span
- Span : 주어진 두 벡터의 (합이나 차와 같은) 조합으로 만들 수 있는 모든 가능한 벡터의 집합

## 선형 관계의 벡터 (Linearly Dependent Vector)
- 두 벡터가 같은 선상에 있는 경우 이 벡터들은 선형 관계에 있다고 표현한다.
- 두 벡터들은 조합을 통해 선 외부의 새로운 백터를 생성할 수 없다.
- 이러한 벡터의 span은 평면 공간이 아닌 벡터가 이미 올려져 있는 선으로 제한된다.

## 비선형 관계의 벡터 (Linearly Independent Vector)
- 같은 선상에 있지 않은 벡터들은 선형적으로 독립되어 있다고 표현한다.
- 주어진 공간 (2개의 벡터의 경우 $R^2$ 평면)의 모든 벡터를 조합을 통해 만들어 낼 수 있다.

```py
plt.xlim(-1, 4)          
plt.ylim(-1, 1)

# 원 벡터 (초록)
v = [1,0] 

# 선형관계의 벡터
v2 = np.multiply(3, v) # (노랑)
v3 = np.multiply(-1, v) # (빨강)

axes = plt.gca()
x_vals = np.array(axes.get_xlim())
y_vals = 0 * x_vals

plt.plot(x_vals, y_vals, '--', color = '#0984e3', linewidth = 1) # span 선 (파랑)
plt.arrow(0, 0, v2[0], v2[1], linewidth = 3, head_width = .05, head_length = .05, color = '#fdcb6e')
plt.arrow(0, 0, v[0], v[1], linewidth = 3, head_width = .05, head_length = .05, color = '#00b894')
plt.arrow(0, 0, v3[0], v3[1], linewidth = 3, head_width = .05, head_length = .05, color = '#d63031')

plt.title("Linearly Dependent Vectors")
plt.show()
```

![스크린샷 2021-09-16 01 29 44](https://user-images.githubusercontent.com/79494088/133472760-b58149c5-84b4-43bc-b894-8d2deea21ce2.png)

```py
# 선형 관계에 있지 않은 벡터

plt.xlim(-2, 3.5)
plt.ylim(-.5, 1.5)

a = [-1.5, .5]
b = [3, 1]
plt.arrow(0, 0, a[0], a[1], linewidth = 3, head_width = .05, head_length = .05, color = '#74b9ff')
plt.arrow(0, 0, b[0], b[1], linewidth = 3, head_width = .05, head_length = .05, color = '#e84393')

plt.title("Linearly Independent Vectors")
plt.show()
```

![스크린샷 2021-09-16 01 30 15](https://user-images.githubusercontent.com/79494088/133472843-af3bcfca-f93f-4d4e-87fc-f7b034e68f96.png)

# Basis
- 벡터 공간 $V$ 의 basis는, $V$ 라는 공간을 채울 수 있는 선형 관계에 있지 않은 벡터들의 모음이다.(Span의 역개념)
- cf. 위의 그림에서 2개의 벡터 (빨강, 파랑)는 벡터 공간 $\mathbb{R}^2$ 의 basis 이다.

## Orthogonal Basis
- Orthogonal Basis : Basis에 추가로 Orthogonal 한 조건이 붙는, 즉 주어진 공간을 채울 수 있는 서로 수직인 벡터들

## Orthonormal Basis
- Orthonormal Basis : Orthogonal Basis에 추가로 Normalized 조건이 붙은 것으로, 길이가 서로 1인 벡터들

![스크린샷 2021-09-16 01 52 06](https://user-images.githubusercontent.com/79494088/133476042-079db6d0-9511-41f8-ad8d-10aa85a41895.png)

# Rank
- Rank : 매트릭스의 열을 이루고 있는 벡터들로 만들 수 있는 (span) 공간의 차원
- 매트릭스의 차원과는 다를 수 있는데, 그 이유는 행과 열을 이루고 있는 벡터들 가운데 서로 선형 관계가 있을 수 있기 때문이다.
- Rank를 확인하는 방법은 여러가지가 있지만, 그 중 하나인 Gaussian Elimination을 통해 알아본다.

## Gaussian Elimination
- Gaussian Elimination : 주어진 매트릭스를 'Row-Echelon form'으로 바꾸는 계산과정
- Row-Echelon form : 각 행에 대해 왼쪽에 1, 그 이후 부분은 0으로 이뤄진 형태
- 이러한 매트릭스는 일반적으로 upper-triangular 형태를 갖고있다.

![스크린샷 2021-09-16 02 06 15](https://user-images.githubusercontent.com/79494088/133478047-675e57e9-aee0-47cd-91a2-264cdb1a1f58.png)

$$
 P = \begin{bmatrix}
  1 & 1 & 3 \\
  0 & 1 & 2 \\
  2 & 1 & 4 
 \end{bmatrix}
$$

- 3행을 1행*2 로 뺀다.

$$
 P = \begin{bmatrix}
  1 & 1 & 3 \\
  0 & 1 & 2 \\
  0 & -1 & -2 
 \end{bmatrix}
$$

- 3행을 2행으로 더한다.

$$
 P = \begin{bmatrix}
  1 & 1 & 3 \\
  0 & 1 & 2 \\
  0 & 0 & 0 
 \end{bmatrix}
$$

- **맨 마지막 줄이 0, 0, 0이다** 라는 것은 3개의 행이 선형 관계가 있다는 의미이다.<br>
(다른 행들의 스칼라 곱과 합으로 표현됨)

$r_3$ = $2 \cdot\ r_1$ - $1 \cdot\ r_2$

$y$ = $a \cdot\ x + b$

- 처음에 주어졌던 매트릭스

$$
 P = \begin{bmatrix}
  1 & 1 & 3 \\
  0 & 1 & 2 \\
  2 & 1 & 4 
 \end{bmatrix}
$$

- Rank 는 2이며 이는 3x3 매트릭스지만 $\mathbb{R}^{3}$ 공간이 아닌 $\mathbb{R}^{2}$ 를 벡터로 만들어 낼 수 있다.

# Linear Projections

![스크린샷 2021-09-16 23 57 29](https://user-images.githubusercontent.com/79494088/133635474-4099f887-9ce3-4723-b0b4-6b981ae3057b.png)

## Python으로 과정 구현
- $\mathbb{R}^{2}$ 공간의 임의의 선 $L$ 가정

```py
plt.xlim(-1, 2)
plt.ylim(-1, 2)
axes = plt.gca()
x_vals = np.array(axes.get_xlim())
y_vals = 0 * x_vals
plt.plot(x_vals, y_vals, '--', color = '#0984e3')
plt.title("A Line")
plt.show()
```

![스크린샷 2021-09-16 23 58 30](https://user-images.githubusercontent.com/79494088/133635647-e98b6d21-325f-4b17-98ff-2864a98804ca.png)

- 임의의 벡터 $v$가 선 위에 있는 경우, $v$는 아무리 스케일을 변화시켜도 결국 선 위에 있다.

```py
plt.xlim(-1.1, 3)          
plt.ylim(-1.1, 3)

v = [1, 0] 

v2 = np.multiply(3, v)
v3 = np.multiply(-1, v)

axes = plt.gca()
x_vals = np.array(axes.get_xlim())
y_vals = 0 * x_vals

plt.plot(x_vals, y_vals, '--', color = '#0984e3')


plt.arrow(0, 0, v2[0], v2[1], linewidth = 3, head_width = .05, head_length = .05, color = '#fdcb6e')
plt.arrow(0, 0, v[0], v[1], linewidth = 3, head_width = .05, head_length = .05, color = '#00b894')
plt.arrow(0, 0, v3[0], v3[1], linewidth = 3, head_width = .05, head_length = .05, color = '#d63031')

plt.title("v scaled two different ways")
plt.show()
```

![스크린샷 2021-09-17 00 02 55](https://user-images.githubusercontent.com/79494088/133636439-b1866d4a-0830-4fb0-a7c8-1cb883e48d7b.png)

- 녹색 벡터 : $v$
- 처음에 주어진 선 $L$은 $\mathbb{R}$ 공간내의 모든 $v$와 동일하다.

$$
L = cv
$$

- $L$ 위에 Project(투사) 하기 위한 새로운 $w$라는 벡터를 추가한다.

```py
plt.xlim(-1.1, 3)          
plt.ylim(-1.1, 3)

v = [1, 0] 
w = [2, 2]

axes = plt.gca()
x_vals = np.array(axes.get_xlim())
y_vals = 0 * x_vals

plt.plot(x_vals, y_vals, '--', color = '#0984e3')
plt.arrow(0, 0, v[0], v[1], linewidth = 3, head_width = .05, head_length = .05, color = '#00b894')
plt.arrow(0, 0, w[0], w[1], linewidth = 3, head_width = .05, head_length = .05, color = '#d63031')

plt.title("vector w")
plt.show()
```

![스크린샷 2021-09-17 00 04 52](https://user-images.githubusercontent.com/79494088/133636793-665448f4-024f-4860-b5e8-295448c978e7.png)

## Notation
- Projection을 표기하는 방법

$$
proj_{L}(\vec{w})
$$

```py
plt.xlim(-1.1, 3)
plt.ylim(-1.1, 3)

v = [1, 0] 
w = [2, 2]
proj = [2, 0]

axes = plt.gca()
x_vals = np.array(axes.get_xlim())
y_vals = 0 * x_vals

plt.plot(x_vals, y_vals, '--', color = '#0984e3')
plt.arrow(0, 0, proj[0], proj[1], linewidth = 3, head_width = .05, head_length = .05, color = '#636e72')
plt.arrow(0, 0, v[0], v[1], linewidth = 3, head_width = .05, head_length = .05, color = '#00b894')
plt.arrow(0, 0, w[0], w[1], linewidth = 3, head_width = .05, head_length = .05, color = '#d63031')

plt.title("Shadow of w")
plt.show()
```

![스크린샷 2021-09-17 00 06 21](https://user-images.githubusercontent.com/79494088/133637062-cf52f9f5-cdfa-4815-8f5f-2df344a65a4a.png)

- 회색벡터($proj_{L}(w)$)는 빨간벡터($w$)의 녹색벡터($L$)에 대한 projection이며, 녹색벡터를 기준으로 표기 될 수 있다.

$$
cv = proj_{L}(w)
$$

- $L$이 $x$축과 평행하지 않은 조금 더 복잡한 상황일 경우

```py
plt.xlim(-1.1, 3)          
plt.ylim(-1.1, 3)

v = [1, 1/2] 
w = [2, 2]
proj = np.multiply(2, v)

axes.set_aspect('equal')

x_vals = np.array(axes.get_xlim())
y_vals = 1/2 * x_vals

plt.plot(x_vals, y_vals, '--', color = '#0984e3')
plt.arrow(0, 0, proj[0], proj[1], linewidth = 3, head_width = .05, head_length = .05, color = '#636e72')
plt.arrow(0, 0, v[0], v[1], linewidth = 3, head_width = .05, head_length = .05, color = '#00b894')
plt.arrow(0, 0, w[0], w[1], linewidth = 3, head_width = .05, head_length = .05, color = '#d63031')

plt.title("non x-axis projection")
plt.show()
```

![스크린샷 2021-09-17 00 09 24](https://user-images.githubusercontent.com/79494088/133637588-a85af0a6-ff0d-4b52-836c-cd59a201258c.png)

```py
plt.xlim(-1.1, 3)          
plt.ylim(-1.1, 3)

v = [1, 1/2] 
w = [2, 2]
proj = np.multiply(2.4, v)
x_minus_proj = w - proj

plt.gca().set_aspect('equal')

x_vals = np.array(axes.get_xlim())
y_vals = 1/2 * x_vals

plt.plot(x_vals, y_vals, '--', color = '#0984e3')
plt.arrow(0, 0, proj[0], proj[1], linewidth = 3, head_width = .05, head_length = .05, color = '#636e72')
plt.arrow(0, 0, v[0], v[1], linewidth = 3, head_width = .05, head_length = .05, color = '#00b894')
plt.arrow(0, 0, w[0], w[1], linewidth = 3, head_width = .05, head_length = .05, color = '#d63031')
plt.arrow(proj[0], proj[1], x_minus_proj[0], x_minus_proj[1], linewidth = 3, head_width = .05, head_length = .05, color = '#fdcb6e')

plt.title("non x-axis projection")
plt.show()
```

![스크린샷 2021-09-17 00 10 03](https://user-images.githubusercontent.com/79494088/133637704-2b651fcf-ab64-4ff3-a3c3-042f41aa9bf2.png)

- $L$과 Orthogonal한 노란벡터를 사용해서 계산하는데, 이는 projection $L$에서 시작해서 $w$로 도착하는 벡터이다.
- 벡터의 합과 차를 기억한다면, 회색 벡터 + 노란 벡터는 빨간 벡터가 된다.
- 노란 벡터는 빨간 벡터에서 회색 벡터를 뺀 것과 동일한 내용이다.

$$
w-proj_{L}(w)
$$

- 이전에 $L$(회색)에 대한 $w$(빨강)의 projection을 $cv$(녹색 * 스칼라)로 표기했다.
- 노란 벡터는 빨간 벡터에서 회색 벡터를 뺀것과 동일하다.

$$
w - cv
$$

- 회색(녹색도 마찬가지)과 노란색은 서로 수직 상태에 있기 때문에 두 벡터의 내적값은 0이 나온다.
- 회색 $\cdot$ 노란색 = 0

$$
v \cdot (w-cv) = 0
$$

$$
v \cdot w - c(v \cdot v) = 0
$$

$$
v \cdot w = c(v \cdot v)
$$

$$
c = \frac{w \cdot v}{v \cdot v}
$$

- $cv = proj_{L}(w)$이므로 양변에 $v$를 곱하여

$$
proj_{L}(w) =  \frac{w \cdot v}{v \cdot v}v
$$

## Linear Projection의 이유

![스크린샷 2021-09-17 00 15 05](https://user-images.githubusercontent.com/79494088/133638550-eedb0a92-4356-435c-8171-b5866562557e.png)