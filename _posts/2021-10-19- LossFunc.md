---
title: '[Deep Learning] Loss Function'
description: 손실함수에 대한 의미와 종류 정리(MAE, MSE, RMSE, Binary, Categorical, Sparse Categorical Cross-entropy)
categories:
 - Deep Learning
tags: [Deep Learning, Loss Function, 손실함수, MAE, MSE, RMSE, Binary Cross-entropy, Categorical Cross-entropy, Sparse Categorical Cross-entropy]
mathjax: enable
---

# Loss Function

![image](https://user-images.githubusercontent.com/79494088/137899053-0f36a059-9561-47ed-b4dd-17b5bf687286.png)

- 위 그림은 Perceptron 형태로 입력값이 들어오면 모델을 통해 예측값이 산출되는 방식이다.
- 이 예측값이 실제값과 얼마나 유사한지 판단하는 기준이 필요한데 그 것이 바로 손실함수(Loss function)이다.
- 예측값과 실제값의 차이를 Loss라고 하며, 이 Loss를 줄이는 방향으로 학습이 진행된다.
- 일반적으로 Deep Learning Model에서는 BackPropagation, Gradient Descent를 통해 줄인다.

## 의미
- 데이터를 토대로 산출한 모델의 예측값과 실제값의 차이를 표현하는 지표이다.
- 모델 성능의 '나쁨'을 나타내는 지표로, '현재 모델이 데이터를 얼마나 잘 처리하지 못하느냐'를 나타내는 지표라고 할 수 있다.
- 모델은 보통 회귀문제(Regression)와 분류문제(Classification)로 나뉘며, 손실 함수도 문제에 따라 나눠진다. 회귀의 대표적 손실 함수는 MAE, MSE, RMSE가 있으며, 분류에 쓰이는 손실 함수는 Binary cross-entropy, Categorical cross-entropy 등이 있다.

## 종류

### Regression

#### MAE(Mean Absolute Error)

$$MAE = \frac{1}{n} \sum_{i = 1}^n |x_i - x |$$

- 예측값과 실제값의 차이에 절댓값을 취하며, 그 값들을 전부 더하고 개수로 나누어 평균을 낸 값이다.
- 전체 데이터의 학습된 정도를 쉽게 파악할 수 있다.
- 절댓값을 취하기 때문에 해당 예측이 어떤 식으로 오차가 발생했는지, 음수인지 양수인지 판단할 수 없다는 단점이 있다.
- 최적값에 가까워지더라도 이동거리가 일정하기 때문에 최적값에 수렴하기 어렵다.

#### MSE(Mean Squared Error)

$$
MSE(\boldsymbol{\theta}, b) = \frac{1}{m}\sum(\hat{y}^{(i)}-y^{(i)})^2 \\ 
= \frac{1}{m}\sum_{i=0}^m(\boldsymbol{\theta} \cdot \boldsymbol{x}^{(i)} + b-y^{(i)})^2
$$

- 가장 많이 쓰이는 손실 함수 중 하나이며, 예측 값과 실제 값 사이의 평균을 제곱하여 평균을 낸 값이다.
- 차이가 커질수록 제곱연산으로 인해 값이 뚜렷해지며 제곱으로 인해 오차가 양수든 음수든 누적 값을 증가시킨다.
- 실제 정답에 대한 정답률의 오차뿐 아니라 다른 오답들에 대한 정답률 오차 또한 포함하여 계산한다.
- 단점으로는 값을 제곱하기 때문에 값의 왜곡이 있을 수 있다.

#### RMSE(Root Mean Squared Error)

$$RMSE = \sqrt{\frac{1}{n}\sum_{k}^{n}(y_k - \hat{y_k})^2}$$

- MSE에 루트를 씌운 지표로 장단점은 MSE와 유사하다.
- 제곱된 값에 루트를 씌우기 때문에 값을 제곱해서 생기는 왜곡이 줄어들며, 오차를 보다 직관적으로 보여준다.
- 그 이유는 루트를 씌워주기 때문에 오류값을 실제값과 유사한 단위로 변환하여 해석할 수 있기 때문이다.

### Classification

#### Cross-entropy

$$L = -\frac{1}{n} \sum_{i=1}^{n} \sum_{c=1}^{C} L_{ic}log(P_{ic})$$

- Cross-entropy는 실제 분포 $q$에 대해 알지 못하는 상태에서, 모델링을 통하여 구한 분포인 $p$를 통하여 $q$의 값을 예측하는 것이다.
- 실제값과 예측값이 맞는 경우에 0으로 수렴하고, 값이 틀릴 경우에는 값이 커지기 때문에 실제 값과 예측값의 차이를 줄이기 위한 방식이다.

##### Binary Cross-entropy

$$L = -\frac{1}{N}\sum_{i=1}^{N}t_ilog(y_i)+(1-t_i)log(1-y_i)$$

- True or False로 분류하는 이진 분류 문제에서 사용된다.
- [0,0]과 [0,1]을 대입해보면, 각각 0 과 무한대가 나오기 때문에 이진 분류에 적절히 사용될 수 있는 손실함수이다.

##### Categorical Cross-entropy

$$L = -\frac{1}{N}\sum_{j=1}^{N}\sum_{i=1}^{C}t_{ij}log(y_{ij})$$

- 분류해야 할 클래스가 3개 이상인 멀티클래스 분류에 사용된다.
- 라벨이 [0,0,1,0,0], [1,0,0,0,0], [0,0,0,1,0]과 같이 one-hot 형태로 제공될 때 사용된다.
- 만약 실제값이 [1,0,0,0,0]일 때, [1,0,0,0,0]과 [0,0,1,0,0]
을 대입해보면 각각 0과 무한대가 나오기 때문에 멀티클래스 분류 문제에 적절히 사용될 수 있는 손실함수이다.

##### Sparse Categorical Cross-entropy
- Categorical Cross-entropy와 비슷하게 멀티클래스 분류 문제에 사용되며, Sparse만 붙은 것을 볼 수 있는데 라벨이 [1,2,3,4,5]과 같이 정수의 형태로 제공될 때 사용한다.

# Reference
- [[ML101] #3. Loss Function](https://brunch.co.kr/@mnc/9)
- [딥러닝 손실 함수(loss function) 정리](https://bskyvision.com/822)