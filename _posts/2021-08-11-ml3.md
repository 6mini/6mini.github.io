---
title: '[Machine Learning] ë¦¿ì§€íšŒê·€(Ridge Regression)ë€?'
description: ë²”ì£¼í˜•(Categorical) ìë£Œë¥¼ ë‹¤ë£¨ê¸° ìœ„í•œ ì›í•«ì¸ì½”ë”©(One-hot encoding) ê¸°ë²•ê³¼ Ridge íšŒê·€ë¥¼ í†µí•œ íŠ¹ì„±ì„ íƒ(Feature Selection) ê³¼ì •ì„ ì´í•´í•˜ë©° ì •ê·œí™”(regularization)ì„ ìœ„í•œ Ridge íšŒê·€ëª¨ë¸ì„ ì´í•´í•˜ê³  ì‚¬ìš©í•œë‹¤.
categories:
 - Machine Learning
tags: [Machine Learning, Supervised Learning, Ridge Regression, One-hot encoding, Feature Selection, regularization, ë¦¿ì§€íšŒê·€, ì›í•«ì¸ì½”ë”©, íŠ¹ì„±ì„ íƒ, ì •ê·œí™”]
mathjax: enable
# 0ï¸âƒ£1ï¸âƒ£2ï¸âƒ£3ï¸âƒ£4ï¸âƒ£5ï¸âƒ£6ï¸âƒ£7ï¸âƒ£8ï¸âƒ£9ï¸âƒ£ğŸ”Ÿ
---

# Ridge Regression
- ridge regressionì„ í†µí•´ì„œ biasë¥¼ ì•½ê°„ ë†’ì´ê²Œ ë˜ë©´ ì–´ë–¤ íš¨ê³¼ë¥¼ ì–»ê²Œ ë˜ë‚˜ìš”?
- ëŒë‹¤(íŒ¨ë„í‹°) ê°’ì„ í¬ê²Œ ì¡ìœ¼ë©´ ì–´ë–¤ íš¨ê³¼ê°€ ìˆë‚˜ìš”?

# One-hot encoding

## ë°ì´í„° ì¤€ë¹„

```py
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.DataFrame({
    'City': ['Seoul', 'Seoul', 'Seoul', 'Busan', 'Busan', 'Busan', 'Incheon', 'Incheon', 'Seoul', 'Busan', 'Incheon'],
    'Room': [3, 4, 3, 2, 3, 3, 3, 3, 3, 3, 2],
    'Price': [55000, 61000, 44000, 35000, 53000, 45000, 32000, 51000, 50000, 40000, 30000]
})

df
```

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-11 14 05 55](https://user-images.githubusercontent.com/79494088/128972580-ea8cc64d-bca7-4c7f-ae45-7eb4314bf3c9.png)

- City Columnì— ìˆëŠ” ë°ì´í„°ëŠ” ë„ì‹œ ì§€ì—­ì„ êµ¬ë¶„í•˜ëŠ” ë²”ì£¼í˜• ë³€ìˆ˜(Categorical vaiable)ì´ë‹¤.
- ë²”ì£¼í˜• ìë£Œ
  - ìˆœì„œê°€ ì—†ëŠ” ëª…ëª©í˜•(Nominal)
  - ìˆœì„œê°€ ìˆëŠ” ìˆœì„œí˜•(Ordinal)
- ë„ì‹œëŠ” ë†’ê³  ë‚®ìŒì´ ì—†ìœ¼ë¯€ë¡œ ëª…ëª©í˜• ë²”ì£¼í˜• ë³€ìˆ˜ì´ë‹¤.

### ë²”ì£¼í˜• ë³€ìˆ˜ë¥¼ ì›í•«ì¸ì½”ë”©ìœ¼ë¡œ ë³€í™˜

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-11 14 08 15](https://user-images.githubusercontent.com/79494088/128972771-efd2fc89-440a-4c9e-90f7-091a22826172.png)

- colorë³€ìˆ˜ë¥¼ ì›í•« ì¸ì½”ë”©ìœ¼ë¡œ ë³€í™˜í•´ ê°€ëŠ¥í•œ ëª¨ë“  ë²”ì£¼ë¥¼ ì—´ë¡œ ë‚˜íƒ€ëƒ„

```py
# Cityì˜ ë²”ì£¼ ë¹„ìœ¨
df['City'].value_counts(normalize=True)

'''
Seoul      0.363636
Busan      0.363636
Incheon    0.272727
Name: City, dtype: float64
'''

# seaborn countplot, City
sns.countplot(x=df['City']);
```

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-11 14 10 54](https://user-images.githubusercontent.com/79494088/128972964-d99687e6-b6f0-465f-ae6c-59ac80f76e68.png)

```py
# Price í‰ê· 
df['Price'].mean()

# 45090.90909090909

# City ê° ë²”ì£¼ì— ëŒ€í•œ Price ê°’
df.groupby('City')['Price'].mean()

'''
City
Busan      43250.000000
Incheon    37666.666667
Seoul      52500.000000
Name: Price, dtype: float64
'''

# aggregationì„ ì‚¬ìš©í•´ City ê° ë²”ì£¼ì— ëŒ€í•œ ì—¬ëŸ¬ í†µê³„ëŸ‰ì„ ë³¼ ìˆ˜ ìˆë‹¤.(min, max, mean, median)
df.groupby('City')['Price'].agg(['min','max','mean','median'])
```
![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-11 14 12 50](https://user-images.githubusercontent.com/79494088/128973097-dafb41d6-b5aa-49e9-847f-37b7e72a6ac5.png)

- ì›í•«ì¸ì½”ë”©ì„ ìˆ˜í–‰í•˜ë©´ ê° ì¹´í…Œê³ ë¦¬ì— í•´ë‹¹í•˜ëŠ” ë³€ìˆ˜ë“¤ì´ ëª¨ë‘ ì°¨ì›ì— ë”í•´ì§€ê²Œ ëœë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ì¹´í…Œê³ ë¦¬ê°€ ë„ˆë¬´ ë§ì€ ê²½ìš°(high cardinality)ì—ëŠ” ì‚¬ìš©í•˜ê¸° ì í•©í•˜ì§€ ì•Šë‹¤.

```py
# ë¶ˆí•„ìš”í•œ ìš”ì†Œë¥¼ ì—†ì¸ ë”ë¯¸ ì½”ë”©
df_dum = pd.get_dummies(df, prefix=['City'], drop_first=True)
df_dum
```

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-11 14 15 41](https://user-images.githubusercontent.com/79494088/128973353-e3d8e9e1-f393-484a-b388-2bc839714639.png)

## ì„ í˜•íšŒê·€ëª¨ë¸ ì›í•«ì¸ì½”ë”© íŠ¹ì„± ì‚¬ìš©

```py
# import LinearRegression
from  sklearn.linear_model import LinearRegression

# Initialize LinearRegression
model_oh = LinearRegression()

# ëª¨ë¸ í•™ìŠµ(fit)
model_oh.fit(df_oh[['City_Seoul','City_Busan','City_Incheon']], df_oh['Price'])

# model coef_, intercept_
print("coefficients: ", model_oh.coef_)
print("intercept:" , model_oh.intercept_)

'''
coefficients:  [ 8027.77777778 -1222.22222222 -6805.55555556]
intercept: 44472.22222222222
'''

# ë”ë¯¸ ì½”ë”©
model_dum = LinearRegression()
model_dum.fit(df_oh[['City_Seoul', 'City_Incheon']], df_oh['Price'])
print("coefficient: ", model_dum.coef_)
print("intercept: ", model_dum.intercept_)

'''
coefficient:  [ 9250.         -5583.33333333]
intercept:  43250.0
'''

import plotly.express as px
px.scatter(
    df_oh,
    x='City_Seoul',
    y='Price',
    trendline='ols'
)
```

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-11 14 21 42](https://user-images.githubusercontent.com/79494088/128973816-6639e431-2a1f-4650-b81f-5360fc58c0c4.png)

```py
import plotly.express as px
px.scatter(
    df_oh,
    x='City_Busan',
    y='Price',
    trendline='ols'
)
```

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-11 14 23 57](https://user-images.githubusercontent.com/79494088/128973993-dd53444a-7303-49b3-8963-01657059bf96.png)

### Category_encoders
- Category_encoders ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ë©´ ë²”ì£¼í˜• ë°ì´í„°ì—ë§Œ('City') ì›í•«ì¸ì½”ë”©ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤.

```py
features = ['City','Room']
target = 'Price'

# ì´ë²ˆì—ëŠ” í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë¶„ë¦¬
X_train = df[features][:8]
y_train = df[target][:8]
X_test = df[features][8:]
y_test = df[target][8:]

# ì¹´í…Œê³ ë¦¬ê°’ì„ ì›í•«ì¸ì½”ë”©ì„ ì‚¬ìš©í•˜ì—¬ ìˆ«ìë¡œ ë³€í™˜

# import OneHotEncoder
from category_encoders import OneHotEncoder

## ì›í•« ì¸ì½”ë”©
encoder = OneHotEncoder(use_cat_names = True)
X_train = encoder.fit_transform(X_train)
X_test = encoder.transform(X_test)

# category_encodersë¥¼ ì‚¬ìš©í•˜ë©´ ë²”ì£¼í˜•ë³€ìˆ˜ë¥¼ ê°€ì§„ íŠ¹ì„±ë§Œ ì›í•«ì¸ì½”ë”©ì„ ìˆ˜í–‰

# show X_train
X_train.head()
```

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-11 14 29 21](https://user-images.githubusercontent.com/79494088/128974475-756f7709-7540-422c-aa0f-2015ff0de32e.png)

```py
## show X_test
X_test
```

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-11 14 29 33](https://user-images.githubusercontent.com/79494088/128974497-7017d6ea-6804-447e-8e6a-3ee6fd0599bf.png)

# íŠ¹ì„± ì„ íƒ(Feature Selection)
- íŠ¹ì„±ê³µí•™ : ê³¼ì œì— ì í•©í•œ íŠ¹ì„±ì„ ë§Œë“¤ì–´ ë‚´ëŠ” ê³¼ì •ì´ë‹¤. ì´ í”„ë¡œì„¸ìŠ¤ëŠ” ì‹¤ë¬´ í˜„ì¥ì—ì„œ ê°€ì¥ ë§ì€ ì‹œê°„ì´ ì†Œìš”ë˜ëŠ” ì‘ì—… ì¤‘ í•˜ë‚˜
- SelectKBestë¥¼ ì‚¬ìš©í•˜ë©´ íŠ¹ì„±ì´ ë§ì•„ë„ ì‰½ê²Œ íƒ€ê²Ÿê°’ê³¼ ê°€ì¥ ìƒê´€ê´€ê³„ë¥¼ ë†’ê²Œ ê°€ì§€ëŠ” ê²ƒë“¤ì„ ì„ íƒí•  ìˆ˜ ìˆë‹¤.

## ë°ì´í„° ì¤€ë¹„

```py
# import house data
df = pd.read_csv('https://ds-lecture-data.s3.ap-northeast-2.amazonaws.com/kc_house_data/kc_house_data.csv')

# to_datetimeì„ í†µí•´ ì‹œê°„ê³¼ ë‚ ì§œë¥¼ ë‹¤ë£¨ê¸° ì‰¬ìš´ datetime64 í˜•íƒœë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
df['date'] = pd.to_datetime(df['date'])

# displot, 'price'
sns.displot(df['price']);
```

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-11 14 32 28](https://user-images.githubusercontent.com/79494088/128974740-6f1b5ae9-1ea2-4920-81d8-1f3e92da19d8.png)

```py
# np.percentile ì‚¬ìš©í•´ price ê°’ì´ ìƒìœ„ 5%, í•˜ìœ„ 5%ì¸ ë°ì´í„°ë¥¼ ì‚­ì œ
df = df[(df['price'] >= np.percentile(df['price'], 5)) & 
        (df['price'] <= np.percentile(df['price'], 95))] 

# displot, 'price'
sns.displot(df['price']);
```

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-11 14 34 31](https://user-images.githubusercontent.com/79494088/128974927-d9edb901-d9ba-4c5d-9f0c-389e200b5780.png)

```py
# 2015-03-01ì„ ê¸°ì¤€ìœ¼ë¡œ í›ˆë ¨/í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¥¼ ë¶„ë¦¬í•©ë‹ˆë‹¤.
cutOff = pd.to_datetime('2015-03-01')
train = df[df['date'] < cutOff]
test  = df[df['date'] >= cutOff]

# train/test shape
train.shape, test.shape

# ((16772, 21), (4721, 21))
```

## ìƒˆë¡œìš´ íŠ¹ì„±ì„ ìƒì„±í•˜ê³  ì‚­ì œ

```py
def engineer_features(X):

  # pandas.DataFrame.copy()
  X = X.copy() # ì–•ì€ ë³µì‚¬
    
  # ìš•ì‹¤ ê°¯ìˆ˜ì„ ì •ìˆ˜í˜•ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
  X['bathrooms'] = X['bathrooms'].round(0).astype(int)

  # ì´ ë°© ìˆ˜ë¥¼ í•©í•˜ì—¬ roomsë¡œ í•©ì³ ë´…ì‹œë‹¤.
  X['rooms'] = X['bedrooms'] + X['bathrooms']

  # ì‚¬ìš©í•˜ì§€ ì•Šì„ íŠ¹ì„±ì„ ì‚­ì œí•©ë‹ˆë‹¤.
  X = X.drop(['id', 'date', 'waterfront'],axis=1)

  return X

train = engineer_features(train)
test = engineer_features(test)

# ì„ íƒ ê°€ëŠ¥í•œ íŠ¹ì„±ë“¤ì˜ ê°€ì§€ìˆ˜ë¥¼ ê°€ì§€ìˆ˜ë¥¼ ê³„ì‚°í•œë‹¤.
# íŠ¹ì„±ì€ ê°€ì§“ìˆ˜ê°€ ëŠ˜ì–´ë‚ ìˆ˜ë¡ ê·¸ ì¡°í•©ì´ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ëŠ˜ì–´ë‚œë‹¤.

from math import factorial

n = len(train.columns)

def n_choose_k(n, k):
    return factorial(n)/(factorial(k)*factorial(n-k))

combinations = sum(n_choose_k(n,k) for k in range(1,n+1))

combinations

# 524287.0
```

- ê°€ëŠ¥í•œ ë°©ë²•ì´ ë§ì€ë° í•„ìš”í•œ ì¢‹ì€ íŠ¹ì„±ì„ ë½‘ê¸°ì—”, ìˆ˜ì‘ì—…ìœ¼ë¡œ í•  ìˆ˜ ì—†ìœ¼ë‹ˆ SelectKBestë¥¼ ì‚¬ìš©í•´ì„œ ìœ ìš©í•œ íŠ¹ì„±ë“¤ì„ ì°¾ì•„ë³¸ë‹¤.
- ì¢‹ì€ íŠ¹ì„±ì„ ë½‘ëŠ” ë°©ë²• : íŠ¹ì„±ë“¤ ë¼ë¦¬ëŠ” ìƒê´€ì„±ì´ ì ìœ¼ë©´ì„œ íƒ€ê²Ÿ íŠ¹ì„±ê³¼ëŠ” ìƒê´€ê´€ê³„ê°€ í° ê²ƒì„ ë½‘ëŠ”ë‹¤.

## ë°ì´í„°ë¥¼ í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ë¶„ë¦¬

```py
target = 'price'

## X_train, y_train, X_test, y_test ë°ì´í„°ë¡œ ë¶„ë¦¬
X_train = train.drop(columns=target)
y_train = train[target]
X_test = test.drop(columns=target)
y_test = test[target]
```

## SelectKBest ì´ìš© íš¨ê³¼ì ì¸ íŠ¹ì„± Kê°œ ê³¨ë¼ë‚¸ë‹¤.

```py
# target(Price)ì™€ ê°€ì¥ correlated ëœ features ë¥¼ kê°œ ê³ ë¥´ëŠ” ê²ƒì´ ëª©í‘œì…ë‹ˆë‹¤.
# ë…ë¦½ì ìœ¼ë¡œ ë½‘íˆê¸° ë•Œë¬¸ì— ê³¼ì í•©ì„ ì¤„ì´ê³  ëª¨ë¸ ì„±ëŠ¥ì„ ë†’ì¸ë‹¤.
# íŠ¹ì„± ê°œìˆ˜ê°€ ì¤„ì–´ì„œ í›ˆë ¨ì‹œê°„ë„ ê°ì†Œí•œë‹¤. 

# f_regresison, SelectKBest
from sklearn.feature_selection import f_regression, SelectKBest

# selctor ì •ì˜í•©ë‹ˆë‹¤.
selector = SelectKBest(score_func=f_regression, k=10)

# í•™ìŠµë°ì´í„°ì— fit_transform 
X_train_selected = selector.fit_transform(X_train, y_train)

# í…ŒìŠ¤íŠ¸ ë°ì´í„°ëŠ” transform
X_test_selected = selector.transform(X_test)


X_train_selected.shape, X_test_selected.shape

# ((16772, 10), (4721, 10))
```

## ì„ íƒëœ íŠ¹ì„±

```py
all_names = X_train.columns

## selector.get_support()
selected_mask = selector.get_support()

## ì„ íƒëœ íŠ¹ì„±ë“¤
selected_names = all_names[selected_mask]

## ì„ íƒë˜ì§€ ì•Šì€ íŠ¹ì„±ë“¤
unselected_names = all_names[~selected_mask] 

print('Selected names: ', selected_names)
print('Unselected names: ', unselected_names)

'''
Selected names: Index(['bedrooms', 'bathrooms', 'sqft_living', 'view', 'grade', 'sqft_above',
                'sqft_basement', 'lat', 'sqft_living15', 'rooms'],
                dtype='object')
Unselected names: Index(['sqft_lot', 'floors', 'condition', 'yr_built', 'yr_renovated',
                  'zipcode', 'long', 'sqft_lot15'],
                  dtype='object')
'''
```

## íŠ¹ì„±ì˜ ìˆ˜ k ë¥¼ ì–´ë–»ê²Œ ê²°ì •í•˜ëŠ”ê²Œ ì¢‹ì„ê¹Œ?

```py
# featuresë¥¼ ëª‡ ê°œ ì„ ì±…í•˜ëŠ” ê²ƒì´ ì¢‹ì€ì§€ ì•Œì•„ ë´…ì‹œë‹¤.

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, r2_score

training = []
testing = []
ks = range(1, len(X_train.columns)+1)

# 1 ë¶€í„° íŠ¹ì„± ìˆ˜ ë§Œí¼ ì‚¬ìš©í•œ ëª¨ë¸ì„ ë§Œë“¤ì–´ì„œ MAE ê°’ì„ ë¹„êµ í•©ë‹ˆë‹¤.
for k in range(1, len(X_train.columns)+ 1):
    print(f'{k} features')
    
    selector = SelectKBest(score_func=f_regression, k=k)
    
    X_train_selected = selector.fit_transform(X_train, y_train)
    X_test_selected = selector.transform(X_test)
    
    all_names = X_train.columns
    selected_mask = selector.get_support()
    selected_names = all_names[selected_mask]
    print('Selected names: ', selected_names)

    
    model = LinearRegression()
    model.fit(X_train_selected, y_train)
    y_pred = model.predict(X_train_selected)
    mae = mean_absolute_error(y_train, y_pred)
    training.append(mae)
    
    y_pred = model.predict(X_test_selected)
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    testing.append(mae)
    print(f'Test MAE: ${mae:,.0f}')
    print(f'Test R2: {r2} \n')

plt.plot(ks, training, label='Training Score', color='b')
plt.plot(ks, testing, label='Testing Score', color='g')
plt.ylabel("MAE ($)")
plt.xlabel("Number of Features")
plt.title('Validation Curve')
plt.legend()
plt.show()

'''
1 features
Selected names:  Index(['sqft_living'], dtype='object')
Test MAE: $167,321
Test R2: 0.4296149194220933 

2 features
Selected names:  Index(['sqft_living', 'grade'], dtype='object')
Test MAE: $157,239
Test R2: 0.4884712916259375 

3 features
Selected names:  Index(['sqft_living', 'grade', 'sqft_living15'], dtype='object')
Test MAE: $156,951
Test R2: 0.49204137332086395 

4 features
Selected names:  Index(['sqft_living', 'grade', 'sqft_above', 'sqft_living15'], dtype='object')
Test MAE: $154,920
Test R2: 0.5019286655041775 

5 features
Selected names:  Index(['bathrooms', 'sqft_living', 'grade', 'sqft_above', 'sqft_living15'], dtype='object')
Test MAE: $154,979
Test R2: 0.5020209934516053 

6 features
Selected names:  Index(['bathrooms', 'sqft_living', 'grade', 'sqft_above', 'sqft_living15',
       'rooms'],
      dtype='object')
Test MAE: $154,376
Test R2: 0.5051572843210005 

7 features
Selected names:  Index(['bathrooms', 'sqft_living', 'view', 'grade', 'sqft_above',
       'sqft_living15', 'rooms'],
      dtype='object')
Test MAE: $149,839
Test R2: 0.532627969843283 

8 features
Selected names:  Index(['bathrooms', 'sqft_living', 'view', 'grade', 'sqft_above', 'lat',
       'sqft_living15', 'rooms'],
      dtype='object')
Test MAE: $126,250
Test R2: 0.6266392465899573 

9 features
Selected names:  Index(['bedrooms', 'bathrooms', 'sqft_living', 'view', 'grade', 'sqft_above',
       'lat', 'sqft_living15', 'rooms'],
      dtype='object')
Test MAE: $126,250
Test R2: 0.6266392465899552 

10 features
Selected names:  Index(['bedrooms', 'bathrooms', 'sqft_living', 'view', 'grade', 'sqft_above',
       'sqft_basement', 'lat', 'sqft_living15', 'rooms'],
      dtype='object')
Test MAE: $126,250
Test R2: 0.6266392465899593 

11 features
Selected names:  Index(['bedrooms', 'bathrooms', 'sqft_living', 'floors', 'view', 'grade',
       'sqft_above', 'sqft_basement', 'lat', 'sqft_living15', 'rooms'],
      dtype='object')
Test MAE: $126,257
Test R2: 0.6273262157764277 

12 features
Selected names:  Index(['bedrooms', 'bathrooms', 'sqft_living', 'floors', 'view', 'grade',
       'sqft_above', 'sqft_basement', 'yr_renovated', 'lat', 'sqft_living15',
       'rooms'],
      dtype='object')
Test MAE: $125,801
Test R2: 0.6313660283782238 

13 features
Selected names:  Index(['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'view',
       'grade', 'sqft_above', 'sqft_basement', 'yr_renovated', 'lat',
       'sqft_living15', 'rooms'],
      dtype='object')
Test MAE: $125,916
Test R2: 0.6308283765247964 

14 features
Selected names:  Index(['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'view',
       'grade', 'sqft_above', 'sqft_basement', 'yr_renovated', 'lat',
       'sqft_living15', 'sqft_lot15', 'rooms'],
      dtype='object')
Test MAE: $125,920
Test R2: 0.6314307726640978 

15 features
Selected names:  Index(['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'view',
       'grade', 'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated',
       'lat', 'sqft_living15', 'sqft_lot15', 'rooms'],
      dtype='object')
Test MAE: $119,578
Test R2: 0.6677358410398033 

16 features
Selected names:  Index(['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'view',
       'grade', 'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated',
       'zipcode', 'lat', 'sqft_living15', 'sqft_lot15', 'rooms'],
      dtype='object')
Test MAE: $119,295
Test R2: 0.6697817823178407 

17 features
Selected names:  Index(['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'view',
       'condition', 'grade', 'sqft_above', 'sqft_basement', 'yr_built',
       'yr_renovated', 'zipcode', 'lat', 'sqft_living15', 'sqft_lot15',
       'rooms'],
      dtype='object')
Test MAE: $118,769
Test R2: 0.671066324876272 

18 features
Selected names:  Index(['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'view',
       'condition', 'grade', 'sqft_above', 'sqft_basement', 'yr_built',
       'yr_renovated', 'zipcode', 'lat', 'long', 'sqft_living15', 'sqft_lot15',
       'rooms'],
      dtype='object')
Test MAE: $118,992
Test R2: 0.6750956927563698 
'''
```

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-11 15 00 29](https://user-images.githubusercontent.com/79494088/128977357-40ed8731-e468-477b-bb83-37d91322f392.png)

- 15ê°œë¥¼ ì‚¬ìš©í–ˆì„ ë•Œì™€ 18ê°œë¥¼ ì‚¬ìš©í–ˆì„ ë•Œê°€ ê±°ì˜ ì°¨ì´ê°€ ë‚˜ì§€ ì•Šìœ¼ë¯€ë¡œ Costë¥¼ ìƒê°í•´ì„œ 

# Ridge Regression ëª¨ë¸ í•™ìŠµ
- Ridge íšŒê·€ëŠ” ê¸°ì¡´ ë‹¤ì¤‘íšŒê·€ì„ ì„ í›ˆë ¨ë°ì´í„°ì— ëœ ì í•©ì´ ë˜ë„ë¡ ë§Œë“ ë‹¤. ê·¸ ê²°ê³¼ë¡œ ë” ì¢‹ì€ ëª¨ë¸ì´ ë§Œë“¤ì–´ì§„ë‹¤.

$$\beta_{ridge}$:  $argmin[\sum_{i=1}^n(y_i - \beta_0 - \beta_1x_{i1}-\dotsc-\beta_px_{ip})^2 + \lambda\sum_{j=1}^p\beta_j^2]$$

- n: ìƒ˜í”Œìˆ˜, p: íŠ¹ì„±ìˆ˜, $\lambda$: íŠœë‹ íŒŒë¼ë¯¸í„°(íŒ¨ë„í‹°)
ì°¸ê³ : alpha, lambda, regularization parameter, penalty term ëª¨ë‘ ê°™ì€ ëœ»

## Ridge Regressionì„ ì‚¬ìš©í•˜ëŠ” ì´ìœ 
- ê³¼ì í•©ì„ ì¤„ì´ê¸° ìœ„í•´ì„œ ì‚¬ìš©í•œë‹¤.
- ê³¼ì í•©ì„ ì¤„ì´ëŠ” ê°„ë‹¨í•œ ë°©ë²• ì¤‘ í•œ ê°€ì§€ëŠ” ëª¨ë¸ì˜ ë³µì¡ë„ë¥¼ ì¤„ì¸ë‹¤.
- íŠ¹ì„±ì˜ ê°¯ìˆ˜ë¥¼ ì¤„ì´ê±°ë‚˜ ëª¨ë¸ì„ ë‹¨ìˆœí•œ ëª¨ì–‘ìœ¼ë¡œ ì í•©í•˜ëŠ” ê²ƒ.
- ì´ í¸í–¥ì„ ì¡°ê¸ˆ ë”í•˜ê³ , ë¶„ì‚°ì„ ì¤„ì´ëŠ” ë°©ë²•ìœ¼ë¡œ ì •ê·œí™”(Regularization)ë¥¼ ìˆ˜í–‰í•œë‹¤.
- ì •ê·œí™”ëŠ” ëª¨ë¸ì„ ë³€í˜•í•˜ì—¬ ê³¼ì í•©ì„ ì™„í™”í•´ ì¼ë°˜í™” ì„±ëŠ¥ì„ ë†’ì—¬ì£¼ê¸° ìœ„í•œ ê¸°ë²•ì„ ë§í•œë‹¤.
- ì •ê·œí™”ì˜ ê°•ë„ë¥¼ ì¡°ì ˆí•´ì£¼ëŠ” íŒ¨ë„í‹°ê°’ì¸ ëŒë‹¤ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì„±ì§ˆì´ ìˆìŠµë‹ˆë‹¤.
  - $\lambda$ â†’ 0,   $\beta_{ridge}$ â†’ $\beta_{OLS}$
  - $\lambda$ â†’ âˆ,   $\beta_{ridge}$ â†’ 0.

## OLS vs Ridge

```py
import seaborn as sns
ans = sns.load_dataset('anscombe').query('dataset=="III"')
ans.plot.scatter('x', 'y');
```

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-11 15 09 35](https://user-images.githubusercontent.com/79494088/128978258-24d91b5f-542b-4224-abc3-2b6ebf4bbd97.png)

### OLS

```py
%matplotlib inline

ax = ans.plot.scatter('x', 'y')

# OLS 
ols = LinearRegression()
ols.fit(ans[['x']], ans['y'])

# íšŒê·€ê³„ìˆ˜ì™€ interceptë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
m = ols.coef_[0].round(2)
b = ols.intercept_.round(2)
title = f'Linear Regression \n y = {m}x + {b}'

# í›ˆë ¨ ë°ì´í„°ë¡œ ì˜ˆì¸¡ì„ í•©ë‹ˆë‹¤.
ans['y_pred'] = ols.predict(ans[['x']])

ans.plot('x', 'y_pred', ax=ax, title=title);
```

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-11 15 11 04](https://user-images.githubusercontent.com/79494088/128978386-823476ef-f799-434a-87be-e95b65e504de.png)

### Ridge Regression ì§„í–‰
- $\lambda$ ê°’ì„ ì¦ê°€ì‹œí‚¤ë©° ê·¸ë˜í”„ë¥¼ í†µí•´ íšŒê·€ê³„ìˆ˜ì˜ ë³€í™”ë¥¼ ì‚´í´ë³¸ë‹¤.

```py
import matplotlib.pyplot as plt
from sklearn.linear_model import Ridge

def ridge_anscombe(alpha):
    """
    alpha : lambda, penalty term
    """
    ans = sns.load_dataset('anscombe').query('dataset=="III"')

    ax = ans.plot.scatter('x', 'y')

    ridge = Ridge(alpha=alpha, normalize=True)
    ridge.fit(ans[['x']], ans['y'])

    # íšŒê·€ê³„ìˆ˜ì™€ interceptë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    m = ridge.coef_[0].round(2)
    b = ridge.intercept_.round(2)
    title = f'Ridge Regression, alpha={alpha} \n y = {m}x + {b}'

    # ì˜ˆì¸¡
    ans['y_pred'] = ridge.predict(ans[['x']])

    ans.plot('x', 'y_pred', ax=ax, title=title)
    plt.show()
    
# ì—¬ëŸ¬ ì•ŒíŒŒê°’ìœ¼ë¡œ ë°˜ë³µí•´ ê·¸ë˜í”„ë¥¼ ê·¸ë¦½ë‹ˆë‹¤.
alphas = np.arange(0, 2, 0.4)
for alpha in alphas:
    ridge_anscombe(alpha=alpha)
```

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-11 15 13 29](https://user-images.githubusercontent.com/79494088/128978646-945b5fcf-e20f-4c2c-8c49-c7e09037efcf.png)

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-11 15 13 57](https://user-images.githubusercontent.com/79494088/128978690-62d0b9c9-3504-49df-8a44-c3f200d1785c.png)

- ê·¸ë˜í”„ë¥¼ ë³´ë©´, alpha = 0ì¸ ê²½ìš°ì—ëŠ” OLS ì™€ ê°™ì€ ê·¸ë˜í”„ í˜•íƒœë¡œ ê°™ì€ ëª¨ë¸ ì„ì„ í™•ì¸ í•  ìˆ˜ ìˆê³ . alpha ê°’ì´ ì»¤ì§ˆ ìˆ˜ë¡ ì§ì„ ì˜ ê¸°ìš¸ê¸°ê°€ 0ì— ê°€ê¹Œì›Œ ì§€ë©´ì„œ í‰ê·  ê¸°ì¤€ëª¨ë¸(baseline)ê³¼ ë¹„ìŠ·í•´ì§€ëŠ” ëª¨ìŠµì„ ë³¼ ìˆ˜ ìˆë‹¤.
- ì´ íŒ¨ë„í‹°ê°’ì„ ë³´ë‹¤ íš¨ìœ¨ì ìœ¼ë¡œ êµ¬í•  ìˆ˜ ìˆëŠ” ë°©ë²•
  - íŠ¹ë³„í•œ ê³µì‹ì´ ìˆëŠ” ê²ƒì€ ì•„ë‹ˆë©°, ì—¬ëŸ¬ íŒ¨ë„í‹° ê°’ì„ ê°€ì§€ê³  ê²€ì¦ì‹¤í—˜ í•´ ë³´ëŠ” ë°©ë²•ì„ ì‚¬ìš©í•œë‹¤.
  - êµì°¨ê²€ì¦(Cross-validation)ì„ ì‚¬ìš©í•´ í›ˆë ¨/ê²€ì¦ ë°ì´í„°ë¥¼ ë‚˜ëˆ„ì–´ ê²€ì¦ì‹¤í—˜ì„ ì§„í–‰í•˜ë©´ ëœë‹¤.

#### RidgeCVë¥¼ í†µí•œ ìµœì  íŒ¨ë„í‹°(alpha, lambda) ê²€ì¦

```py
from sklearn.linear_model import RidgeCV

alphas = [0.01, 0.05, 0.1, 0.2, 1.0, 10.0, 100.0]

ridge = RidgeCV(alphas=alphas, normalize=True, cv=3)
ridge.fit(ans[['x']], ans['y'])
print("alpha: ", ridge.alpha_)
print("best score: ", ridge.best_score_)

'''
alpha:  0.2
best score:  0.4389766255562206
'''
```

- Ridge íšŒê·€ ì§ì„ ì˜ ìƒê¹€ìƒˆëŠ” OLSì™€ ë§¤ìš° ë¹„ìŠ·í•˜ì§€ë§Œ ì´ìƒì¹˜(outlier) ì˜í–¥

```py
ax = ans.plot.scatter('x', 'y')

m = ridge.coef_[0].round(2)
b = ridge.intercept_.round(2)
title = f'Ridge Regression, alpha={ridge.alpha_} \n y = {m}x + {b}'

ans['y_pred'] = ridge.predict(ans[['x']])

ans.plot('x', 'y_pred', ax=ax, title=title)
plt.show()
```

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-11 15 31 09](https://user-images.githubusercontent.com/79494088/128980513-7e530835-66d4-4de3-8064-162319128783.png)

#### House Dataì—ì„œ í™•ì¸
- ì§‘ê°’ ì˜ˆì¸¡ ë°ì´í„°ë¡œ ëŒì•„ì™€ì„œ, ridge íšŒê·€ì˜ íŒ¨ë„í‹°ì— ëŒ€í•œ ì˜í–¥ì„ í™•ì¸í•œë‹¤. íŠ¹ì„±ì€ sqft_livingì„ ì‚¬ìš©í•œë‹¤.

```py
# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html
for alpha in [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]:
    
    feature = 'sqft_living'
    print(f'Ridge Regression, with alpha={alpha}')
    model = Ridge(alpha=alpha, normalize=True)
    model.fit(X_train[[feature]], y_train)

    
    # Get Test MAE
    y_pred = model.predict(X_test[[feature]])    
    mae = mean_absolute_error(y_test, y_pred)
    print(f'Test MAE: ${mae:,.0f}')
    
    train.plot.scatter(feature, target, alpha=0.1)
    plt.plot(X_test[feature], y_pred, color='green')
    plt.show()
```

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-11 15 33 44](https://user-images.githubusercontent.com/79494088/128980836-a473921b-28ee-48f5-bbcc-fa40c00be9e4.png)

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-11 15 34 14](https://user-images.githubusercontent.com/79494088/128980886-8b45cb6c-a545-4562-90b0-1defd3f0f133.png)

##### ì—¬ëŸ¬ íŠ¹ì„±ì„ ì‚¬ìš©í•´ì„œ Ridge íšŒê·€ë¥¼ í•™ìŠµ

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-11 15 37 48](https://user-images.githubusercontent.com/79494088/128981258-f1bb8dbc-2516-4082-8799-29b22d03cd1b.png)

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-11 15 38 36](https://user-images.githubusercontent.com/79494088/128981332-034d9986-a783-44ad-900c-577da9001510.png)

##### ë‹¤í•­í•¨ìˆ˜ì— Ridge íšŒê·€ ì ìš©
- ë‹¤ìˆ˜ì˜ íŠ¹ì„±ì„ ì‚¬ìš©í•˜ëŠ” ë‹¤í•­í•¨ìˆ˜ì— Ridge íšŒê·€ë¥¼ ì‚¬ìš©í•˜ë©´ ì •ê·œí™” íš¨ê³¼ë¥¼ ë” ì˜ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

```py
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline

def RidgeRegression(degree=3, **kwargs):
    return make_pipeline(PolynomialFeatures(degree), 
                         Ridge(**kwargs))


for alpha in [0.001, 0.01, 0.0025, 0.05, 0.09, 0.12, 0.4, 1.0, 1, 5, 10, 100]:
        
    print(f'Ridge Regression, alpha={alpha}')

    # Ridge ëª¨ë¸ í•™ìŠµ
    model = RidgeRegression(alpha=alpha, normalize=True)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    # MAE for test
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    print(f'Test MAE: ${mae:,.0f}')
    print(f'R2 Score: {r2:,.4f}\n')

coefs = model.named_steps["ridge"].coef_
print(f'Number of Features: {len(coefs)}')

'''
Ridge Regression, alpha=0.001
Test MAE: $115,090
R2 Score: -0.5563

Ridge Regression, alpha=0.01
Test MAE: $112,684
R2 Score: 0.5540

Ridge Regression, alpha=0.0025
Test MAE: $113,995
R2 Score: 0.0050

Ridge Regression, alpha=0.05
Test MAE: $111,764
R2 Score: 0.6929

Ridge Regression, alpha=0.09
Test MAE: $111,875
R2 Score: 0.6943

Ridge Regression, alpha=0.12
Test MAE: $112,023
R2 Score: 0.6936

Ridge Regression, alpha=0.4
Test MAE: $112,310
R2 Score: 0.6928

Ridge Regression, alpha=1.0
Test MAE: $112,566
R2 Score: 0.6914

Ridge Regression, alpha=1
Test MAE: $112,566
R2 Score: 0.6914

Ridge Regression, alpha=5
Test MAE: $116,650
R2 Score: 0.6680

Ridge Regression, alpha=10
Test MAE: $121,811
R2 Score: 0.6442

Ridge Regression, alpha=100
Test MAE: $150,360
R2 Score: 0.5087

Number of Features: 1330
'''
```

### ìµœì¢…ëª¨ë¸
- ìµœì¢… ëª¨ë¸ì„ ë§Œë“¤ê¸° ìœ„í•´ì„œëŠ” ê°€ì§€ê³  ìˆëŠ” ë°ì´í„°ë¥¼ ë‹¤ ì‚¬ìš©í•´ ìµœì ì˜ ëª¨ë¸ì„ ë§Œë“¤ì–´ì•¼ í•œë‹¤.
- ì§€ê¸ˆ ê°€ì§€ê³  ìˆëŠ” í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ê²€ì¦ ë°ì´í„°ë¡œ ì‚¬ìš©í•˜ë ¤ë©´ RidgeCVì— í›ˆë ¨ ë°ì´í„°ë¡œ í•¨ê»˜ ë„£ì–´ ì£¼ì–´ì•¼ í•œë‹¤. RidgeCVë‚´ì—ì„œ í›ˆë ¨ ë°ì´í„°ë¥¼ í›ˆë ¨/ê²€ì¦ ë°ì´í„°ë¡œ ë‚˜ëˆ„ì–´ ìµœê³  ìŠ¤ì½”ì–´ë¥¼ ê°€ì§€ëŠ” alphaë¥¼ ì°¾ì•„ì¤€ë‹¤.

```py
X_total = pd.concat([X_train, X_test])
y_total = pd.concat([y_train, y_test])

# ëª¨ë“  ë°ì´í„°ë¥¼ ì‚¬ìš©í•´ ìµœì¢… ëª¨ë¸ì„ ë§Œë“­ë‹ˆë‹¤.
model = RidgeCVRegression(alphas=alphas, normalize=True, cv=5)
model.fit(X_total, y_total)

coefs = model.named_steps["ridgecv"].coef_
print(f'Number of Features: {len(coefs)}')

print(f'alpha: {model.named_steps["ridgecv"].alpha_}')
print(f'cv best score: {model.named_steps["ridgecv"].best_score_}')

'''
Number of Features: 1330
alpha: 0.19
cv best score: 0.7255485997685633
'''

coefs.max(), coefs.mean()

# (70325.16065760396, 42.115772616436196)

# íšŒê·€ê³„ìˆ˜ë“¤ì„ ì •ë ¬
coefs.sort()

coefs

'''
array([-17325.27799422,  -1175.83656738,   -919.34030661, ...,
          715.54010685,    719.44854755,  70325.1606576 ])
'''
```

#### íšŒê·€ê³„ìˆ˜ ê·¸ë˜í”„
- ëª‡ëª‡ ì¤‘ìš”í•œ íŠ¹ì„±ë“¤ë§Œ íšŒê·€ê³„ìˆ˜ê°€ í¬ê³  ëŒ€ë¶€ë¶„ 0 ê·¼ì²˜ì— ìˆìŒì„ ë³¼ ìˆ˜ ìˆë‹¤.

```py
plt.plot(coefs)
```

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-11 15 51 42](https://user-images.githubusercontent.com/79494088/128982846-8be7d50a-7375-4373-b481-f54406c6f593.png)

