---
title: '[Tree Based Model] ëª¨ë¸ì„ íƒ(Model Selection)'
description: ëª¨ë¸ì„ íƒì„ ìœ„í•œ êµì°¨ê²€ì¦ ë°©ë²•ê³¼ í•˜ì´í¼íŒŒë¼ë¯¸í„°ì˜ ìµœì í™”ë¥¼ í†µí•œ ëª¨ë¸ ì„±ëŠ¥ í–¥ìƒ ì„¤ëª…
categories:
 - Machine Learning
tags: [Machine Learning, Model Selection, Cross Validation, êµì°¨ê²€ì¦, í•˜ì´í¼íŒŒë¼ë¯¸í„°, ëª¨ë¸ì„ íƒ]
mathjax: enable
# 0ï¸âƒ£1ï¸âƒ£2ï¸âƒ£3ï¸âƒ£4ï¸âƒ£5ï¸âƒ£6ï¸âƒ£7ï¸âƒ£8ï¸âƒ£9ï¸âƒ£ğŸ”Ÿ
---

# 1ï¸âƒ£ êµì°¨ê²€ì¦(Cross-Validation)
- ì´ ì „ í¬ìŠ¤íŒ… ê¹Œì§€ **í›ˆë ¨/ê²€ì¦/í…ŒìŠ¤íŠ¸** ì„¸íŠ¸ë¡œ ë‚˜ëˆ„ì–´ í•™ìŠµí–ˆë‹¤.
- ì´ ë°©ë²•ì„ **Hold-out êµì°¨ê²€ì¦**ì´ë¼ í•˜ëŠ”ë° ì´ ë°©ë²•ì˜ ë¬¸ì œì ì€
  - í•™ìŠµì— ì‚¬ìš©ê°€ëŠ¥í•œ ë°ì´í„°ê°€ ì¶©ë¶„í•˜ë‹¤ë©´ ë¬¸ì œê°€ ì—†ê² ì§€ë§Œ, í›ˆë ¨ì„¸íŠ¸ì˜ í¬ê¸°ê°€ ëª¨ë¸í•™ìŠµì— ì¶©ë¶„í•˜ì§€ ì•Šì„ ê²½ìš° ë¬¸ì œê°€ ë  ìˆ˜ ìˆë‹¤.
  - ê²€ì¦ì„¸íŠ¸ í¬ê¸°ê°€ ì¶©ë¶„íˆ í¬ì§€ ì•Šë‹¤ë©´ ì˜ˆì¸¡ ì„±ëŠ¥ì— ëŒ€í•œ ì¶”ì •ì´ ë¶€ì •í™•í•  ê²ƒì´ë‹¤.
- **ëª¨ë¸ ì„ íƒ(Model Selection)ë¬¸ì œ**
  - ë¬¸ì œë¥¼ í’€ê¸° ìœ„í•´ ì–´ë–¤ í•™ìŠµ ëª¨ë¸ì„ ì‚¬ìš©í•´ì•¼ í•  ê²ƒì¸ê°€?
  - ì–´ë–¤ **í•˜ì´í¼íŒŒë¼ë¯¸í„°**ë¥¼ ì‚¬ìš©í•  ê²ƒì¸ê°€?
- ë°ì´í„°ì˜ í¬ê¸°ì— ëŒ€í•œ ë¬¸ì œ, ëª¨ë¸ì„ íƒì— ëŒ€í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•˜ëŠ” ë°©ë²• ì¤‘ í•œê°€ì§€ëŠ” **êµì°¨ê²€ì¦**ì´ë‹¤.<br>
(êµì°¨ê²€ì¦ì€ ì‹œê³„ì—´(time series))ë°ì´í„°ì— ì í•©í•˜ì§€ ì•Šë‹¤.)

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-20 10 25 24](https://user-images.githubusercontent.com/79494088/130164422-9708bffd-c9f2-4721-88ce-bff8071f3fd3.png)

## êµì°¨ê²€ì¦ ì‚¬ìš©
- êµì°¨ê²€ì¦ì„ ìœ„í•´ì„œëŠ” ë°ì´í„°ë¥¼ kê°œë¡œ ë“±ë¶„í•´ì•¼ í•˜ëŠ”ë° ì´ë¥¼ k-fold cross validation(CV)ë¼ê³  í•œë‹¤.
- kê°œì˜ ì§‘í•©ì—ì„œ k-1 ê°œì˜ ë¶€ë¶„ì§‘í•©ì„ í›ˆë ¨ì— ì‚¬ìš©í•˜ê³  ë‚˜ë¨¸ì§€ ë¶€ë¶„ì§‘í•©ì„ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ê²€ì¦í•˜ê²Œ ëœë‹¤.
  - ë°ì´í„°ë¥¼ 3ë“±ë¶„í•˜ê³  ê²€ì¦ê³¼ í›ˆë ¨ì„¸íŠ¸ë¥¼ ì´ ì„¸ë²ˆ ë°”ê¾¸ì–´ê°€ë©° ê²€ì¦í•˜ëŠ” ê²ƒì€ 3-fold CV ì´ë‹¤.

### ì„ í˜•ëª¨ë¸ êµì°¨ê²€ì¦

```py
from category_encoders import OneHotEncoder
from sklearn.feature_selection import f_regression, SelectKBest
from sklearn.impute import SimpleImputer
from sklearn.linear_model import Ridge
from sklearn.model_selection import cross_val_score
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler

target = 'SalePrice'

features = train.columns.drop([target])

X_train = train[features]
y_train = train[target]

X_test = test[features]
y_test = test[target]


pipe = make_pipeline(
    OneHotEncoder(use_cat_names=True), 
    SimpleImputer(strategy='mean'), 
    StandardScaler(), 
    SelectKBest(f_regression, k=20),
    Ridge(alpha=1.0)
)

# 3-fold êµì°¨ê²€ì¦ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
k = 3
scores = cross_val_score(pipe, X_train, y_train, cv=k, 
                         scoring='neg_mean_absolute_error')

print(f'MAE ({k} folds):', -scores)
'''
MAE (3 folds): [19912.3716215  23214.74205495 18656.29713167]
'''

-scores.mean()
'''
20594.470269371817
'''

scores.std()
'''
1922.4635156881875
'''
```

### ëœë¤í¬ë ˆìŠ¤íŠ¸ êµì°¨ê²€ì¦

```py
from category_encoders import TargetEncoder
from sklearn.ensemble import RandomForestRegressor

pipe = make_pipeline(
    # TargetEncoder: ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”ë¡œ, íƒ€ê²Ÿê°’ì„ íŠ¹ì„±ì˜ ë²”ì£¼ë³„ë¡œ í‰ê· ë‚´ì–´ ê·¸ ê°’ìœ¼ë¡œ ì¸ì½”ë”©
    TargetEncoder(min_samples_leaf=1, smoothing=1),
    SimpleImputer(strategy='median'), 
    RandomForestRegressor(max_depth = 10, n_jobs=-1, random_state=2)
)

k = 3
scores = cross_val_score(pipe, X_train, y_train, cv=k, 
                         scoring='neg_mean_absolute_error')

print(f'MAE for {k} folds:', -scores)
-scores.mean()
'''
17018.19573706592
'''

scores.std()
'''
1797.7668143704145
'''
```

# 2ï¸âƒ£ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
- ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ ë§Œë“¤ ë•Œ ì¤‘ìš”í•œ ì´ìŠˆëŠ” **ìµœì í™”(Optimization)**ì™€, **ì¼ë°˜í™”(Generalization)**ì´ë‹¤.
  - **ìµœì í™”** : í›ˆë ¨ ë°ì´í„°ë¡œ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ì–»ê¸° ìœ„í•´ ëª¨ë¸ì„ ì¡°ì •í•˜ëŠ” ê³¼ì •
  - **ì¼ë°˜í™”** : í•™ìŠµëœ ëª¨ë¸ì´ ì²˜ìŒ ë³¸ ë°ì´í„°ì—ì„œ ì–¼ë§ˆë‚˜ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚´ëŠ” ì§€
- ëª¨ë¸ì˜ ë³µì¡ë„ë¥¼ ë†’ì´ëŠ” ê³¼ì •ì—ì„œ í›ˆë ¨/ê²€ì¦ ì„¸íŠ¸ì˜ ì†ì‹¤ì´ í•¨ê»˜ ê°ì†Œí•˜ëŠ” ì‹œì ì€ **ê³¼ì†Œì í•©(Underfitting)**ë˜ì—ˆë‹¤ê³  í•œë‹¤.
- ì–´ëŠ ì‹œì ë¶€í„° í›ˆë ¨ë°ì´í„°ì˜ ì†ì‹¤ì€ ê³„ì† ê°ì†Œí•˜ì§€ë§Œ, ê²€ì¦ë°ì´í„°ì˜ ì†ì‹¤ì´ ì¦ê°€í•˜ëŠ” ë•ŒëŠ” **ê³¼ì í•©(Overfitting)**ë˜ì—ˆë‹¤ê³  í•œë‹¤.
- **ì´ìƒì ì¸ ëª¨ë¸ì€ Underfittingê³¼ Overfitting ì‚¬ì´ì— ì¡´ì¬**í•œë‹¤.

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-20 10 36 20](https://user-images.githubusercontent.com/79494088/130165256-b3b1c571-1171-4667-9482-a8a5972a1540.png)

## ê²€ì¦ê³¡ì„ (Validation curve)
- ê²€ì¦ê³¡ì„  : í›ˆë ¨/ê²€ì¦ë°ì´í„°ì— ëŒ€í•´ yëŠ” score, xëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ê·¸ë¦° ê·¸ë˜í”„
- í•˜ë‚˜ì˜ íŒŒë¼ë¯¸í„°ë§Œ ê°€ì§€ê³  ê²€ì¦ê³¡ì„ ì„ ê·¸ë¦¬ëŠ” ê²ƒì´ í˜„ì‹¤ì ìœ¼ë¡œ ìœ ìš©í•˜ì§€ëŠ” ì•Šë‹¤.

```py
import matplotlib.pyplot as plt
from category_encoders import OrdinalEncoder
from sklearn.model_selection import validation_curve
from sklearn.tree import DecisionTreeRegressor

pipe = make_pipeline(
    OrdinalEncoder(), 
    SimpleImputer(), 
    DecisionTreeRegressor()
)

depth = range(1, 30, 2)
ts, vs = validation_curve(
    pipe, X_train, y_train
    , param_name='decisiontreeregressor__max_depth'
    , param_range=depth, scoring='neg_mean_absolute_error'
    , cv=3
    , n_jobs=-1
)

train_scores_mean = np.mean(-ts, axis=1)
validation_scores_mean = np.mean(-vs, axis=1)

fig, ax = plt.subplots()

# í›ˆë ¨ì„¸íŠ¸ ê²€ì¦ê³¡ì„ 
ax.plot(depth, train_scores_mean, label='training error')

# ê²€ì¦ì„¸íŠ¸ ê²€ì¦ê³¡ì„ 
ax.plot(depth, validation_scores_mean, label='validation error')

# ì´ìƒì ì¸ max_depth
ax.vlines(5,0, train_scores_mean.max(), color='blue')

# ê·¸ë˜í”„ ì…‹íŒ…
ax.set(title='Validation Curve'
      , xlabel='Model Complexity(max_depth)', ylabel='MAE')
ax.legend()
fig.dpi = 100
```

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-20 10 38 08](https://user-images.githubusercontent.com/79494088/130165420-d0b3a188-9d5a-45d0-a8fb-4d8a84e441fd.png)

- ì´ëŒ€ë¡œ íŠ¸ë¦ì˜ ê¹Šì´ë¥¼ ì •í•œë‹¤ë©´ `max_depth = 5` ë¶€ê·¼ì—ì„œ ì„¤ì •í•´ì¤˜ì•¼ ê³¼ì í•©ì„ ë§‰ê³  ì¼ë°˜í™” ì„±ëŠ¥ì„ ì§€í‚¬ ìˆ˜ ìˆë‹¤.

## Randomized Search CV

- [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV): ê²€ì¦í•˜ê³  ì‹¶ì€ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë“¤ì˜ ìˆ˜ì¹˜ë¥¼ ì •í•´ì£¼ê³  ê·¸ ì¡°í•©ì„ ëª¨ë‘ ê²€ì¦í•œë‹¤.
- [RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html): ê²€ì¦í•˜ë ¤ëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„°ë“¤ì˜ ê°’ ë²”ìœ„ë¥¼ ì§€ì •í•´ì£¼ë©´ ë¬´ì‘ìœ„ë¡œ ê°’ì„ ì§€ì •í•´ ê·¸ ì¡°í•©ì„ ëª¨ë‘ ê²€ì¦í•œë‹¤.

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-20 10 39 52](https://user-images.githubusercontent.com/79494088/130165573-0dca9307-c8ac-48d0-9b7d-567868a3df29.png)

### Ridge íšŒê·€ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹

```py
from sklearn.model_selection import RandomizedSearchCV

pipe = make_pipeline(
    OneHotEncoder(use_cat_names=True)
    , SimpleImputer()
    , StandardScaler()
    , SelectKBest(f_regression)
    , Ridge()
)

# íŠœë‹í•  í•˜ì´í¼íŒŒë¼ë¯¸í„°ì˜ ë²”ìœ„ë¥¼ ì§€ì •í•´ ì£¼ëŠ” ë¶€ë¶„
dists = {
    'simpleimputer__strategy': ['mean', 'median'], 
    'selectkbest__k': range(1, len(X_train.columns)+1), 
    'ridge__alpha': [0.1, 1, 10], 
}

clf = RandomizedSearchCV(
    pipe, 
    param_distributions=dists, 
    n_iter=50, 
    cv=3,
    scoring='neg_mean_absolute_error',
    verbose=1,
    n_jobs=-1
)

clf.fit(X_train, y_train);
'''
Fitting 3 folds for each of 50 candidates, totalling 150 fits
'''

print('ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°: ', clf.best_params_)
print('MAE: ', -clf.best_score_)
'''
ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°:  {'simpleimputer__strategy': 'median', 'selectkbest__k': 55, 'ridge__alpha': 10}
MAE:  18414.633797820472
'''
```

### ëœë¤í¬ë ˆìŠ¤íŠ¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹

```py
from scipy.stats import randint, uniform

pipe = make_pipeline(
    TargetEncoder(), 
    SimpleImputer(), 
    RandomForestRegressor(random_state=2)
)

dists = {
    'targetencoder__smoothing': [2.,20.,50.,60.,100.,500.,1000.], # intë¡œ ë„£ìœ¼ë©´ error(bug)
    'targetencoder__min_samples_leaf': randint(1, 10),     
    'simpleimputer__strategy': ['mean', 'median'], 
    'randomforestregressor__n_estimators': randint(50, 500), 
    'randomforestregressor__max_depth': [5, 10, 15, 20, None], 
    'randomforestregressor__max_features': uniform(0, 1) # max_features
}

clf = RandomizedSearchCV(
    pipe, 
    param_distributions=dists, 
    n_iter=50, 
    cv=3, 
    scoring='neg_mean_absolute_error',  
    verbose=1,
    n_jobs=-1
)

clf.fit(X_train, y_train);
'''
Fitting 3 folds for each of 50 candidates, totalling 150 fits
'''

print('ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°: ', clf.best_params_)
print('MAE: ', -clf.best_score_)
'''
ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°:  {'randomforestregressor__max_depth': 20, 'randomforestregressor__max_features': 0.22612308958451122, 'randomforestregressor__n_estimators': 498, 'simpleimputer__strategy': 'mean', 'targetencoder__min_samples_leaf': 8, 'targetencoder__smoothing': 1000.0}
MAE:  15741.360087309344
'''

# ë§Œë“¤ì–´ì§„ ëª¨ë¸ì—ì„œ ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì€ ëª¨ë¸
pipe = clf.best_estimator_

from sklearn.metrics import mean_absolute_error

y_pred = pipe.predict(X_test)
mae = mean_absolute_error(y_test, y_pred)
print(f'í…ŒìŠ¤íŠ¸ì„¸íŠ¸ MAE: ${mae:,.0f}')
'''
í…ŒìŠ¤íŠ¸ì„¸íŠ¸ MAE: $15,778
'''

features
'''
Index(['MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'Street', 'Alley',
       'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope',
       'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle',
       'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'RoofStyle',
       'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'MasVnrArea',
       'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond',
       'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1', 'BsmtFinType2',
       'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'Heating', 'HeatingQC',
       'CentralAir', 'Electrical', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath',
       'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual',
       'TotRmsAbvGrd', 'Functional', 'Fireplaces', 'FireplaceQu', 'GarageType',
       'GarageYrBlt', 'GarageFinish', 'GarageCars', 'GarageArea', 'GarageQual',
       'GarageCond', 'PavedDrive', 'WoodDeckSF', 'OpenPorchSF',
       'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'PoolQC',
       'Fence', 'MiscFeature', 'MiscVal', 'MoSold', 'YrSold', 'SaleType',
       'SaleCondition', 'All_Flr_SF', 'All_Liv_SF'],
      dtype='object')
'''
```

## ì„ í˜•íšŒê·€, ëœë¤í¬ë˜ìŠ¤íŠ¸ ëª¨ë¸ë“¤ì˜ íŠœë‹ ì¶”ì²œ í•˜ì´í¼íŒŒë¼ë¯¸í„°
### Random Forest
- class_weight (ë¶ˆê· í˜•(imbalanced) í´ë˜ìŠ¤ì¸ ê²½ìš°)
- max_depth (ë„ˆë¬´ ê¹Šì–´ì§€ë©´ ê³¼ì í•©)
- n_estimators (ì ì„ê²½ìš° ê³¼ì†Œì í•©, ë†’ì„ê²½ìš° ê¸´ í•™ìŠµì‹œê°„)
- min_samples_leaf (ê³¼ì í•©ì¼ê²½ìš° ë†’ì„)
- max_features (ì¤„ì¼ ìˆ˜ë¡ ë‹¤ì–‘í•œ íŠ¸ë¦¬ìƒì„±)

### Logistic Regression
- C (Inverse of regularization strength)
- class_weight (ë¶ˆê· í˜• í´ë˜ìŠ¤ì¸ ê²½ìš°)
- penalty

### Ridge / Lasso Regression
- alpha