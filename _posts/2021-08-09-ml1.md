---
title: '[Machine Learning] 선형회귀(Linear Regression)'
description: 선형회귀모델과 지도학습을 이해하고 회귀모델에 기준모델을 설정하며 Scikit-learn을 이용해 선형회귀모델을 만들고 사용하며 해석
categories:
 - Machine Learning
tags: [Machine Learning, Supervised Learning, 선형회귀모델, 지도학습, 기준모델, Scikit learn]
mathjax: enable
# 0️⃣1️⃣2️⃣3️⃣4️⃣5️⃣6️⃣7️⃣8️⃣9️⃣🔟
---

- 회귀선을 그리기 위한 계수들 ($b_0$, $b_1$) 구하는 공식?
- Tabular data의 특징을 세 부분으로 나누면?
- 지도학습(Supervised Learning)에는 두 가지 방식이 있습니다: 회귀(Regression) and 분류(Classification)
  - “How Much / How Many?” (회귀)
  - “Is this A or B?” (분류)
- 회귀분석과, 분류의 타겟 특성은 무엇이 다른가요?

# 1️⃣ 주택 판매 가격 예측

```py
import pandas as pd
# 주어진 url 주소를 이용해 house prices 데이터 import
df = pd.read_csv('https://ds-lecture-data.s3.ap-northeast-2.amazonaws.com/house-prices/house_prices_train.csv')
df_t = pd.read_csv('https://ds-lecture-data.s3.ap-northeast-2.amazonaws.com/house-prices/house_prices_test.csv')

df.head()
df_t.head()
```

![](/assets/images/8.png)

![](/assets/images/9.png)

```py
# 여러 특성 중 'GrLivArea', 'LotArea', 'SalePrice'를 사용
# SalePrice: 예측해야 하는 타겟값으로 주택판매가격(달러)
# LotArea: 집과 마당의 사이즈(square feet)
# GrLivArea: 지상 생활면적(square feet)
df = df[['GrLivArea', 'LotArea', 'SalePrice']]
df_t = df_t[['GrLivArea', 'LotArea']]

# 테이블 형테 출력, 이런 형태의 데이터 : tabular data
df
```

![스크린샷 2021-08-09 10 22 50](https://user-images.githubusercontent.com/79494088/128652118-8e656215-4c04-4473-8510-cb2b165c5be3.png)

## 예측 방법

### 1. 기존 경험을 바탕으로 예측
- 보통 좋은 결과를 내기도 하지만, 사람마다 편견이 존재하며 오류에 빠질 위험이 높다.

### 2. 통계정보를 활용

```py
df['SalePrice'].describe()

'''
count     1,460.0
mean    180,921.2
std      79,442.5
min      34,900.0
25%     129,975.0
50%     163,000.0
75%     214,000.0
max     755,000.0
Name: SalePrice, dtype: float64
'''

import matplotlib.pyplot as plt
import seaborn as sns

## SalePrice의 확률밀도함수
sns.displot(df['SalePrice'], kde=True)

## 평균과, 중간값 수직선
plt.axvline(df['SalePrice'].mean(), color='blue')
plt.axvline(df['SalePrice'].median(), color='red');
```

![스크린샷 2021-08-09 10 26 11](https://user-images.githubusercontent.com/79494088/128652258-8b053aa2-e39b-4ecc-b603-c72bb850281d.png)

- 만일 가격을 처음으로 예측할 때, 가장 간단하고 직관적인 방법으로 평균이나 중간값을 이용해 보는 것도 좋은 선택일 것이다.

## 기준모델(Baseline Model)
- 예측모델을 구체적으로 만들기 전에 가장 간단하면서도 직관적이면서 최소한의 성능을 나타내는 기준이 되는 모델
- 평균값을 기준으로 사용하면 '평균기준모델'
- 문제별로 기준모델은 다음과 같이 설정한다.
  - 분류문제 : 타겟의 최빈 클래스
  - 회귀문제 : 타겟의 평균값
  - 시계열회귀문제 : 이전 타임 스탬프의 값

```py
# predict: 우리가 정한 기준모델인 평균으로 예측
predict = df['SalePrice'].mean()

# 평균값으로 예측할 때 샘플 별 평균값과의 차이(error)를 저장
errors = predict - df['SalePrice']

errors

'''
0      -27,578.8
1         -578.8
2      -42,578.8
3       40,921.2
4      -69,078.8
          ...   
1455     5,921.2
1456   -29,078.8
1457   -85,578.8
1458    38,796.2
1459    33,421.2
Name: SalePrice, Length: 1460, dtype: float64
'''

# mean_absolute_error(MAE), error에 절대값을 취한 후 평균을 계산
mean_absolute_error = errors.abs().mean()
```

- MAE(Mean Absolute Error, 평균절대오차)는 예측 Error의 절대값 평균을 나타냄

$$Error = (price - guess)$$

$$
\begin{align}mae = (\frac{1}{n})\sum_{i=1}^{n}\left | price_{i} - guess_{i} \right |\end{align}
$$

```py
x = df['GrLivArea']
y = df['SalePrice']

predict = df['SalePrice'].mean()
errors = predict - df['SalePrice']
mean_absolute_error = errors.abs().mean()

sns.lineplot(x=x, y=predict, color='red')
sns.scatterplot(x=x, y=y, color='blue');
```

![스크린샷 2021-08-09 10 51 57](https://user-images.githubusercontent.com/79494088/128653268-7bbce9d8-17d1-4314-8eb6-cd8e431f2235.png)

```py
print(f'예측한 주택 가격이 ${predict:,.0f}이며 절대평균에러가 ${mean_absolute_error:,.0f}임을 확인할 수 있습니다.')

# 예측한 주택 가격이 $180,921이며 절대평균에러가 $57,435임을 확인할 수 있습니다.
```

- 평균예측은 에러가 상당히 크다.

```py
# 주택 가격이 전체공간크기(LotArea)와 같은 다른 특성과 어떤 상관관계(dependent)가 있을까?

sns.set(style='whitegrid', context='notebook')
cols = ['GrLivArea', 'LotArea','SalePrice']
sns.pairplot(df[cols], height=2);
```

![스크린샷 2021-08-09 14 23 39](https://user-images.githubusercontent.com/79494088/128663374-7bfad9b7-4fad-488d-a77a-2ceaf351c408.png)

## 예측모델(Predictive Model) 활용
- Scatter plot에 가장 잘 맞는 (Base fit) 직선을 그려주면 그것이 회귀 예측모델이 된다.

### 회귀직선을 만드는 방법
- 회귀분석에서 중요한 개념은 예측값과 잔차(Residual)
  - 예측값 : 만들어진 모델이 추정하는 값
  - 잔차 : 예측값과 관측값의 차이<br>
  (오차(Error) : 모집단에서의 예측값과 관측값의 차이)
- 회귀선은 잔차 제곱들의 합인 RSS(Residual sum of Squares)를 최소화하는 직선
  - RSS : SSE(Sum of Square Error)라고도 말하며 이 값이 회귀모델의 비용함수(Cost Function)가 된다.
- 머신러닝에서는 비용함수를 최소화하는 모델을 찾는 과정을 학습이라고 한다.

$${\displaystyle \operatorname {RSS} =\sum _{i=1}^{n}(\varepsilon _{i})^{2}=\sum _{i=1}^{n}(y_{i}-f(x_{i}))^{2}=\sum _{i=1}^{n}(y_{i}-(\alpha x_{i} + \beta))^{2}}$$

- 여기서 계수 $a$와 $b$는 RSS를 최소화하는 값으로 모델 학습을 통해 얻어지는 값.
- 잔차 제곱합을 최소화하는 방법을 최소제곱회귀 or OLS(Ordinary Least Squares)라고 부른다.
- OLS는 계수 계산을 위해 다음 공식을 사용한다.

{% raw %}$$\beta =\displaystyle {\bar {y}}-\alpha{\bar {x}}$${% endraw %}

{% raw %}$$\alpha ={\frac {S_{xy}}{S_{xx}}}$${% endraw %}

{% raw %}$${\displaystyle S_{xy}=\sum _{i=1}^{n}(x_{i}-{\bar {x}})(y_{i}-{\bar {y}})}$${% endraw %}

{% raw %}$${\displaystyle S_{xx}=\sum _{i=1}^{n}(x_{i}-{\bar {x}})^{2}}$${% endraw %}

- 최소제곱법으로 선형 회귀계수를 쉽게 구할 수 있다.

```py
# Seaborn regplot으로 그리면, 
sns.regplot(x=df['GrLivArea'], y=df['SalePrice']);
```

![스크린샷 2021-08-09 13 33 02](https://user-images.githubusercontent.com/79494088/128660669-9be27d55-f279-4fb2-ad64-3d55751b5869.png)

```py
# GrLivArea > 3500 & GrLivArea < 4500 사이의 데이터
df[(df['GrLivArea'] > 3500) & (df['GrLivArea'] < 4500)]
```

![스크린샷 2021-08-09 13 33 54](https://user-images.githubusercontent.com/79494088/128660732-d004e108-4b92-44d7-9a62-f6b65374159a.png)

- 선형회귀는 주어져 있지 않은 점의 함수값을 보간(Interpolate)하여 예측하는데 도움을 준다.
- 선형회귀모델을 사용해 4000sqft 주택 가격을 어림잡아 예측해 볼 수 있다.
- 선형회귀모델은 기존 데이터의 범위를 넘어서는 값을 예측하기 위한 외삽(Extrapolate)도 제공한다.

# 2️⃣ Scikit-Learn 활용 선형회귀모델
- 현재 다루는 데이터에 6000sqft 이상인 주택의 거래 정보가 없다.

## 이런 경우 예측하는 방법
- 선형회귀 직선은 독립변수(Independent Variable, $x$)와 종속변수(Dependent Variable, $y$) 간의 관계를 요약해준다.
  - 종속변수
    - 연구자가 독립변수의 변화에 따라 어떻게 변하는지 알고 싶어하는 변수
    - 반응변수(Response), Label, Target 등
  - 독립변수
    - 연구자가 의도적으로 변화시키는 변수
    - 다른 변수에 영향을 받지 않고 오히려 종속변수에 영향을 준다.
    - 예측변수(Predictor), 설명(Explanatory), 특성(Feature) 등

## Scikit-Learn Data 구조

<img src="https://www.researchgate.net/publication/301946040/figure/fig1/AS:362519232303116@1463442728351/Data-representation-in-scikit-learn.png" alt="Data representation in scikit-learn"/>

- Scikit-Learn을 활용해 모델을 만들고 데이터를 분석하기 위해서는 위와 같은 구조를 사용해야 한다. 
- 특정 데이터와 타겟 데이터를 나누어준다.
- 특성행렬은 주로 `X`로 표현하고 보통 2차원 행렬이다.<br>
주로 Numpy 행렬이나 Pandas 데이터 프레임으로 표현
- 타겟배열은 주로 `y`로 표현하고 보통 1차원 형태이다.<br>
주로 Numpy 행렬이나 Pandas 데이터 프레임으로 표현

## Scikit-Learn의 수많은 머신러닝 모델
- 모두 유사한 프로세스를 통해서 사용할 수 있다. 
- 적합한 모델을 선택하여 클래스를 찾아본 후 관련 속성이나 하이퍼파라미터를 확인한다.
- `fit()` 메소드를 사용하여 모델 학습
- `predict()` 메소드를 사용하여 새로운 데이터 예측
- [Scikit-Learn 소개](https://jakevdp.github.io/PythonDataScienceHandbook/05.02-introducing-scikit-learn.html#Basics-of-the-API)

# 3️⃣ Simple Linear Regression (단순 선형 회귀)
- [sklearn.linear_model.LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)

```py
# Scikit-Learn 라이브러리에서 사용할 예측모델 클래스 Import
from sklearn.linear_model import LinearRegression

# 예측모델 인스턴스
model = LinearRegression()

# X 특성들의 테이블과, y 타겟 벡터 생성
feature = ['GrLivArea']
target = ['SalePrice']
X_train = df[feature]
y_train = df[target]

# 모델학습(fit)
model.fit(X_train, y_train)

# LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)

# 새로운 데이터 한 샘플을 선택해 학습한 모델을 통해 예측
X_test = [[4000]]
y_pred = model.predict(X_test)

print(f'{X_test[0][0]} sqft GrLivArea를 가지는 주택의 예상 가격은 ${int(y_pred)} 입니다.')

# 4000 sqft GrLivArea를 가지는 주택의 예상 가격은 $447090 입니다.

# 전체 테스트 데이터를 모델을 통해 예측
X_test = [[x] for x in df_t['GrLivArea']]
y_pred = model.predict(X_test)

# 전체 예측값
y_pred

'''
array([[114557.82748987],
       [160945.27292207],
       [193084.38061182],
       ...,
       [149696.58523066],
       [122485.47405334],
       [232829.74378814]])
'''

# train 데이터와 예측에 대한 그래프
plt.scatter(X_train, y_train, color='black', linewidth=1)
plt.scatter(X_test, y_pred, color='blue', linewidth=1);
```

![스크린샷 2021-08-09 13 59 08](https://user-images.githubusercontent.com/79494088/128661948-c17210d7-bc4b-4773-9f16-a6b6f3790ad4.png)

- 위 코드는 이 [다이어그램](https://ogrisel.github.io/scikit-learn.org/sklearn-tutorial/tutorial/text_analytics/general_concepts.html#supervised-learning-model-fit-x-y)에서 표현한 일반적인 머신러닝 프로세스

<img src="https://ogrisel.github.io/scikit-learn.org/sklearn-tutorial/_images/plot_ML_flow_chart_12.png" width="75%">

- 머신러닝을 새로운 프로그래밍 패러다임으로써의 관점

<img src="https://pbs.twimg.com/media/ECQDlFOWkAEJzlY.jpg" width="70%">

- 데이터를 입력하고 어떤 룰에 따라 답을 구해내는 일반적인 프로그래밍과 달리 머신러닝은 데이터와 답을 통해 룰을 찾아내는 방법

# 선형회귀모델의 계수(Coefficients)
- 모델이 주택의 크기와 가격 사이의 어떤 관계를 학습했는지 보기 위해 `LinearRegression` 객체의 `coef_`, `intercept_` 속성을 확인

```py
# 계수(coefficient)
model.coef_

# array([[107.13035897]])

# 절편(intercept)
model.intercept_

# array([18569.02585649])
```

## Coefficients의 영향
- 예측함수를 만들어 새로운 데이터 반복해서 예측

```py
def explain_prediction(sqft):
    y_pred = model.predict([[sqft]])
    pred = f"{int(sqft)} sqft 주택 가격 예측: ${int(y_pred[0])} (1 sqft당 추가금: ${int(model.coef_[0])})"

    return pred

# square_feet = 4000 인 테스트 데이터로 예측
print(explain_prediction(4000))

# 4000 sqft 주택 가격 예측: $447090 (1 sqft당 추가금: $107)
```

## ipywidgets 사용

```py
from ipywidgets import interact

# 데코레이터 interact를 추가
@interact
def explain_prediction(sqft=(500,10000)):
    y_pred = model.predict([[sqft]])
    pred = f"{int(sqft)} sqft 주택 가격 예측: ${int(y_pred[0])} (1 sqft당 추가금: ${int(model.coef_[0])})"

    return pred
```

![스크린샷 2021-08-09 14 14 39](https://user-images.githubusercontent.com/79494088/128662853-1ca5c8f4-7aa0-4b81-a559-9347b256302c.png)

# 4️⃣ 참고자료

#### 기준모델
- [Always start with a stupid model, no exceptions](https://blog.insightdatascience.com/always-start-with-a-stupid-model-no-exceptions-3a22314b9aaa)

#### Scikit-Learn
- [Python Data Science Handbook, Chapter 5.2: Introducing Scikit-Learn](https://jakevdp.github.io/PythonDataScienceHandbook/05.02-introducing-scikit-learn.html#Basics-of-the-API)
- [2.4.2.2. Supervised Learning](https://ogrisel.github.io/scikit-learn.org/sklearn-tutorial/tutorial/text_analytics/general_concepts.html#supervised-learning-model-fit-x-y)
- [sklearn.linear_model.LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)
- [sklearn.metrics.mean_absolute_error](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html)

#### 읽어보세요
- [Art of Choosing Metrics in Supervised Models](https://towardsdatascience.com/art-of-choosing-metrics-in-supervised-models-part-1-f960ae46902e)
- [The Discovery of Statistical Regression](https://priceonomics.com/the-discovery-of-statistical-regression/)

#### 최소제곱법
- [수학산책-최소제곱법](https://terms.naver.com/entry.nhn?cid=58944&docId=3569970&categoryId=58970)

#### (참고) 더 세련된 시각화툴: Plotly
- [Plotly Express](https://plot.ly/python/plotly-express/)
- [plotly_express.scatter](https://www.plotly.express/plotly_express/#plotly_express.scatter)

#### ipywidgets interact
- [Using Interact](https://ipywidgets.readthedocs.io/en/stable/examples/Using%20Interact.html#Using-Interact)