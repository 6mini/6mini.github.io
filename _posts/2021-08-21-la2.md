---
title: '[Linear Algebra] Intermediate Linear Algebra'
description: 분산과 공분산의 차이와 상관계수의 목적, 벡터의 직교, 단위 벡터, Span, Basis, Rank, Gaussian Elemination, Linear Projection 등의 설명
categories:
 - Fundamentals to DS
tags: [Linear Algebra, 분산, 공분산, 상관계수, 단위벡터, Span, Basis, Rank, Gaussian Elemination, Linear Projection]
mathjax: enable
# 0️⃣1️⃣2️⃣3️⃣4️⃣5️⃣6️⃣7️⃣8️⃣9️⃣🔟
---

# 1️⃣ 분산(Varicance)
- 분산 : 데이터가 얼마나 퍼져있는지를 측정하는 방법
- 각 값들의 평균으로부터 차이의 제곱 평균이다.
- 분산을 구하기 위해서 일반적으로 평균을 먼저 계산해야한다.

```py
# 랜덤한 50개의 정수를 포함하는 2 변수 설정
variance_one = []
variance_two = []
for x in range(50):
  variance_one.append(random.randint(25,75))
  variance_two.append(random.randint(0,100))
  
variance_data = {'v1': variance_one, 'v2': variance_two}

variance_df = pd.DataFrame(variance_data)
variance_df['zeros'] = pd.Series(list(np.zeros(50)))

variance_df.head()
```

![스크린샷 2021-08-21 21 26 37](https://user-images.githubusercontent.com/79494088/130321706-3cc584a6-86c2-46d6-9789-00fd0e52bcd3.png)

```py
# scatter plot

plt.scatter(variance_df.v1, variance_df.zeros)
plt.xlim(0,100)
plt.title("Plot 1")
plt.show()

plt.scatter(variance_df.v2, variance_df.zeros)
plt.xlim(0,100)
plt.title("Plot 2")
plt.show()
```

![스크린샷 2021-08-21 21 27 14](https://user-images.githubusercontent.com/79494088/130321726-cc6e80da-8e3b-4a90-bbac-137da2cff6c7.png)

- Scatter Plot의 두 데이터를 통해 벌어져있는 정도의 차이를 쉽게 확인할 수 있다.
- $\overline{X}$ 는 평균, $N$ 은 관측의 수(샘플의 수)
- $v$ 혹은 분산은 일반적으로 소문자 v로 표기되며 필요에 따라 $\sigma^{2}$로 표기한다.

$$
v = \frac{\sum{(X_{i} - \overline{X})^{2}} }{N}
$$

```py
# 평균
v1_mean = variance_df.v1.mean()
print("v1 mean: ", v1_mean)
v2_mean = variance_df.v2.mean()
print("v2 mean: ", v2_mean)

# 각 데이터로부터 평균까지의 거리
variance_df['v1_distance'] = variance_df.v1-v1_mean
variance_df['v2_distance'] = variance_df.v2-v2_mean

variance_df.head()

# 제곱 
variance_df['v1_squared_distance'] = variance_df.v1_distance**2
variance_df['v2_squared_distance'] = variance_df.v2_distance**2

# 제곱을 통해서 음수를 양수로 (거리의 크기) 바꿀 수 있습니다.
variance_df.head()
```

![스크린샷 2021-08-21 21 45 24](https://user-images.githubusercontent.com/79494088/130322214-68d879bb-aff2-43f1-be41-d5408556305b.png)

```py
# 더하고 나눔
observations = len(variance_df)
print("Number of Observations: ", observations)

Variance_One = variance_df.v1_squared_distance.sum()/observations
Variance_Two = variance_df.v2_squared_distance.sum()/observations

print("Variance One: ", Variance_One)
print("Variance Two: ", Variance_Two)
'''
Number of Observations:  50
Variance One:  212.17640000000003
Variance Two:  764.3844
'''
```

- Random Number를 생성할 때, v1은 `25 ~ 75`까지 `50`범위를 / v2는 `0 ~ 100`까지 `100`범위를 바탕으로 2배 정도 차이나게 생성했지만, **분산의 차이는 4배정도 차이**가 난다.

## Variance 쉽게 계산하기

```py
print(variance_df.v1.var(ddof = 1))
print(variance_df.v2.var(ddof = 1))
'''
216.50653061224492
779.9840816326531
'''
```

- 앞서 계산했던 결과와 조금 다르다.
  - 그 이유는 분산을 계산하는 방법이 모집단이냐 혹은 샘플이냐에 따라서 달라지기 때문이다.
- **모집단의 분산** $\sigma^{2}$ : 모집단의 **PARAMETER** (aspect, property, attribute, etc)
- The **샘플의 분산** $s^{2}$ : 샘플의 **STATISTIC** (estimated attribute)
- 샘플 분산 $s^{2}$ 는 모집단 분산 $\sigma^{2}$의 추정치다.
- 일반적으로, **샘플**의 분산을 계산 할 때 $N-1$로 나누어야 한다.
- 앞서 우리가 데이터를 통해 계산했던 방식은 **모집단의 분산**이다.
- 그렇기 때문에 자유도를 0 으로 설정하는 경우, 동일한 값을 얻을 수 있다.

```py
print(variance_df.v1.var(ddof = 0))
print(variance_df.v2.var(ddof = 0))
'''
212.17640000000003
764.3844
'''
```

# 2️⃣ Standard Deviation
- 표준편차 : 분산의 값에 $\sqrt()$를 씌운 것이다.

## 분산/표준편차
- 표준편차는 분산이 평균값에 비해서 스케일의 문제가 있어서 이를 해결하기 위해 제곱된 스케일을 낮춘 방법이다.
- 이는 많은 통계분석 프로세스에서 표준편차를 사용하여 게산하는 이유 중 하나다.

```py
print(variance_df.v1.std(ddof=0)) # std 에 주의
print(variance_df.v2.std(ddof=0))
'''
14.566276119859873
27.647502599692437
'''
```

# 3️⃣ 공분산(Covariance)
- 공분산(Covariance) : 1개 변수 값이 변화할 때 다른 변수가 어떤 연관성을 나타내며 변하는지를 측정하는 것

![스크린샷 2021-08-21 21 57 26](https://user-images.githubusercontent.com/79494088/130322521-9fa56c84-abb6-435e-8fcf-d18121e1cac4.png)

- 첫번째 그래프 : 음의 공분산 값을 갖는다.(nagative)
- 두번째 그래프 : 양 변수의 높고 낮음에 대해 관련성을 알 수 없다. 0에 가까운 공분산 값을 갖는다.
- 세번재 그래프 : 양 변수간의 공분산 값은 양의 값이다.(positive)

## 공분산 이해하기
- 큰 값의 공분산은 두 변수간의 큰 연관성을 나타낸다.
- 만약 변수들이 다른 스케일을 가지고 있다면 공분산은 실제 변수의 연관성에 관계 없이 영향을 받게 될 것이다.
- 두 변수가 연관성이 적더라도 큰 스케일을 가지고 있다면, 연관성이 높지만 스케일이 작은 변수들에 비해서 높은 공분산 값을 갖게 될 것이다.

```py
a = b = np.arange(5, 50, 5)
c = d = np.arange(10,100,10)

fake_data = {"a": a, "b": b, "c": c, "d": d}

df = pd.DataFrame(fake_data)

plt.scatter(df.a, df.b)
plt.xlim(0,100)
plt.ylim(0,100)
plt.show()

plt.scatter(df.c, df.d)
plt.xlim(0,100)
plt.ylim(0,100)
plt.show()
```

![스크린샷 2021-08-21 22 01 32](https://user-images.githubusercontent.com/79494088/130322622-3ce36c3c-162c-4068-892b-f5dbf46cce6b.png)

## Variance covariance matrix

```py
# 공분산 계산
df.cov()
```

![스크린샷 2021-08-21 22 03 13](https://user-images.githubusercontent.com/79494088/130322682-f5c40162-26fc-4011-b667-35a7f2a99427.png)

- 이러한 matrix를 variance-covariance matrix 라고 표현하며, 대각선 부분은 공분산이 아닌, 분산을 표현한다.
- 두 데이터셋(a-b, c-d)은 동일한 연관성을 갖고 있지만 ($x = y$), 계산된 공분산의 값은 매우 다르다.

## Correlation coefficient
- 분산에서 표준편차를 사용했던 것처럼, 공분산의 스케일을 조정할 수 있다.
- 공분산을 두 변수의 표준편차로 각각 나눠주면 스케일을 조정할 수 있으며 그것을 **상관계수**라고 부른다.
- 상관계수는 -1에서 1까지로 정해진 범위 안의 값만을 가지며 선형연관성이 없는 경우 0에 근접하게 된다.
- 대부분의 경우, **상관계수**는 공분산에 비해 더 좋은 지표로 사용된다.
  - 공분산은 이론상 모든 값을 가질 수 있지만, 상관계수는 `-1 ~ 1`사이로 정해져 비교하기 쉽다.
  - 공분산은 항상 스케일, 단위를 포함하고 있지만, 상관계수는 이에 영향을 받지 않는다.
  - 상관계수는 데이터의 평균 혹은 분산의 크기에 영향을 받지 않는다.
- 상관계수는 일반적으로 소문자 $r$로 표현된다.

$$cor(X,Y) = r = \frac{cov(X,Y)}{\sigma_{X}\sigma_{Y}} $$

```py
# correlation 계산
df.corr()
```

![스크린샷 2021-08-21 22 22 48](https://user-images.githubusercontent.com/79494088/130323150-8ec28e0f-561a-48bc-a51b-ebe6a2033eef.png)

- 상관계수가 1이라는 것은 한 변수가 다른 변수에 대해 완벽한 양의 선형 관계를 갖고 있다는 것을 의미한다.

![스크린샷 2021-08-21 22 31 06](https://user-images.githubusercontent.com/79494088/130323378-7a4ff648-3571-46e9-998a-595928d1ccb2.png)

## Spearman Correlation
- 상관계수(correlation coefficient)는 Pearson correlation이라 부르며 이는 데이터로부터 분산과 같은 통계치를 계산할 수 있을 때 사용가능하다.
- 만약 데이터가 numeric이 아니라, categorical이라면 spearman correlation coefficien는 값들에 대해 순서 혹은 rank를 매기고, 그를 바탕으로 correlation을 측정하는 Non-parametiric한 방식이다.

# 4️⃣ Orthogonality
- Orthogonality : 벡터 혹은 매트릭스가 서로 수직으로 있는 상태

![스크린샷 2021-08-21 22 37 29](https://user-images.githubusercontent.com/79494088/130323527-794089b9-0045-4a4c-a293-377e6d643806.png)

- 좌표상에 있는 거의 모든 벡터는 다른 벡터와 상관이 아주 작게라도 있다.
- 하나가 증가할 때, 다른 하도 증가하는 경향을 파악하는 것이 공분산이다.
- 딱 하나, 수직인 벡터만 상관관계가 전혀 없다.
- 데이터에 선형관계가 있는 것과 상관관계가 없는 것이 어떤 의미를 가질까?

## 벡터의 orthogonality 확인
- 임의의 두 벡터의 내적값이 0이라면 서로 수직으로 배치되어 있다.

```py
vector_1 = [0, 5]
vector_2 = [5, 0]

plt.arrow(0, 0, vector_1[0], vector_1[1], head_width = .1, head_length = .1, color ='#d63031')
plt.arrow(0, 0, vector_2[0], vector_2[1], head_width = .1, head_length = .1, color ='#00b894')
plt.xlim(-1, 7)          
plt.ylim(-1, 7)
plt.title("Orthogonal Vectors")
plt.show()
```

![스크린샷 2021-08-21 22 46 05](https://user-images.githubusercontent.com/79494088/130323770-513e6098-30f5-4495-ab6b-1cac10f4de14.png)

- 그래프상으로 명확하게 벡터들이 서로 수직하다.

$$
a = \begin{bmatrix} 0 & 5\end{bmatrix}
\qquad
b = \begin{bmatrix} 5 & 0\end{bmatrix}
\\
a \cdot b = (0)(5) + (5)(0) = 0
$$

- 내적값은 0이다.

# 5️⃣ 단위 벡터(Unit Vectors)
- 선형대수에서 단위벡터란 '단위길이(1)'을 갖는 모든 벡터이다.
  - $v$ = [1, 2, 2] 
  - $\vert\vert v\vert\vert$ = $\sqrt{1^2 + 2^2 + 2^2}$ = 3
  - $\hat{v}$ = 1 / $\vert\vert v\vert\vert$ $\cdot$ $v$ = $1 \over 3$ $\cdot$ [1, 2, 2] = [$1 \over 3$, $2 \over 3$, $2 \over 3$]
  - $\vert\vert\hat{v}\vert\vert$ = 1

---

- 각각 1, 2, 3차원의 단위 벡터 입니다.
  - $\mathbb{R}$ unit vector: $\hat{i} = \begin{bmatrix} 1 \end{bmatrix}$
  - $\mathbb{R}^2$ unit vectors: $\hat{i} = \begin{bmatrix} 1 \\ 0 \end{bmatrix}$, $\hat{j} = \begin{bmatrix} 0 \\ 1 \end{bmatrix}$
  - $\mathbb{R}^3$ unit vectors: $\hat{i} = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}$, $\hat{j} = \begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix}$,  $\hat{k} = \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}$

```py
  # 축 설정

plt.xlim(-1,2)          
plt.ylim(-1,2)

# 단위 벡터
i_hat = [1,0]
j_hat = [0,1]

# 축 고정 
plt.gca().set_aspect('equal')

# 벡터 그리기
plt.arrow(0, 0, i_hat[0], i_hat[1], linewidth = 3, head_width = .05, head_length = .05, color = '#d63031')
plt.arrow(0, 0, j_hat[0], j_hat[1], linewidth = 3, head_width = .05, head_length = .05, color = '#0984e3')
plt.title("basis vectors in R^2")
plt.show()
```
![스크린샷 2021-08-21 22 54 03](https://user-images.githubusercontent.com/79494088/130323950-822e9699-a058-4127-9897-fec12c6f78b9.png)

## 벡터를 단위 벡터의 조합으로 표기
- 모든 벡터는 단위 벡터의 선형 조합으로 표기된다.
- v = [5, 30] = [1, 0] * 5 + [0, 1] * 30 = 5 $\cdot$ $\hat{i}$ + 30 $\cdot$ $\hat{j}$
- 위의 예시처럼, $\mathbb{R}^2$내에 있는 임의의 벡터를 단위 벡터의 조합으로 표기 할 수 있다.

# Span
- Span : 주어진 두 벡터의 (합이나 차와 같은) 조합으로 만들 수 있는 모든 가능한 벡터의 집합

## 선형 관계의 벡터 (Linearly Dependent Vector)
- 두 벡터가 같은 선상에 있는 경우 이 벡터들은 선형 관계에 있다고 표현한다.
- 두 벡터들은 조합을 통해 선 외부의 새로운 백터를 생성할 수 없다.
- 이러한 벡터의 span은 평면 공간이 아닌 벡터가 이미 올려져 있는 선으로 제한된다.

## 비선형 관계의 벡터 (Linearly Independent Vector)
- 같은 선상에 있지 않은 벡터들은 선형적으로 독립되어 있다고 표현한다.
- 주어진 공간 (2개의 벡터의 경우 $R^2$ 평면)의 모든 벡터를 조합을 통해 만들어 낼 수 있다.

[진행중]