---
title: '[Linear Models] ë‹¤ì¤‘ì„ í˜•íšŒê·€(Multiple Linear Regression)'
description: ë¨¸ì‹ ëŸ¬ë‹ëª¨ë¸ì„ ë§Œë“¤ ë•Œ í•™ìŠµê³¼ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë¶„ë¦¬ í•´ì•¼ í•˜ëŠ” ì´ìœ ì™€ ë‹¤ì¤‘ ì„ í˜•íšŒê·€ì— ëŒ€í•œ ì´í•´, ê³¼ì í•©/ê³¼ì†Œì í•©ê³¼ í¸í–¥/ë¶„ì‚°ì˜ íŠ¸ë ˆì´ë“œ ì˜¤í”„ ê°œë…ì— ëŒ€í•œ ì´í•´
categories:
 - Machine Learning
tags: [Machine Learning, Linear Models, Supervised Learning, Multiple Linear Regression, ì„ í˜•íšŒê·€ëª¨ë¸, íšŒê·€ë¶„ì„, ë‹¤ì¤‘ì„ í˜•íšŒê·€ëª¨ë¸, íšŒê·€ê³„ìˆ˜, íšŒê·€í‰ê°€ì§€í‘œ, ê³¼ì í•©/ê³¼ì†Œì í•©, ë¶„ì‚°/í¸í–¥, ë‹¤í•­íšŒê·€ëª¨ë¸]
mathjax: enable
# 0ï¸âƒ£1ï¸âƒ£2ï¸âƒ£3ï¸âƒ£4ï¸âƒ£5ï¸âƒ£6ï¸âƒ£7ï¸âƒ£8ï¸âƒ£9ï¸âƒ£ğŸ”Ÿ
---

# 1ï¸âƒ£ ë³µìŠµ
## íšŒê·€ëª¨ë¸ì„ ë§Œë“¤ ë•Œ ê¸°ì¤€ëª¨ë¸ì„ ì–´ë–»ê²Œ ì •ì˜í• ê¹Œ? ì´ ê³¼ì •ì´ ì™œ ì¤‘ìš”í• ê¹Œ?
  - ì ì ˆí•œ ë°°ì¹˜ í¬ê¸° íƒìƒ‰ì€ ëª¨ë¸ í›ˆë ¨ì—ì„œ ë§¤ìš° ì¤‘ìš”í•œ ê³¼ì—… ì¤‘ í•˜ë‚˜
  - ê¸°ì¤€ì´ ë˜ëŠ” ìµœì†Œí™” ê¸°ëŠ¥ì˜ ê°’ì„ íƒìƒ‰

## íšŒê·€ë¶„ì„ì´ë€?
  - ì¼ë°˜í™”ë¥¼ í†µí•´ ë¯¸ë˜ ì˜ˆì¸¡ì„ ì¶”ì •í•˜ê³ ìí•˜ëŠ” ê²ƒ

# 2ï¸âƒ£ í•™ìŠµ / í…ŒìŠ¤íŠ¸

```py
import pandas as pd
df = pd.read_csv('https://ds-lecture-data.s3.ap-northeast-2.amazonaws.com/house-prices/house_prices_train.csv')

df.columns

'''
Index(['Id', 'MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'Street',
       'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig',
       'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType',
       'HouseStyle', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd',
       'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType',
       'MasVnrArea', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual',
       'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1',
       'BsmtFinType2', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'Heating',
       'HeatingQC', 'CentralAir', 'Electrical', '1stFlrSF', '2ndFlrSF',
       'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath',
       'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual',
       'TotRmsAbvGrd', 'Functional', 'Fireplaces', 'FireplaceQu', 'GarageType',
       'GarageYrBlt', 'GarageFinish', 'GarageCars', 'GarageArea', 'GarageQual',
       'GarageCond', 'PavedDrive', 'WoodDeckSF', 'OpenPorchSF',
       'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'PoolQC',
       'Fence', 'MiscFeature', 'MiscVal', 'MoSold', 'YrSold', 'SaleType',
       'SaleCondition', 'SalePrice'],
      dtype='object')
'''
```

- ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ê¸° ìœ„í•´ í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ë‚˜ëˆˆë‹¤.
- ê´€ì‹¬ìˆëŠ”ê²ƒì€ ëª¨ë¸ í•™ìŠµì— ì‚¬ìš©í•œ **í›ˆë ¨(train) ë°ì´í„°ë¥¼ ì˜ ë§ì¶”ëŠ” ëª¨ë¸ì´ ì•„ë‹ˆë¼, í•™ìŠµì— ì‚¬ìš©í•˜ì§€ ì•Šì€ í…ŒìŠ¤íŠ¸(test) ë°ì´í„°ë¥¼ ì–¼ë§ˆë‚˜ ì˜ ë§ì¶”ëŠ”ì§€**ì´ë‹¤.
- ë°ì´í„°ë¥¼ í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ë‚˜ëˆ„ì–´ì•¼ ìš°ë¦¬ê°€ ë§Œë“  ëª¨ë¸ì˜ ì˜ˆì¸¡ ì„±ëŠ¥ì„ ì œëŒ€ë¡œ í‰ê°€í•  ìˆ˜ ìˆë‹¤.
- **í•™ìŠµì— ì‚¬ìš©í•˜ëŠ” ë°ì´í„°ì™€ ëª¨ë¸ì„ í‰ê°€í•˜ëŠ”ë° ì‚¬ìš©í•˜ëŠ” ë°ì´í„°ê°€ ë‹¬ë¼ì•¼ í•œë‹¤**
- ë°ì´í„°ë¥¼ ë¬´ì‘ìœ„ë¡œ ì„ íƒí•´ ë‚˜ëˆ„ëŠ” ë°©ë²•ì´ ì¼ë°˜ì ì´ì§€ë§Œ, ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ê°€ì§€ê³  ê³¼ê±°ì—ì„œ ë¯¸ë˜ë¥¼ ì˜ˆì¸¡í•˜ë ¤ê³  í•˜ëŠ” ê²½ìš° ë¬´ì‘ìœ„ë¡œ ë°ì´í„°ë¥¼ ì„ìœ¼ë©´ ì•ˆëœë‹¤.<br>
ì´ë•ŒëŠ” í›ˆë ¨ ë°ì´í„° ë³´ë‹¤ í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ë¯¸ë˜ì˜ ê²ƒì´ì–´ì•¼ í•œë‹¤.

## Data ë‚˜ëˆ„ê¸°
- DFì— ì‹œê°„/ë‚ ì§œì— ê´€í•œ ì •ë³´ë¥¼ í¬í•¨í•œ íŠ¹ì„±ë“¤ì´ ìˆë‹¤. í•˜ì§€ë§Œ ì‹œê°„ë³€í™”ì— ìƒê´€ì—†ëŠ” ì§‘ê°’ ì˜ˆì¸¡ì´ ëª©í‘œì´ê¸° ë•Œë¬¸ì— ë¬´ì‘ìœ„ë¡œ í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ìœ¼ë¡œ ë‚˜ëˆˆë‹¤.

```py
# train/test ë°ì´í„°ë¥¼ sample ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•´ ë‚˜ëˆˆë‹¤.
train = df.sample(frac=0.75,random_state=1) # 75%
test = df.drop(train.index)

train.head()
```

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-10 10 17 35](https://user-images.githubusercontent.com/79494088/128793725-c6f330d8-bcf2-4c1a-97a2-6b6962a7e224.png)

```py
# train, test ê¸¸ì´ ë¹„êµ
len(train), len(test)

# (1095, 365)
```

# 3ï¸âƒ£ ë‹¤ì¤‘ì„ í˜•íšŒê·€ëª¨ë¸

## ê¸°ì¤€ëª¨ë¸
- SalePriceì˜ í‰ê· ì„ ê¸°ì¤€ëª¨ë¸ë¡œ ì‚¬ìš©

```py
# label ì •ì˜
target = 'SalePrice'
y_train = train[target] # í•™ìŠµ
y_test = test[target] # í…ŒìŠ¤íŠ¸

# ê¸°ì¤€ëª¨ë¸ ì„¤ì • : í‰ê· ê°’
pred = y_train.mean()

pred
# 180327.24200913243

# ê¸°ì¤€ëª¨ë¸ë¡œ í›ˆë ¨ ì—ëŸ¬(MAE) ê³„ì‚°
from sklearn.metrics import mean_absolute_error
y_pred = [pred] * len(y_train)
mae = mean_absolute_error(y_train, y_pred)

print(f'í›ˆë ¨ ì—ëŸ¬: {mae:.2f}')
# í›ˆë ¨ ì—ëŸ¬: 57775.57

# í…ŒìŠ¤íŠ¸ ì—ëŸ¬(MAE)
y_pred = [pred] * len(y_test)
mae = mean_absolute_error(y_test, y_pred)
print(f'í…ŒìŠ¤íŠ¸ ì—ëŸ¬: {mae:.2f}')
í…ŒìŠ¤íŠ¸ ì—ëŸ¬: 55862.90
```

## ë‹¨ìˆœì„ í˜•íšŒê·€ëª¨ë¸

```py
import seaborn as sns
sns.regplot(x=train['GrLivArea'], y=train['SalePrice']).set_title('Housing Prices');
```

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-10 10 27 25](https://user-images.githubusercontent.com/79494088/128794443-63db3f11-983c-4d5c-a877-ad8410cb182f.png)

```py
from sklearn.linear_model import LinearRegression

model = LinearRegression()

features = ['GrLivArea']
X_train = train[features]
X_test = test[features]

# ëª¨ë¸ fit
model.fit(X_train, y_train)
y_pred = model.predict(X_train)
mae = mean_absolute_error(y_train, y_pred)
print(f'í›ˆë ¨ ì—ëŸ¬: {mae:.2f}')

# í›ˆë ¨ ì—ëŸ¬: 38327.78

# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ì ìš©
y_pred = model.predict(X_test)
mae = mean_absolute_error(y_test, y_pred)
print(f'í…ŒìŠ¤íŠ¸ ì—ëŸ¬: {mae:.2f}')

# í…ŒìŠ¤íŠ¸ ì—ëŸ¬: 35476.63
```

- ê¸°ì¤€ëª¨ë¸ê³¼ ë¹„êµí•´ ë³´ë©´ ì—ëŸ¬ê°€ ì¤„ì–´ë“  ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

## ë‹¤ì¤‘ì„ í˜•íšŒê·€ëª¨ë¸ í•™ìŠµ(íŠ¹ì„± 2ê°œ ì´ìƒ)

```py
import numpy as np
import plotly.express as px
import plotly.graph_objs as go
import itertools

px.scatter_3d(
    train,
    x='GrLivArea', 
    y='OverallQual', 
    z='SalePrice',  
    title='House Prices'
)
```

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-10 10 34 14](https://user-images.githubusercontent.com/79494088/128794914-1e1544f5-6830-43d1-b8d2-b0dfb3608557.png)

```py
# ë‹¤ì¤‘ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ íŠ¹ì„±
features = ['GrLivArea', 
            'OverallQual']
X_train = train[features]
X_test = test[features]

# ëª¨ë¸ fit
model.fit(X_train, y_train)
y_pred = model.predict(X_train)
mae = mean_absolute_error(y_train, y_pred)
print(f'í›ˆë ¨ ì—ëŸ¬: {mae:.2f}')

# í›ˆë ¨ ì—ëŸ¬: 29129.58

# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ì ìš©í•´ ë´…ì‹œë‹¤ 
y_pred = model.predict(X_test)
mae = mean_absolute_error(y_test, y_pred)
print(f'í…ŒìŠ¤íŠ¸ ì—ëŸ¬: {mae:.2f}')

í…ŒìŠ¤íŠ¸ ì—ëŸ¬: 27598.31
```

- í•˜ë‚˜ì˜ íŠ¹ì„±ì„ ì‚¬ìš©í•œ ë‹¨ìˆœì„ í˜•íšŒê·€ëª¨ë¸ë³´ë‹¤ í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜ê°€ ë” ì¤„ì–´ë“¤ì—ˆë‹¤.

### ë‹¨ìˆœ vs ë‹¤ì¤‘ì„ í˜•íšŒê·€

```py
import numpy as np
import plotly.express as px
import plotly.graph_objs as go
import itertools

def surface_3d(df, f1, f2, target, length=20, **kwargs):
    """
    2íŠ¹ì„± 1íƒ€ê²Ÿ ì„ í˜•ëª¨ë¸í‰ë©´ì„ ì‹œê°í™” í•©ë‹ˆë‹¤.
    
    df : ë°ì´í„°í”„ë ˆì„
    f1 : íŠ¹ì„± 1 ì—´ ì´ë¦„
    f2 : íŠ¹ì„± 2 ì—´ ì´ë¦„
    target : íƒ€ê²Ÿ ì—´ ì´ë¦„
    length : ê° íŠ¹ì„±ì˜ ê´€ì¸¡ì¹˜ ê°¯ìˆ˜
    
    """
    
    # scatter plot(https://plotly.com/python-api-reference/generated/plotly.express.scatter_3d)
    plot = px.scatter_3d(df, x=f1, y=f2, z=target, opacity=0.5, **kwargs)
    
    # ë‹¤ì¤‘ì„ í˜•íšŒê·€ë°©ì •ì‹ í•™ìŠµ
    model = LinearRegression()
    model.fit(df[[f1, f2]], df[target])    

    # ì¢Œí‘œì¶• ì„¤ì •
    x_axis = np.linspace(df[f1].min(), df[f1].max(), length)
    y_axis = np.linspace(df[f2].min(), df[f2].max(), length)
    coords = list(itertools.product(x_axis, y_axis))
    
    # ì˜ˆì¸¡
    pred = model.predict(coords)
    z_axis = pred.reshape(length, length).T
    
    # plot ì˜ˆì¸¡í‰ë©´
    plot.add_trace(go.Surface(x=x_axis, y=y_axis, z=z_axis, colorscale='Viridis'))
    
    return plot

surface_3d(
    train,
    f1='GrLivArea', 
    f2='OverallQual', 
    target='SalePrice',  
    title='House Prices'
)    
```

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-10 21 20 00](https://user-images.githubusercontent.com/79494088/128865647-437d55df-0b9c-4c0d-a789-5627518eaead.png)

# 4ï¸âƒ£ íšŒê·€ê³„ìˆ˜ í•´ì„/ëª¨ë¸ í‰ê°€
- ë‹¨ìˆœì„ í˜•íšŒê·€ì‹ : $y = \beta_0 + \beta_1 x $ 
- 2íŠ¹ì„±ì˜ ë‹¤ì¤‘ì„ í˜•íšŒê·€ ì‹ $y = \beta_0 + \beta_1x_1 + \beta_2x_2$
- ê³„ìˆ˜ë“¤(coefficients or parameters) $\beta_0$~$\beta_2$ ëŠ” ì–´ë–»ê²Œ êµ¬í• ê¹Œ?

```py
# ì ˆí¸(intercept)ê³¼ ê³„ìˆ˜ë“¤(coefficients)
model.intercept_, model.coef_

# (-102743.02342270731, array([   54.40145532, 33059.44199506]))

# íšŒê·€ì‹ì„ ë§Œë“¤ì–´ ë´…ì‹œë‹¤.
b0 = model.intercept_
b1, b2 = model.coef_

print(f'y = {b0:.0f} + {b1:.0f}x\u2081 + {b2:.0f}x\u2082')

# y = -102743 + 54xâ‚ + 33059xâ‚‚
```

- $\beta_1$ê³¼ $\beta_2$ ëª¨ë‘ ì–‘ìˆ˜ì´ë‹¤. ì´ê²ƒì€ $x_1$, $x_2$ì´ ì¦ê°€í•  ë•Œë§ˆë‹¤ $y$ ë„ ì¦ê°€í•œë‹¤ëŠ” ëœ»ì´ë‹¤.
- ë§Œì•½ ìŒìˆ˜ì¸ ê²½ìš°ì—ëŠ” $y$ ê°€ ê°ì†Œí•œë‹¤ëŠ” ëœ»ì´ë‹¤.
- ê°€ìƒì˜ ê´€ì¸¡ ë°ì´í„°ë¥¼ ë„£ì–´ ëª¨ë¸ì´ ì–´ë–»ê²Œ ì˜ˆì¸¡í•˜ëŠ”ì§€ ê´€ì°°

```py
model.predict([[2000, 10]])

# array([336654.30716253])

model.predict([[2000, 3]])

# array([105238.21319714])
```

- ì„ í˜•íšŒê·€ëŠ” ë‹¤ë¥¸ ML ëª¨ë¸ì— ë¹„í•´ ìƒëŒ€ì ìœ¼ë¡œ í•™ìŠµì´ ë¹ ë¥´ê³  ì„¤ëª…ë ¥ì´ ê°•í•˜ë‹¤.
- í•˜ì§€ë§Œ ì„ í˜• ëª¨ë¸ì´ë¯€ë¡œ ê³¼ì†Œì í•©(underfitting)ì´ ì˜ ì¼ì–´ë‚œë‹¤ëŠ” ë‹¨ì ì´ ìˆë‹¤.
{% raw %}
## íšŒê·€ëª¨ë¸ì„ í‰ê°€í•˜ëŠ” í‰ê°€ì§€í‘œë“¤(evaluation metrics)
- MSE (Mean Squared Error) = 
$\frac{1}{n}\sum_{i=1}^{n}(y_{i} - \hat{y_{i}})^{2}$
- MAE (Mean absolute error) = $\frac{1}{n}\sum_{i=1}^{n}\left \vert  y_{i} - \hat{y_{i}} \right \vert$
- RMSE (Root Mean Squared Error) = 
$\sqrt{MSE}$
- R-squared (Coefficient of determination) = 
$1 - \frac{\sum_{i=1}^{n}(y_{i} - \hat{y_{i}})^{2}}{\sum_{i=1}^{n}(y_{i} - \bar{y_{i}})^{2}} = 1 - \frac{SSE}{SST} = \frac {SSR}{SST}$
- ì°¸ê³ 
  - SSE(Sum of Squares `Error`, ê´€ì¸¡ì¹˜ì™€ ì˜ˆì¸¡ì¹˜ ì°¨ì´): $\sum_{i=1}^{n}(y_{i} - \hat{y_{i}})^{2}$
  - SSR(Sum of Squares due to `Regression`, ì˜ˆì¸¡ì¹˜ì™€ í‰ê·  ì°¨ì´): $\sum_{i=1}^{n}(\hat{y_{i}} - \bar{y_{i}})^{2}$
  - SST(Sum of Squares `Total`, ê´€ì¸¡ì¹˜ì™€ í‰ê·  ì°¨ì´): $\sum_{i=1}^{n}(y_{i} - \bar{y_{i}})^{2}$ , SSE + SSR 
{% endraw %}

```py
# í›ˆë ¨
model.fit(X_train, y_train)
y_pred = model.predict(X_train)
hMAE = mean_absolute_error(y_train, y_pred)
hMSE = mean_squared_error(y_train, y_pred)
hRMSE = np.sqrt(hMSE)
hR2 = r2_score(y_train, y_pred)
print(f'í›ˆë ¨ MAE : {hMAE}\ní›ˆë ¨ MSE : {hMSE}\ní›ˆë ¨ RMSE : {hRMSE}\ní›ˆë ¨ R2 : {hR2}')

# í…ŒìŠ¤íŠ¸
y_pred = model.predict(X_test)
mae = mean_absolute_error(y_test, y_pred)
tMAE =  mean_absolute_error(y_test, y_pred)
tMSE = mean_squared_error(y_test, y_pred)
tRMSE = np.sqrt(tMSE)
tR2 = r2_score(y_test, y_pred)
print(f'í…ŒìŠ¤íŠ¸ MAE : {tMAE}\ní…ŒìŠ¤íŠ¸ MSE : {tMSE}\ní…ŒìŠ¤íŠ¸ RMSE : {tRMSE}\ní…ŒìŠ¤íŠ¸ R2 : {tR2}')

'''
í›ˆë ¨ MAE : 170777.34212565765
í›ˆë ¨ MSE : 67099053309.953606
í›ˆë ¨ RMSE : 259034.84960513248
í›ˆë ¨ R2 : 0.5076085988757708
í…ŒìŠ¤íŠ¸ MAE : 179252.52593261775
í…ŒìŠ¤íŠ¸ MSE : 71083994178.75656
í…ŒìŠ¤íŠ¸ RMSE : 266615.81757044455
í…ŒìŠ¤íŠ¸ R2 : 0.45999300199894533
'''
```

# 5ï¸âƒ£ ê³¼ì í•©(Overfitting)ê³¼ ê³¼ì†Œì í•©(Underfitting)

- ì¼ë°˜í™”
  - í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ ë§Œë“¤ì–´ë‚´ëŠ” ì˜¤ì°¨ë¥¼ ì¼ë°˜í™” ì˜¤ì°¨ë¼ê³  ë¶€ë¥¸ë‹¤.
  - í›ˆë ¨ ë°ì´í„°ì—ì„œì™€ ê°™ì´ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œë„ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚´ëŠ” ëª¨ë¸ì€ ì¼ë°˜í™”ê°€ ì˜ ëœ ëª¨ë¸ì´ë¼ê³  ë¶€ë¥¸ë‹¤.
  - ëª¨ë¸ì´ ë„ˆë¬´ í›ˆë ¨ë°ì´í„°ì— ê³¼í•˜ê²Œ í•™ìŠµ(ê³¼ì í•©)ì„ í•˜ì§€ ì•Šë„ë¡ í•˜ëŠ” ë§ì€ ì¼ë°˜í™” ë°©ë²•ë“¤ì´ ìˆë‹¤.
- ë‹¹ì—°íˆ ì˜ˆì¸¡ëª¨ë¸ì´ í›ˆë ¨ë°ì´í„°ì—ì„œë³´ë‹¤ í…ŒìŠ¤íŠ¸ë°ì´í„°ì—ì„œ ì˜¤ì°¨ê°€ ì ê²Œ ë‚˜ì˜¤ê¸°ë¥¼ ê¸°ëŒ€í•˜ì§€ë§Œ í˜„ì‹¤ì ìœ¼ë¡œ ëª¨ë“  ë°ì´í„°ë¥¼ ì–»ì„ ìˆ˜ ì—†ê¸° ë•Œë¬¸ì— í›ˆë ¨ë°ì´í„°ë¡œë¶€í„° ì¼ë°˜í™”ê°€ ì˜ ë˜ëŠ” ëª¨ë¸ì„ í•™ìŠµì‹œì¼œì•¼ í•œë‹¤.
  - ê³¼ì í•© : í›ˆë ¨ë°ì´í„°ì—ë§Œ íŠ¹ìˆ˜í•œ ì„±ì§ˆì„ ê³¼í•˜ê²Œ í•™ìŠµí•´ ì¼ë°˜í™”ë¥¼ ëª»í•´ ê²°êµ­ í…ŒìŠ¤íŠ¸ë°ì´í„°ì—ì„œ ì˜¤ì°¨ê°€ ì»¤ì§€ëŠ” í˜„ìƒ
  - ê³¼ì†Œì í•© : í›ˆë ¨ë°ì´í„°ì— ê³¼ì í•©ë„ ëª»í•˜ê³  ì¼ë°˜í™” ì„±ì§ˆë„ í•™ìŠµí•˜ì§€ ëª»í•´, í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë°ì´í„° ëª¨ë‘ì—ì„œ ì˜¤ì°¨ê°€ í¬ê²Œ ë‚˜ì˜¤ëŠ” ê²½ìš°
- ë¨¸ì‹ ëŸ¬ë‹ê³¼ì • ì¤‘ì—ì„œ ê³¼ì í•©ì€ í”¼í•  ìˆ˜ ì—†ëŠ” ë¬¸ì œì´ê³  ì™„ì „íˆ ê·¹ë³µí•  ìˆ˜ë„ ì—†ë‹¤. ê·¸ë˜ì„œ ëŒ€ë¶€ë¶„ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì€ ì´ëŸ° ê³¼ì í•©ì„ ì™„í™”ì‹œí‚¬ ìˆ˜ ìˆëŠ” ë°©ë²•ì„ ì œê³µí•œë‹¤.
- ë‹¤ì¤‘ ì„ í˜• íšŒê·€ì—ì„œëŠ” $R^2$ê°’ìœ¼ë¡œ ê³¼ì í•© ì—¬ë¶€ë¥¼ íŒë‹¨í•  ìˆ˜ ìˆë‹¤.

## ë¶„ì‚°/í¸í–¥ íŠ¸ë ˆì´ë“œì˜¤í”„
- ê³¼/ì†Œì í•©ì€ ì˜¤ì°¨ì˜ í¸í–¥(Bias)ê³¼ ë¶„ì‚°(Variance)ê°œë…ê³¼ ê´€ê³„ê°€ ìˆë‹¤.
- ê²°ë¡ 
  - ë¶„ì‚°ì´ ë†’ì€ê²½ìš°ëŠ”, ëª¨ë¸ì´ í•™ìŠµ ë°ì´í„°ì˜ ë…¸ì´ì¦ˆì— ë¯¼ê°í•˜ê²Œ ì í•©í•˜ì—¬ í…ŒìŠ¤íŠ¸ë°ì´í„°ì—ì„œ ì¼ë°˜í™”ë¥¼ ì˜ ëª»í•˜ëŠ” ê²½ìš° ì¦‰ ê³¼ì í•© ìƒíƒœ
  - í¸í–¥ì´ ë†’ì€ê²½ìš°ëŠ”, ëª¨ë¸ì´ í•™ìŠµ ë°ì´í„°ì—ì„œ, íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë³€ìˆ˜ì˜ ê´€ê³„ë¥¼ ì˜ íŒŒì•…í•˜ì§€ ëª»í•´ ê³¼ì†Œì í•© ìƒíƒœ

{% raw %} $${\displaystyle \operatorname {E} _{D}{\Big [}{\big (}y-{\hat {f}}(x;D){\big )}^{2}{\Big ]}={\Big (}\operatorname {Bias} _{D}{\big [}{\hat {f}}(x;D){\big ]}{\Big )}^{2}+\operatorname {Var} _{D}{\big [}{\hat {f}}(x;D){\big ]}+\sigma ^{2}}$$

$${\displaystyle \operatorname {Bias} _{D}{\big [}{\hat {f}}(x;D){\big ]}=\operatorname {E} _{D}{\big [}{\hat {f}}(x;D){\big ]}-f(x)}$$

$${\displaystyle \operatorname {Var} _{D}{\big [}{\hat {f}}(x;D){\big ]}=\operatorname {E} _{D}[{\big (}\operatorname {E} _{D}[{\hat {f}}(x;D)]-{\hat {f}}(x;D){\big )}^{2}]}$$ {% endraw %}

### EX) ë…ë¦½ë³€ìˆ˜ì™€ ì¢…ì†ë³€ìˆ˜ê°€ ë¹„ì„ í˜•ê´€ê³„ì¸ ëª¨ë¸ë¡œ í•™ìŠµì„ í•´ì•¼í•˜ëŠ” ë°ì´í„°ì—ì„œ,
- ë‹¨ìˆœì„ í˜•ëª¨ë¸ë¡œ í•™ìŠµí•˜ëŠ” ê²½ìš°
- ë°ì´í„° í¬ì¸í„°ë¥¼ ëª¨ë‘ ì§€ë‚˜ê°€ë„ë¡ ê³¡ì„  í”¼íŒ…ì´ ê°€ëŠ¥í•œ ë‹¤í•­ëª¨ë¸ë¡œ í•™ìŠµì„ ì§„í–‰í•œë‹¤ê³  ê°€ì •
  - ì„ í˜•ëª¨ë¸ ì˜ˆì¸¡ì€ í•™ìŠµë°ì´í„°ì—ì„œ íƒ€ê²Ÿê°’ê³¼ ì˜¤ì°¨ê°€ í¬ë‹¤. ì´ë¥¼ "í¸í–¥ì´ ë†’ë‹¤"ê³  í•œë‹¤.(ê³¼ì†Œì í•©)
  - í•˜ì§€ë§Œ í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë‘ ë°ì´í„°ì—ì„œ ê·¸ ì˜¤ì°¨ê°€ ë¹„ìŠ·í•˜ë‹¤. ì´ë¥¼ "ë¶„ì‚°ì´ ë‚®ë‹¤"ê³  í•©ë‹ˆë‹¤. (ì˜¤ì°¨ëŠ” ì—¬ì „íˆ ë§ì§€ë§Œ)

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-10 14 03 25](https://user-images.githubusercontent.com/79494088/128810921-3d37189c-7f41-44af-b0f3-8eb4322ea426.png)

- ê³¡ì„ ì„ í”¼íŒ…í•œ ëª¨ë¸ì—ì„œëŠ”, í•™ìŠµë°ì´í„°ì—ì„œ ì˜¤ì°¨ê°€ 0ì— ê°€ê¹Œìš°ë‚˜("ë‚®ì€ í¸í–¥"), í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ ì˜¤ì°¨ê°€ ë§ì•„ì§„ë‹¤.
- ì´ë ‡ê²Œ í•œ ë°ì´í„°ì„¸íŠ¸ì—ì„œëŠ” ì˜¤ì°¨ê°€ ì ì€ë° ë‹¤ë¥¸ ë°ì´í„° ì„¸íŠ¸ì—ì„œëŠ” ì˜¤ì°¨ê°€ ë§ì´ ëŠ˜ì–´ë‚˜ëŠ” í˜„ìƒ(ë°ì´í„° ì„¸íŠ¸ì˜ ì¢…ë¥˜ì— ë”°ë¼ ì˜ˆì¸¡ê°’ ë¶„ì‚°ì´ ë†’ì„ ë•Œ)ì„ ê³¼ì í•©ì´ë¼ê³  í•˜ë©° "ë¶„ì‚°ì´ ë†’ë‹¤"ë¼ê³  í•œë‹¤.

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-10 14 15 55](https://user-images.githubusercontent.com/79494088/128811913-652907d6-2859-4a29-a355-7449376f25f9.png)

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-10 14 16 13](https://user-images.githubusercontent.com/79494088/128811936-9bc2e2e3-a971-4bf0-a5e0-ad51b294219e.png)

- ë§Œë“¤ê¸° ì–´ë µì§€ë§Œ, í¸í–¥ë„ ì ê³  ë¶„ì‚°ë„ ì ì€ ëª¨ë¸ì´ ì¢‹ì€ ëª¨ë¸ì´ë‹¤.
- ì–´ë–¤ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ë“  í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œì˜ ëª¨ë¸ì˜ ì„±ëŠ¥ê³¼ ê·¸ ì°¨ì´ë¥¼ ë³´ê³  ê³¼ì í•©ê³¼ ê³¼ì†Œì í•©ì„ ì ì ˆí•˜ê²Œ êµ¬ë¶„í•´ ë‚¼ ìˆ˜ ìˆëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤.

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-10 14 17 31](https://user-images.githubusercontent.com/79494088/128812052-a7165b17-2e62-4da9-a864-f744f0058d2d.png)

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-10 14 17 23](https://user-images.githubusercontent.com/79494088/128812056-431edb11-54f8-4ea7-b921-46e7a7f6d152.png)

- ëª¨ë¸ì˜ ë³µì¡ì„±ì— ë”°ë¼ ì„±ëŠ¥ ê·¸ë˜í”„ë¥¼ ê·¸ë ¤ë³´ë©´, ëª¨ë¸ì´ ë³µì¡í•´ì§ˆìˆ˜ë¡ í›ˆë ¨ë°ì´í„° ì„±ëŠ¥ì€ ê³„ì† ì¦ê°€í•˜ëŠ”ë° ê²€ì¦ë°ì´í„° ì„±ëŠ¥ì€ ì–´ëŠì •ë„ ì¦ê°€í•˜ë‹¤ê°€ ì¦ê°€ì„¸ê°€ ë©ˆì¶”ê³  ì˜¤íˆë ¤ ë‚®ì•„ì§€ëŠ” ì§€ì ì„ ì°¾ì„ ìˆ˜ ìˆë‹¤.
- ìš°ë¦¬ëŠ” ë³´í†µ ì´ ì‹œì ì„ ê³¼ì í•©ì´ ì¼ì–´ë‚˜ëŠ” ì‹œì ìœ¼ë¡œ íŒŒì•…í•˜ê³  ë” ë³µì¡í•œ ëª¨ë¸ì€ ë¶ˆí•„ìš”í•¨ì„ ì•Œê²Œ ëœë‹¤.
- ì•ìœ¼ë¡œ ë” ë§ì€ featuresë¥¼ ì‚¬ìš©í•˜ê³  ì—¬ëŸ¬ ëª¨ë¸ë“¤ì„ ë°°ìš°ê³  ì‚¬ìš©í•˜ê²Œ ë  í…ë° ê³¼ì í•©/ê³¼ì†Œì í•©ì— ëŒ€í•œ ë‚´ìš©ì€ ê³„ì† ìˆ™ì§€í•´ì•¼í•œë‹¤.

## ë‹¤í•­íšŒê·€ëª¨ë¸ ì´ìš© ê³¼ì í•© í…ŒìŠ¤íŠ¸
- ë…ë¦½ë³€ìˆ˜ì™€ íƒ€ê²Ÿë³€ìˆ˜ ì‚¬ì´ì— ë¹„ì„ í˜• ê´€ê³„ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ë‹¤í•­íšŒê·€ëª¨ë¸(polynomial regression)ì˜ ì°¨ìˆ˜(degrees)ë¥¼ ì¡°ì •í•´ íšŒê·€ê³¡ì„ ì„ ë§Œë“¤ì–´ë³´ëŠ” í…ŒìŠ¤íŠ¸

```py
# ì‹¤í—˜ì— ì‚¬ìš©í•  ëœë¤ ë°ì´í„°ë¥¼ ë§Œë“­ë‹ˆë‹¤ (30, 2)
rng = np.random.RandomState(1)
data = np.dot(rng.rand(2, 2), rng.randn(2, 30)).T
X = pd.DataFrame([i[0] for i in data])
y = pd.DataFrame([i[1] for i in data])

from sklearn.model_selection import train_test_split

## X_train, X_test, y_train, y_test
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)

from sklearn.preprocessing import PolynomialFeatures
X1 = np.arange(6).reshape(3, 2)
print(X1)

poly = PolynomialFeatures(2)
X_poly = poly.fit_transform(X1)

# poly = PolynomialFeatures(interaction_only=True)
# poly.fit_transforim(X)

'''
[[0 1]
 [2 3]
 [4 5]]
'''
```

- Sklearnì˜ PolynomialFeaturesëŠ” ë‹¤í•­íšŒê·€ëª¨ë¸ì„ ì‰½ê²Œ êµ¬í˜„í•˜ë„ë¡ ë„ì™€ì¤€ë‹¤.
- ì´ë¦„ì—ì„œ ì•Œ ìˆ˜ ìˆë“¯ì´ ë‹¤í•­ íŠ¹ì„±(polynomial features)ì„ ë°©ì •ì‹ì— ì¶”ê°€í•˜ëŠ” ê²ƒ
- ë‹¤í•­ íŠ¹ì„±ì€ íŠ¹ì„±ë“¤ì˜ ìƒí˜¸ì‘ìš©ì„ ë³´ì—¬ì¤„ ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ìƒí˜¸ì‘ìš©íŠ¹ì„±(interaction features)ë¼ê³ ë„ ë¶€ë¥¸ë‹¤.

```py
## X_poly: [1, a, b, a^2, ab, b^2]
X_poly
'''
array([[ 1.,  0.,  1.,  0.,  0.,  1.],
       [ 1.,  2.,  3.,  4.,  6.,  9.],
       [ 1.,  4.,  5., 16., 20., 25.]])
'''

from IPython.display import display, HTML
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import PolynomialFeatures
import matplotlib.pyplot as plt

plt.rcParams["figure.figsize"] = (5,5)

# ë‹¤í•­íšŒê·€ëª¨ë¸ë„ ê²°êµ­ ë‹¤ì¤‘ì„ í˜•íšŒê·€ëª¨ë¸ë¡œ ë³€í˜•í•˜ì—¬ ëª¨ë¸ì„ ë§Œë“¤ ìˆ˜ ìˆëŠ” ì„ í˜•ëª¨ë¸ì…ë‹ˆë‹¤.
def PolynomialRegression(degree=2, **kwargs):
    return make_pipeline(PolynomialFeatures(degree), 
                         LinearRegression(**kwargs))


polynomial_degrees = [1, 3, 4, 6, 10, 20]
train_r2s = []
test_r2s = []

for degree in polynomial_degrees:
    model = PolynomialRegression(degree)
    print(f'Degree={degree}')
    
    model.fit(X_train, y_train)
    train_r2 = model.score(X_train, y_train)
    test_r2 = model.score(X_test, y_test)
    display(HTML(f'<b style="color: blue">train R2 {train_r2:.2f}</b>'))
    display(HTML(f'<b style="color: red">test R2 {test_r2:.2f}</b>'))

    plt.scatter(X_train, y_train, color='blue', alpha=0.5)
    plt.scatter(X_test, y_test, color='red', alpha=0.5)
    

    x_domain = np.linspace(X.min(), X.max())
    curve = model.predict(x_domain)
    plt.plot(x_domain, curve, color='blue')
    plt.axis([-2., 2.0, -0.5, 0.5])
    plt.show()
    display(HTML('<hr/>'))
    
    train_r2s.append(train_r2)
    test_r2s.append(test_r2)
```

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-10 14 23 29](https://user-images.githubusercontent.com/79494088/128812548-edb334c2-5daf-48a2-97d7-6cc1bf7ad535.png)

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-10 14 24 06](https://user-images.githubusercontent.com/79494088/128812614-3d562f74-a84a-44c9-bc1c-5088e697e31e.png)

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-10 14 24 23](https://user-images.githubusercontent.com/79494088/128812644-b3cc419b-c6d0-49fa-8c8e-eb788f72e0d2.png)

- ëª¨ë¸ ë³µì¡ë„ê°€(ì°¨ìˆ˜) ì˜¬ë¼ê°ˆ ìˆ˜ë¡ ê³¼ì í•© ë˜ì–´ í›ˆë ¨ $R^2$ ê°’ì´ ì¢‹ì•„ì§€ì§€ë§Œ í…ŒìŠ¤íŠ¸ $R^2$ ê°’ì€ ì¤„ì–´ë“œëŠ” ê²ƒì„ í™•ì¸ í•  ìˆ˜ ìˆë‹¤.

## ì°¸ê³ ìë£Œ

### plotly
- [plotly API reference](https://plotly.github.io/plotly.py-docs/index.html)

### í›ˆë ¨/í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ë¶„ë¦¬
- [Forecasting,Chapter 3.4](https://otexts.com/fpp2/accuracy.html)
- [How (and why) to create a good validation set](https://www.fast.ai/2017/11/13/validation-sets/)

### Bias-Variance Tradeoff
- [Bias/Variance](https://youtu.be/SjQyLhQIXSM)
- [Machine Learning Fundamentals: Bias and Variance](https://youtu.be/EuBBz3bI-aA)
- [The Bias-Variance Tradeoff](https://towardsdatascience.com/the-bias-variance-tradeoff-8818f41e39e9)
- [Python Data Science Handbook,Chapter 5.3](https://jakevdp.github.io/PythonDataScienceHandbook/05.03-hyperparameters-and-model-validation.html#The-Bias-variance-trade-off)

### Regression
- [5 step process](https://jakevdp.github.io/PythonDataScienceHandbook/05.02-introducing-scikit-learn.html#Basics-of-the-API)
- [Scikit-Learn LinearRegression documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)
- [Sum of Squares](https://corporatefinanceinstitute.com/resources/knowledge/other/sum-of-squares/)
- [$R^2$](https://en.wikipedia.org/wiki/Coefficient_of_determination)
- [ìƒê´€ê³„ìˆ˜ & ê²°ì •ê³„ìˆ˜](https://m.blog.naver.com/istech7/50153288534)
- [íšŒê·€ì˜ ì˜¤ë¥˜ì§€í‘œ ì•Œì•„ë³´ê¸°](https://partrita.github.io/posts/regression-error/)