---
title: '[Data Pipeline Project] Spotify recommend(4) AWS S3와 Athena'
description: 스포티파이 API 이용 데이터 파이프라인 구축 토이 프로젝트 AWS S3과 Athena를 통해 Data Lake 구축
categories:
 - Project
tags: [Project, Data Engineering, Data Pipeline, NoSQL, AWS, S3, Athena, 데이터 파이프라인, 데이터 엔지니어링, 아테나]
---

- [Sixpotify Facebook Page 바로가기](https://www.facebook.com/sixpotify)
- [Sixpotify GitHub Repository 바로가기](https://github.com/6mini/sixpotify)
- 전체 포스팅 목록
    - [[Data Pipeline Project] Spotify recommend(1) Spotify API](https://6mini.github.io/project/2021/10/14/spotify/)
    - [[Data Pipeline Project] Spotify recommend(2) AWS PostgreSQL](https://6mini.github.io/project/2021/10/14/spotify2/)
    - [[Data Pipeline Project] Spotify recommend(3) AWS DynamoDB](https://6mini.github.io/project/2021/10/14/Spotify3/)
    - [[Data Pipeline Project] Spotify recommend(4) AWS S3와 Athena](https://6mini.github.io/project/2021/10/14/spotify4/)
    - [[Data Pipeline Project] Spotify recommend(5) Apache Spark](https://6mini.github.io/project/2021/10/14/spotify5/)
    - [[Data Pipeline Project] Spotify recommend(6) 자동화](https://6mini.github.io/project/2021/10/15/spotify6/)
    - [[Data Pipeline Project] Spotify recommend(7) Chatbot](https://6mini.github.io/project/2021/10/17/spotify7/)
    - [[Data Pipeline Project] Spotify recommend(8) Final](https://6mini.github.io/project/2021/11/29/spotify8/)

# Data Lake

## 시대의 변화에 따른 저장방식의 진화

**구분**|**Data Lake**|**Data Warehouse**
**Data Structure**|Raw|Processed
**Purpose of Data**|Not Yet Determined|In Use
**Users**|Data Scientists|Business Professionals
**Accessibility**|High / Quick to update|Complicated / Costly

<img width="987" alt="스크린샷 2021-10-04 16 25 49" src="https://user-images.githubusercontent.com/79494088/135810173-45d8fc54-a7df-4c2b-a458-b68f4fb01618.png">


## Architecture

<img width="1377" alt="스크린샷 2021-10-04 16 28 07" src="https://user-images.githubusercontent.com/79494088/135810508-b86b349f-d0ea-40fe-9479-870a986db5ef.png">

### Data Pipeline
- 중간에 pipe가 고장난다면?

<img width="1353" alt="스크린샷 2021-10-04 16 29 53" src="https://user-images.githubusercontent.com/79494088/135810718-3b365de7-af92-4c7c-80ce-776248c8a365.png">

# S3
- buckit 생성

<img width="820" alt="스크린샷 2021-10-04 16 37 25" src="https://user-images.githubusercontent.com/79494088/135811649-d3a5c60d-e102-4ab6-848f-521da92d4b48.png">

```py
import sys
import os
import logging
import boto3
import requests
import base64
import json
import psycopg2
from datetime import datetime
import pandas as pd
import jsonpath  # pip3 install jsonpath --user

client_id = ""
client_secret = ""

host = ""
port = 5432
username = ""
database = ""
password = ""


def main():

    try:
        conn = psycopg2.connect(
            host=host,
            database=database,
            user=username,
            password=password)
        cursor = conn.cursor()
    except:
        logging.error("could not connect to rds")
        sys.exit(1)

    headers = get_headers(client_id, client_secret)

    # RDS - 아티스트 ID를 가져오고
    cursor.execute("SELECT id FROM artists LIMIT 10")
    top_track_keys = {
        "id": "id",
        "name": "name",
        "popularity": "popularity",
        "external_url": "external_urls.spotify"
    }
    # Top Tracks Spotify 가져오고
    # Parquet화 : 스파크가 좋아하는 형태
    top_tracks = []
    for (id, ) in cursor.fetchall():
        URL = "https://api.spotify.com/v1/artists/{}/top-tracks".format(id)
        params = {
            'country': 'US'
        }

        r = requests.get(URL, params=params, headers=headers)
        raw = json.loads(r.text)

        for i in raw['tracks']:
            top_track = {}
            for k, v in top_track_keys.items():
                value = jsonpath.jsonpath(i, v)
                if type(value) == bool:
                    continue
                top_track.update({k: value[0]})
                top_track.update({'artist_id': id})
                top_tracks.append(top_track)
    

    # track_ids

    track_ids = [i['id'] for i in top_tracks]

    top_tracks = pd.DataFrame(top_tracks)
    top_tracks.to_parquet('top-tracks.parquet', engine='pyarrow', compression='snappy')

    dt = datetime.utcnow().strftime("%Y-%m-%d")

    s3 = boto3.resource('s3')
    object = s3.Object('6mini-spotify', 'top-tracks/dt={}/top-tracks.parquet'.format(dt)) # partition
    data = open('top-tracks.parquet', 'rb')
    object.put(Body=data)
    # S3 import

    tracks_batch = [track_ids[i: i+100] for i in range(0, len(track_ids), 100)]
    audio_features = []
    for i in tracks_batch:
        ids = ','.join(i)
        URL = "https://api.spotify.com/v1/audio-features/?ids={}".format(ids)

        r = requests.get(URL, headers=headers)
        raw = json.loads(r.text)

        audio_features.extend(raw['audio_features'])

    audio_features = pd.DataFrame(audio_features)
    audio_features.to_parquet('audio-features.parquet', engine='pyarrow', compression='snappy')

    s3 = boto3.resource('s3')
    object = s3.Object('6mini-spotify', 'audio-features/dt={}/top-tracks.parquet'.format(dt)) # dt : 파티션 -> 날짜로 만든다
    data = open('audio-features.parquet', 'rb')
    object.put(Body=data)


def get_headers(client_id, client_secret):

    endpoint = "https://accounts.spotify.com/api/token"
    encoded = base64.b64encode("{}:{}".format(client_id, client_secret).encode('utf-8')).decode('ascii')

    headers = {
        "Authorization": "Basic {}".format(encoded)
    }
    payload = {
        "grant_type": "client_credentials"
    }

    r = requests.post(endpoint, data=payload, headers=headers)

    access_token = json.loads(r.text)['access_token']

    headers = {
        "Authorization": "Bearer {}".format(access_token)
    }

    return headers


if __name__=='__main__':
    main()
```

## Amazon S3 확인

![스크린샷 2021-10-05 15 04 23](https://user-images.githubusercontent.com/79494088/135969102-b5d881e9-033a-4c49-a2ea-ca90172eacad.png)

# Presto
- Spark의 단점이라 하면, 물론 Spark SQL도 있지만, 어느 정도 Scripting이 필요한 부분이 있다.
- 다양한 multiple data source를 single query를 통해서 진행 할 수 있다.
- Hadoop의 경우는 performance나 여러가지 data analytics 할때 여러가지 issue들이있으며 이전 방식이기 때문에 최근에는 Spark와 Presto로 넘어오는 추세이다.
- AWS는 Presto기반인 Athena를 통해서 S3의 데이터를 작업할 수 있다.


# Serverless
- 어떠한 요청이 들어올때 server를 띄우는데 지속적으로 요청이 들어온다면 계속적으로 병렬적인 server를 띄운다는 것이다.
- server안에서 용량을 정하는 것을 알아서 자동적으로 해결해 주므로 비용적인 문제를 보완한다.
- AWS에서 EC2같은 경우는 server 하나를 띄우는 것이고, Lambda가 Serverless의 개념을 갖는 서비스이다.
- Athena도 Serverless의 개념을 갖는 서비스이다.

## AWS Athena
- AWS Athena에서도 data lake의 시스템 형태로 데이터를 작업하더라도 query를 통해 작업을 하려면 data warehouse 처럼 table의 형식을 만들어야한다.
- AWS Glue를 쓰면 크롤러를 이용해서 저장되어있는 데이터의 로케이션에 들어가서 생성을 한다.
- 데이터를 S3으로 파이썬을 통해 옮기고 아테나를 통해 테이블을 만들고 쿼리를 통해 데이터를 뽑아오는 부분을 진행한다.

### query location

<img width="830" alt="스크린샷 2021-10-05 15 49 30" src="https://user-images.githubusercontent.com/79494088/135973954-71ba25fd-3668-426c-b27d-8324f9f20a51.png">

### query run
- table과 partition을 생성하고 repair를 진행한다.
- key와 velue가 모두 list가 아닌 형태여야 한다.

```sql
CREATE EXTERNAL TABLE IF NOT EXISTS top_tracks(
    id string,
    artsist_id string,
    name string,
    popularity int,
    external_url string
) PARTITIONED BY (dt string)
STORED AS PARQUET LOCATION 's3://6mini-spotify/top-tracks/' tblproperties("parquet.compress"="SNAPPY")

MSCK REPAIR TABLE top_tracks

SELECT * FROM top_tracks LIMIT 10


CREATE EXTERNAL TABLE IF NOT EXISTS audio_features(
    id string,
    danceability double,
    energy double,
    key int,
    loudness double,
    mode int,
    speechiness double,
    acousticness double,
    instrumentalness double
) PARTITIONED BY (dt string)
STORED AS PARQUET LOCATION 's3://6mini-spotify/audio-features/' tblproperties("parquet.compress"="SNAPPY")

MSCK REPAIR TABLE audio_features

SELECT * FORM audio_features
```

<img width="1282" alt="스크린샷 2021-10-05 21 38 26" src="https://user-images.githubusercontent.com/79494088/136023910-ebad98d0-3f9b-4eef-8a5f-5f6d57985286.png">

- [Presto Functions and Operators](https://prestodb.io/docs/current/functions.html)

```sql
SELECT
AVG(danceability),
AVG(loudness)
FROM audio_features
WHERE CAST(dt AS date) = CURRENT_DATE 
```

- Partition dt를 잘 선택해야한다.

_col0 | _col1
0.6186399999999985	|-7.51500000000001