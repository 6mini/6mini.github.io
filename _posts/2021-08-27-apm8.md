---
title: '[Applied Predictive Modeling] Interpreting ML Model'
description: PDP(Partal dependence plot, ë¶€ë¶„ì˜ì¡´ë„ê·¸ë¦¼) ì‹œê°í™” ë° í•´ì„, ê±”ë³„ ì˜ˆì¸¡ ì‚¬ë¡€ Shap value plots ì´ìš© ì„¤ëª…
categories:
 - Machine Learning
tags: [Machine Learning, Applied Predictive Modeling, Interpreting ML Model, PDP, Partal dependence plot, Shap value plots, ë¶€ë¶„ì˜ì¡´ë„ê·¸ë¦¼]
mathjax: enable
# 0ï¸âƒ£1ï¸âƒ£2ï¸âƒ£3ï¸âƒ£4ï¸âƒ£5ï¸âƒ£6ï¸âƒ£7ï¸âƒ£8ï¸âƒ£9ï¸âƒ£ğŸ”Ÿ
---

# 1ï¸âƒ£ Partial Dependence Plots(PDP)
- ë†’ì€ ì„±ëŠ¥ì˜ Modelingì„ ìœ„í•´ RandomForest, Boostingê³¼ ê°™ì€ Ensemble modelì„ ì£¼ë¡œ ì‚¬ìš©í•˜ê²Œ ëœë‹¤.
- ì´ëŸ° ë³µì¡ë„ê°€ ë†’ì€ Modelì€ Linear modelì— ë¹„í•´ í•´ì„í•˜ê¸° ì–´ë µë‹¤.
  - Complex model : ì´í•´í•˜ê¸° ì–´ë µì§€ë§Œ ì„±ëŠ¥ì´ ì¢‹ë‹¤.
  - Simple model : ì´í•´í•˜ê¸° ì‰½ì§€ë§Œ ì„±ëŠ¥ì´ ì•„ì‰½ë‹¤.
- ì˜ˆë¡œ RandomForest, Boostingì˜ ê²½ìš° ì‰½ê²Œ Feature importance ê°’ì„ ì–»ì„ ìˆ˜ ìˆëŠ”ë°, ì´ë¥¼ í†µí•´ì„œ ì•Œ ìˆ˜ ìˆëŠ” ê²ƒì€ ì–´ë–¤ íŠ¹ì„±ë“¤ì´ Modelì˜ ì„±ëŠ¥ì— ì¤‘ìš”í•˜ë‹¤, ë§ì´ ì“°ì¸ë‹¤ ì •ë„ë‹¤.
- íŠ¹ì„±ì˜ ê°’ì— ë”°ë¼ Target ê°’ì´ ì¦ê°€/ê°ì†Œí•˜ëŠëƒì™€ ê°™ì€ ì–´ë–»ê²Œ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€ì— ëŒ€í•œ ì •ë³´ëŠ” ì•Œ ìˆ˜ ì—†ë‹¤.
- PDP(Partal dependence plot, ë¶€ë¶„ì˜ì¡´ë„ê·¸ë¦¼)ë¥¼ ì‚¬ìš©í•˜ë©´ íŠ¹ì„±ë“¤ì´ íƒ€ê²Ÿì— ì–´ë–»ê²Œ ì˜í–¥ì„ ì£¼ëŠ”ì§€ íŒŒì•…í•  ìˆ˜ ìˆë‹¤.

```py
import pandas as pd

# Kaggle ë°ì´í„°ì„¸íŠ¸ì—ì„œ 10% ìƒ˜í”Œë§ëœ ë°ì´í„°
## Source: https://www.kaggle.com/wordsforthewise/lending-club
## 10% of expired loans (loan_status: ['Fully Paid' and 'Charged Off'])
## grades A-D
## term ' 36 months'

df = pd.read_csv('https://ds-lecture-data.s3.ap-northeast-2.amazonaws.com/lending_club/lending_club_sampled.csv')
df['issue_d'] = pd.to_datetime(df['issue_d'], infer_datetime_format=True)

# issue_dë¡œ ì •ë ¬
df = df.set_index('issue_d').sort_index()

df['interest_rate'] = df['int_rate'].astype(float)
df['monthly_debts'] = df['annual_inc'] / 12 * df['dti'] / 100

# 152 íŠ¹ì„± ì¤‘ 6íŠ¹ì„±ë§Œ ì‚¬ìš©
columns = ['annual_inc', # ì—°ìˆ˜ì…
           'fico_range_high', # ì‹ ìš©ì ìˆ˜ 
           'funded_amnt', # ëŒ€ì¶œ
           'title', # ëŒ€ì¶œ ëª©ì 
           'monthly_debts', # ì›”ê°„ ë¶€ì±„
           'interest_rate'] # ì´ììœ¨

df = df[columns]
df = df.dropna()

# ë§ˆì§€ë§‰ 10,000 ëŒ€ì¶œì€ í…ŒìŠ¤íŠ¸ì…‹
# í…ŒìŠ¤íŠ¸ì…‹ ì „ 10,000 ëŒ€ì¶œì´ ê²€ì¦ì…‹
# ë‚˜ë¨¸ì§€ëŠ” í•™ìŠµì…‹
test = df[-10000:]
val = df[-20000:-10000]
train = df[:-20000]


df.columns
'''
Index(['annual_inc', 'fico_range_high', 'funded_amnt', 'title',
       'monthly_debts', 'interest_rate'],
      dtype='object')
'''


test.shape, val.shape, train.shape
'''
((10000, 6), (10000, 6), (76408, 6))
'''


# íƒ€ê²Ÿì€ ì´ììœ¨
target = 'interest_rate' 
features = df.columns.drop('interest_rate')

X_train = train[features]
y_train = train[target]
X_val = val[features]
y_val = val[target]
X_test = test[features]
y_test = test[target]


# íƒ€ê²Ÿì´ ì•½ê°„ right skewed ë˜ì–´ ìˆìœ¼ë‚˜ í° ë¬¸ì œëŠ” ì•„ë‹˜
%matplotlib inline
import seaborn as sns
sns.displot(y_train, kde=True);
```

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-27 10 27 24](https://user-images.githubusercontent.com/79494088/131057034-d66aceef-7d38-47d8-ac0a-107e6e2bf8c1.png)


```py
# ì„ í˜•íšŒê·€ í•™ìŠµ
from category_encoders import TargetEncoder
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler

linear = make_pipeline(
    TargetEncoder(),  
    LinearRegression()
)

linear.fit(X_train, y_train)
print('R^2', linear.score(X_val, y_val))
'''
R^2 0.17585064958162422
'''


# ì„ í˜•íšŒê·€ ê²°ê³¼ í•´ì„
## íšŒê·€ê³„ìˆ˜
coefficients = linear.named_steps['linearregression'].coef_
pd.Series(coefficients, features)
'''
annual_inc        -0.000005 # ì—°ìˆ˜ì…
fico_range_high   -0.052805 # ì‹ ìš©ì ìˆ˜
funded_amnt        0.000021
title              1.007214
monthly_debts      0.000019
dtype: float64
'''

-0.000005 * 10000
'''
-0.05
ì—°ìˆ˜ì…ì˜ ì´ììœ¨ì— ëŒ€í•œ ì˜í–¥ : $10k ë” ë²Œ ìˆ˜ë¡ 0.05 ì´ìœ¨ì´ ì¤„ì–´ë“ ë‹¤.
'''

-0.052805 * 100
'''
-5.2805
ì‹ ìš©ì ìˆ˜ì˜ ì´ììœ¨ì— ëŒ€í•œ ì˜í–¥ : 100 point ì˜¤ë¥¼ ë•Œë§ˆë‹¤ 5% ê°€ê¹Œì´ ì´ìœ¨ì´ ì¤„ì–´ë“ ë‹¤.
'''


# Gradiant Boosting
from category_encoders import OrdinalEncoder
from sklearn.metrics import r2_score
from xgboost import XGBRegressor

encoder = OrdinalEncoder()
X_train_encoded = encoder.fit_transform(X_train) # í•™ìŠµë°ì´í„°
X_val_encoded = encoder.transform(X_val) # ê²€ì¦ë°ì´í„°

boosting = XGBRegressor(
    n_estimators=1000,
    objective='reg:squarederror', # default
    learning_rate=0.2,
    n_jobs=-1
)

eval_set = [(X_train_encoded, y_train), 
            (X_val_encoded, y_val)]

boosting.fit(X_train_encoded, y_train, 
          eval_set=eval_set,
          early_stopping_rounds=50
         )
'''
[0]	validation_0-rmse:9.44760	validation_1-rmse:10.18662
Multiple eval metrics have been passed: 'validation_1-rmse' will be used for early stopping.

Will train until validation_1-rmse hasn't improved in 50 rounds.
[1]	validation_0-rmse:7.74920	validation_1-rmse:8.52176
[2]	validation_0-rmse:6.42741	validation_1-rmse:7.23395
[3]	validation_0-rmse:5.41177	validation_1-rmse:6.24461
.
.
.
[100]	validation_0-rmse:2.53256	validation_1-rmse:3.27319
[101]	validation_0-rmse:2.53215	validation_1-rmse:3.27314
[102]	validation_0-rmse:2.53171	validation_1-rmse:3.27313
Stopping. Best iteration:
[52]	validation_0-rmse:2.59294	validation_1-rmse:3.26819

XGBRegressor(base_score=0.5, booster=None, colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
             importance_type='gain', interaction_constraints=None,
             learning_rate=0.2, max_delta_step=0, max_depth=6,
             min_child_weight=1, missing=nan, monotone_constraints=None,
             n_estimators=1000, n_jobs=-1, num_parallel_tree=1, random_state=0,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
             tree_method=None, validate_parameters=False, verbosity=None)
'''


y_pred = boosting.predict(X_val_encoded)
print('R^2', r2_score(y_val, y_pred))
'''
R^2 0.22947518106177134
'''
```

## í•œ íŠ¹ì„± ì‚¬ìš©
- ì„ í˜•ëª¨ë¸ì€ íšŒê·€ê³„ìˆ˜ë¥¼ ì´ìš©í•´ ë³€ìˆ˜ì™€ íƒ€ê²Ÿ ì‚¬ì´ì˜ ê´€ê³„ë¥¼ í•´ì„í•  ìˆ˜ ìˆì§€ë§Œ íŠ¸ë¦¬ëª¨ë¸ì€ í•  ìˆ˜ ì—†ë‹¤.
- ëŒ€ì‹  PDPë¥¼ ì‚¬ìš©í•´ ê°œë³„ íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ê°„ì˜ ê´€ê³„ë¥¼ ë³¼ ìˆ˜ ìˆë‹¤.

```py
# dpi(dots per inch) ìˆ˜ì¹˜ë¥¼ ì¡°ì •í•´ ì´ë¯¸ì§€ í™”ì§ˆ ì¡°ì •
import matplotlib.pyplot as plt
plt.rcParams['figure.dpi'] = 144


# ì„ í˜•íšŒê·€ì—ì„œì˜ annual_inc
from pdpbox.pdp import pdp_isolate, pdp_plot

feature = 'annual_inc'


isolated = pdp_isolate(
    model=linear, 
    dataset=X_val, 
    model_features=X_val.columns, 
    feature=feature,
    grid_type='percentile', # default='percentile', or 'equal'
    num_grid_points=10 # default=10
)
pdp_plot(isolated, feature_name=feature);
```

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-27 10 37 48](https://user-images.githubusercontent.com/79494088/131057794-2a53f6a6-34b5-4e43-ad6d-46f5662aa30e.png)

```py
# Gradiant Boostingì—ì„œì˜ annual_inc
isolated = pdp_isolate(
    model=boosting, 
    dataset=X_val_encoded, 
    model_features=X_val_encoded.columns, 
    feature=feature
)

pdp_plot(isolated, feature_name=feature);
```

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-27 10 39 35](https://user-images.githubusercontent.com/79494088/131057933-b20fb0d6-009f-4b0f-b27d-01641c88b8b9.png)

```py
# ì¼ë¶€ë¶„ í™•ëŒ€
pdp_plot(isolated, feature_name=feature)
plt.xlim((20000,150000));
```

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-27 10 40 09](https://user-images.githubusercontent.com/79494088/131057968-39c118db-ebef-4240-9a5c-bd25327c8208.png)

### 10ê°œì˜ ICE(Individual Conditional Expectation) curves
- í•œ ICE ê³¡ì„ ì€ í•˜ë‚˜ì˜ ê´€ì¸¡ì¹˜ì— ëŒ€í•´ ê´€ì‹¬ íŠ¹ì„±ì„ ë³€í™”ì‹œí‚´ì— ë”°ë¥¸ íƒ€ê²Ÿê°’ ë³€í™” ê³¡ì„ ì´ê³  ì´ ICEì˜ í‰ê· ì´ PDPì´ë‹¤.

```py
pdp_plot(isolated
         , feature_name=feature
         , plot_lines=True # ICE plots
         , frac_to_plot=0.001 # or 10 (# 10000 val set * 0.001)
         , plot_pts_dist=True) 

plt.xlim(20000,150000);
```

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-27 10 41 46](https://user-images.githubusercontent.com/79494088/131058095-2848b89c-770f-4d0d-9fc3-afd9bec2994c.png)

```py
X_val_encoded['annual_inc'].value_counts()
'''
60000.00    391
50000.00    388
65000.00    290
70000.00    282
40000.00    271
           ... 
52850.00      1
39998.40      1
29498.00      1
65101.71      1
15360.00      1
Name: annual_inc, Length: 1386, dtype: int64
'''
```

- [ICE curves -> PDPë¥¼ í‘œí˜„í•˜ëŠ” GIF(Christoph Molnar)](https://twitter.com/ChristophMolnar/status/1066398522608635904)
- í•œ íŠ¹ì„±ì— ëŒ€í•´ PDPë¥¼ ê·¸ë¦´ ê²½ìš° ë°ì´í„°ì…‹ ì‚¬ì´ì¦ˆì— grid pointsë¥¼ ê³±í•œ ìˆ˜ë§Œí¼ ì˜ˆì¸¡í•´ì•¼í•œë‹¤.

```py
isolated = pdp_isolate(
    model=boosting, 
    dataset=X_val_encoded, 
    model_features=X_val.columns, 
    feature=feature,
    # grid pointë¥¼ í¬ê²Œ ì£¼ë©´ ê²¹ì¹˜ëŠ” ì ì´ ìƒê²¨ Number of unique grid pointsëŠ” grid point ë³´ë‹¤ ì‘ì„ ìˆ˜ ìˆë‹¤.
    num_grid_points=100, # grid í¬ì¸íŠ¸ë¥¼ ë” ì¤„ ìˆ˜ ìˆë‹¤. default = 10
)


isolated = pdp_isolate(
    model=boosting, 
    dataset=X_val_encoded, 
    model_features=X_val.columns, 
    feature=feature,
    # grid pointë¥¼ í¬ê²Œ ì£¼ë©´ ê²¹ì¹˜ëŠ” ì ì´ ìƒê²¨ Number of unique grid pointsëŠ” grid point ë³´ë‹¤ ì‘ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    num_grid_points=100, # grid í¬ì¸íŠ¸ë¥¼ ë” ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. default = 10
)
'''
ì˜ˆì¸¡ìˆ˜:  1000000
'''


pdp_plot(isolated
         , feature_name=feature
         , plot_lines=True
         , frac_to_plot=0.01 # ICE curvesëŠ” 100ê°œ
         , plot_pts_dist=True )

plt.xlim(20000,150000);
```

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-27 10 45 18](https://user-images.githubusercontent.com/79494088/131058404-ba397ade-174e-417f-9100-386bfb5e3786.png)

## ë‘ íŠ¹ì„± ì‚¬ìš©

---
(ì°¸ê³ : PDPBox version <= 0.20 ê³¼ ëª‡ëª‡ matplotlib ë²„ì „ì—ì„œ `pdp_interact_plot`ì—ì„œ`plot_type='contour'` ì„¤ì •ì‹œ ì—ëŸ¬ê°€ ë°œìƒí•  ìˆ˜ ìˆë‹¤.
`TypeError: clabel() got an unexpected keyword argument 'contour_label_fontsize'`
https://github.com/SauceCat/PDPbox/issues/40)
---

```py
from pdpbox.pdp import pdp_interact, pdp_interact_plot

features = ['annual_inc', 'fico_range_high']

interaction = pdp_interact(
    model=boosting, 
    dataset=X_val_encoded,
    model_features=X_val.columns, 
    features=features
)

pdp_interact_plot(interaction, plot_type='grid', 
                  feature_names=features);
```

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-27 10 47 49](https://user-images.githubusercontent.com/79494088/131058619-f57a1d45-d38f-41c0-a3e4-068099e57a86.png)

```py
# Plotlyë¡œ 3D êµ¬í˜„
features
'''
['annual_inc', 'fico_range_high']
'''


# 2D PDP dataframe
interaction.pdp
```

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-27 10 48 47](https://user-images.githubusercontent.com/79494088/131058695-75087a04-bd49-43c9-806f-70d0803e7e38.png)

```py
type(interaction.pdp)
'''
pandas.core.frame.DataFrame
'''
```

```py
# ìœ„ì—ì„œ ë§Œë“  2D PDPë¥¼ í…Œì´ë¸”ë¡œ ë³€í™˜(using Pandas, df.pivot_table)í•˜ì—¬ ì‚¬ìš©

pdp = interaction.pdp.pivot_table(
    values='preds', # interaction['preds']
    columns=features[0], 
    index=features[1]
)[::-1] # ì¸ë±ìŠ¤ë¥¼ ì—­ìˆœìœ¼ë¡œ ë§Œë“œëŠ” slicing


pdp
```

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-27 13 14 45](https://user-images.githubusercontent.com/79494088/131070816-5a4f6ff7-2af9-4607-aa8e-f4ee5915cd3c.png)


```py
# ì–‘ë‹¨ì— ê·¹ë‹¨ì ì¸ annual_incë¥¼ drop
pdp = pdp.drop(columns=[1764.0, 1500000.0])


import plotly.graph_objs as go

surface = go.Surface(
    x=pdp.columns, 
    y=pdp.index, 
    z=pdp.values
)


layout = go.Layout(
    scene=dict(
        xaxis=dict(title=features[0]), 
        yaxis=dict(title=features[1]), 
        zaxis=dict(title=target)
    )
)

fig = go.Figure(surface, layout)
fig.show()
```

<img width="903" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-27 13 17 05" src="https://user-images.githubusercontent.com/79494088/131071000-2f152f1c-c656-4b98-b723-b929024ef34f.png">

## PDP Categiry íŠ¹ì„± ì‚¬ìš©
- ì¹´í…Œê³ ë¦¬ íŠ¹ì„±ì„ í•™ìŠµí•  ë•Œ Ordina Encoder, Target Encoder ë“±ì˜ ì¸ì½”ë”ë¥¼ ì‚¬ìš©í•œë‹¤
- Encodingì„ í•˜ë©´ í•™ìŠµ í›„ PDPë¥¼ ê·¸ë¦´ ë•Œ Encodingëœ ê°’ì´ ë‚˜ì˜¤ê²Œ ë˜ì–´ ì¹´í…Œê³ ë¦¬ íŠ¹ì„±ì˜ ì‹¤ì œê°’ì„ í™•ì¸í•˜ê¸° ì–´ë µë‹¤.

### PDPì— Encodingë˜ê¸° ì „ Category ê°’ì„ ë³´ì—¬ì£¼ê¸° ìœ„í•œ ë°©ë²•

```py
# Titanic Dataset
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import make_pipeline

df = sns.load_dataset('titanic')
df['age'] = df['age'].fillna(df['age'].median())
df = df.drop(columns='deck') # NaN 77%
df = df.dropna()

target = 'survived'
features = df.columns.drop(['survived', 'alive'])

X = df[features]
y = df[target]


pipe = make_pipeline(
    OrdinalEncoder(), 
    RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
)
pipe.fit(X, y);


encoder = pipe.named_steps['ordinalencoder']
X_encoded = encoder.fit_transform(X)
rf = pipe.named_steps['randomforestclassifier']


import matplotlib.pyplot as plt
from pdpbox import pdp
feature = 'sex'
pdp_dist = pdp.pdp_isolate(model=rf, dataset=X_encoded, model_features=features, feature=feature)
pdp.pdp_plot(pdp_dist, feature); # ì¸ì½”ë”©ëœ sex ê°’ í™•ì¸
```

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-27 13 20 25](https://user-images.githubusercontent.com/79494088/131071258-f9a99301-269e-4e00-acfd-6c53f3943889.png)

```py
# encoder ë§µí•‘ì„ í™•ì¸, {male:1, female:2}
encoder.mapping
'''
[{'col': 'sex',
  'mapping': male      1
  female    2
  NaN      -2
  dtype: int64,
  'data_type': dtype('O')},
 {'col': 'embarked',
  'mapping': S      1
  C      2
  Q      3
  NaN   -2
  dtype: int64,
  'data_type': dtype('O')},
 {'col': 'class',
  'mapping': Third     1
  First     2
  Second    3
  NaN      -2
  dtype: int64,
  'data_type': CategoricalDtype(categories=['First', 'Second', 'Third'], ordered=False)},
 {'col': 'who',
  'mapping': man      1
  woman    2
  child    3
  NaN     -2
  dtype: int64,
  'data_type': dtype('O')},
 {'col': 'embark_town',
  'mapping': Southampton    1
  Cherbourg      2
  Queenstown     3
  NaN           -2
  dtype: int64,
  'data_type': dtype('O')}]
'''


pdp.pdp_plot(pdp_dist, feature)

# xticks labels ì„¤ì •ì„ ì¸ì½”ë”©ëœ ì½”ë“œë¦¬ìŠ¤íŠ¸ì™€, ì¹´í…Œê³ ë¦¬ ê°’ ë¦¬ìŠ¤íŠ¸ë¥¼ ìˆ˜ë™ìœ¼ë¡œ
plt.xticks([1, 2], ['male', 'female',]);
```

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-27 13 22 50](https://user-images.githubusercontent.com/79494088/131071457-eb2335f7-8759-4ee4-a9d3-0907d2aeae86.png)

```py
# ì´ë²ˆì—ëŠ” PDP ì¹´í…Œê³ ë¦¬ê°’ ë§µí•‘ ìë™ìœ¼ë¡œ

feature = 'sex'
for item in encoder.mapping:
    if item['col'] == feature:
        feature_mapping = item['mapping'] # Series
        
feature_mapping = feature_mapping[feature_mapping.index.dropna()]
category_names = feature_mapping.index.tolist()
category_codes = feature_mapping.values.tolist()

pdp.pdp_plot(pdp_dist, feature)

# xticks labels ì„¤ì •ì„ ìœ„í•œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì§ì ‘ ë„£ì§€ ì•Šì•„ë„ ë¨
plt.xticks(category_codes, category_names);
```

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-27 13 23 48](https://user-images.githubusercontent.com/79494088/131071509-5c78f182-ab2b-4f2d-98e6-c7feca703698.png)

```py
# 2D PDP
features = ['sex', 'age']

interaction = pdp_interact(
    model=rf, 
    dataset=X_encoded, 
    model_features=X_encoded.columns, 
    features=features
)

pdp_interact_plot(interaction, plot_type='grid', feature_names=features);
```

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-27 13 24 14](https://user-images.githubusercontent.com/79494088/131071542-b30095e4-2000-46bd-91d0-192330885e43.png)

```py
# 2D PDP ë¥¼ Seaborn Heatmapìœ¼ë¡œ ê·¸ë¦¬ê¸° ìœ„í•´ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤
pdp = interaction.pdp.pivot_table(
    values='preds', 
    columns=features[0], 
    index=features[1]
)[::-1]

pdp = pdp.rename(columns=dict(zip(category_codes, category_names)))
plt.figure(figsize=(6,5))
sns.heatmap(pdp, annot=True, fmt='.2f', cmap='viridis')
plt.title('PDP decoded categorical');
```

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-27 13 24 41](https://user-images.githubusercontent.com/79494088/131071567-19dfc734-3305-4db9-a495-7a58913bcf80.png)

# 2ï¸âƒ£ SHAP
- ì–´ë–¤ ML Modelì´ë“  ë‹¨ì¼ ê´€ì¸¡ì¹˜ë¡œë¶€í„° íŠ¹ì„±ë“¤ì˜ ê¸°ì—¬ë„(Feature attribution)ë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•œ ë°©ë²•ì´ë‹¤.
- Shapley valuëŠ” ì›ë˜ ê²Œì„ì´ë¡ ì—ì„œ ë‚˜ì˜¨ ê°œë…ì´ì§€ë§Œ ë³µì¡í•œ ML modelì˜ ì˜ˆì¸¡ì„ ì„¤ëª…í•˜ê¸° ìœ„í•œ ìœ ìš©í•œ ë°©ë²•ì´ë‹¤.

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-27 13 27 05](https://user-images.githubusercontent.com/79494088/131071733-9863f983-0e95-4bfa-8b6b-f20430b07a6f.png)

```py
# íšŒê·€ ëª¨ë¸ ì˜ˆì‹œ
import numpy as np
import pandas as pd

# í‚¹ì¹´ìš´í‹° ì£¼íƒê°€ê²©
df = pd.read_csv('https://ds-lecture-data.s3.ap-northeast-2.amazonaws.com/kc_house_data/kc_house_data.csv')

# price, longitude, latitude ì–‘ ëë‹¨ ê°’ 1% ì œê±°
# Remove the most extreme 1% prices,
# the most extreme .1% latitudes, &
# the most extreme .1% longitudes
df = df[(df['price'] >= np.percentile(df['price'], 0.5)) & 
        (df['price'] <= np.percentile(df['price'], 99.5)) & 
        (df['long'] >= np.percentile(df['long'], 0.05)) & 
        (df['long'] <= np.percentile(df['long'], 99.95)) &
        (df['lat'] >= np.percentile(df['lat'], 0.05)) & 
        (df['lat'] < np.percentile(df['lat'], 99.95))]

# split train/test, 2015-03-01 ê¸°ì¤€ìœ¼ë¡œ ë‚˜ëˆ„ê¸°
df['date'] = pd.to_datetime(df['date'], infer_datetime_format=True)
cutoff = pd.to_datetime('2015-03-01')
train = df[df['date'] < cutoff]
test  = df[df['date'] >= cutoff]


train.shape, test.shape
'''
((16660, 21), (4691, 21))
'''


train.columns
'''
Index(['id', 'date', 'price', 'bedrooms', 'bathrooms', 'sqft_living',
       'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade',
       'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode',
       'lat', 'long', 'sqft_living15', 'sqft_lot15'],
      dtype='object')
'''


features = ['bedrooms', 'bathrooms', 'long', 'lat']
target = 'price'
X_train = train[features]
y_train = train[target]
X_test = test[features]
y_test = test[target]


from scipy.stats import randint, uniform
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import RandomizedSearchCV

param_distributions = { 
    'n_estimators': randint(50, 500), 
    'max_depth': [5, 10, 15, 20, None], 
    'max_features': uniform(0, 1), 
}

search = RandomizedSearchCV(
    RandomForestRegressor(random_state=2), 
    param_distributions=param_distributions, 
    n_iter=5, 
    cv=3, 
    scoring='neg_mean_absolute_error', 
    verbose=10, 
    return_train_score=True, 
    n_jobs=-1, 
    random_state=2
)

search.fit(X_train, y_train);
'''
Fitting 3 folds for each of 5 candidates, totalling 15 fits
'''


print('ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°: ', search.best_params_)
print('CV MAE: ', -search.best_score_)
model = search.best_estimator_
'''
ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°:  {'max_depth': 15, 'max_features': 0.6327377306009369, 'n_estimators': 166}
CV MAE:  101224.42224844794
'''
```

## Shapley values
- ê²Œì„ì´ë¡ ì—ì„œ ê°™ì€ íŒ€ ì„ ìˆ˜ë“¤(íŠ¹ì„±ë“¤)ì´ ê²Œì„ ëª©í‘œ(ì˜ˆì¸¡) ë‹¬ì„±ì„ ìœ„í•´ ê°ì ìì‹ ì˜ ì—­í• (ê¸°ì—¬)ì„ í•œë‹¤ê³  í•  ë•Œ ê²Œì„ ëª©í‘œ ë‹¬ì„± í›„ ë°›ì€ í¬ìƒì„ ì–´ë–»ê²Œ í•˜ë©´ ê·¸ë“¤ì˜ ê¸°ì—¬ë„ì— ë”°ë¼ ê³µí‰í•˜ê²Œ ë‚˜ëˆ„ì–´ ì¤„ ìˆ˜ ìˆì„ ê²ƒì¸ê°€? ë¼ëŠ” ì§ˆë¬¸ê³¼ ì—°ê´€ëœë‹¤.
- Shapley valueë¥¼ MLì˜ íŠ¹ì„± ê¸°ì—¬ë„ì‚°ì •ì— í™œìš©

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-27 13 30 08](https://user-images.githubusercontent.com/79494088/131071944-fbfde542-46b4-4863-a8a9-856a9af89a1f.png)

- íŠ¹ì„± ê°¯ìˆ˜ê°€ ë§ì•„ì§ˆ ìˆ˜ë¡ Shapley valueë¥¼ êµ¬í•  ë•Œ í•„ìš”í•œ ê³„ì‚°ëŸ‰ì´ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ëŠ˜ì–´ë‚œë‹¤.
- SHAPì—ì„œëŠ” ìƒ˜í”Œë§ì„ ì´ìš©í•´ ê·¼ì‚¬ì ìœ¼ë¡œ ê°’ì„ êµ¬í•©ë‹ˆë‹¤

```py
# Test set ë‘ë²ˆì§¸ sampleì˜ Shap value
row = X_test.iloc[[1]]  # ì¤‘ì²© bracketsì„ ì‚¬ìš©í•˜ë©´ ê²°ê³¼ë¬¼ì´ DataFrame
row
```

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-27 13 31 59](https://user-images.githubusercontent.com/79494088/131072084-ae9eccb4-ee7a-40bf-bc19-eee0f380bcd8.png)

```py
# ì‹¤ì œ ì§‘ê°’
y_test.iloc[[1]] # 2ë²ˆì§¸ ë°ì´í„°ë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤
'''
9    323000.0
Name: price, dtype: float64
'''


# ëª¨ë¸ ì˜ˆì¸¡ê°’
model.predict(row)
'''
array([341878.50142523])
'''


# ëª¨ë¸ì´ ì´ë ‡ê²Œ ì˜ˆì¸¡í•œ ì´ìœ ë¥¼ ì•Œê¸° ìœ„í•˜ì—¬ SHAP Force Plotì„ ê·¸ë¦°ë‹¤.
import shap

explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(row)

shap.initjs()
shap.force_plot(
    base_value=explainer.expected_value, 
    shap_values=shap_values,
    features=row
)
```

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-27 13 33 12](https://user-images.githubusercontent.com/79494088/131072183-0c76cf1e-a430-41af-beed-e9dbe00b5533.png)

```py
# ì§‘ ê°€ê²© í‰ê· ê°’ base value
explainer.expected_value[0]
'''
525264.9249674568
'''


# ì´ ê´€ì¸¡ì¹˜ì˜ ì˜ˆì¸¡ê°’ì´ ì™œ 341,878.50 ì´ ë‚˜ì˜¤ê²Œ ë˜ì—ˆëŠ”ì§€ ê° íŠ¹ì„±(bathrooms, lat, bedrooms)ì˜ ì˜í–¥ì„ ì‹œê°í™”

# ì˜ˆì¸¡í•¨ìˆ˜ ì •ì˜
def predict(bedrooms, bathrooms, longitude, latitude):

    # í•¨ìˆ˜ ë‚´ì—ì„œ ì˜ˆì¸¡ì— ì‚¬ìš©ë  input ìƒì„±
    df = pd.DataFrame(
        data=[[bedrooms, bathrooms, longitude, latitude]], 
        columns=['bedrooms', 'bathrooms', 'long', 'lat']
    )

    # ì˜ˆì¸¡
    pred = model.predict(df)[0]

    # Shap value ê³„ì‚°
    explainer = shap.TreeExplainer(model)
    shap_values = explainer.shap_values(df)

    # Shap value, íŠ¹ì„±ì´ë¦„, íŠ¹ì„±ê°’ì„ ê°€ì§€ëŠ” Series ìƒì„±
    feature_names = df.columns
    feature_values = df.values[0]
    shaps = pd.Series(shap_values[0], zip(feature_names, feature_values))

    # ê²°ê³¼ í”„ë¦°íŠ¸
    result = f'í‰ê· ê°€ê²©: ${explainer.expected_value[0]:,.0f} \n'
    result += f'ì˜ˆì¸¡ê°€ê²©: ${pred:,.0f}. \n'
    result += shaps.to_string()
    print(result)


    # SHAP Force Plot
    shap.initjs()
    return shap.force_plot(
        base_value=explainer.expected_value, 
        shap_values=shap_values, 
        features=df
    )


# lat ë¶„í¬
df['lat'].plot.hist(bins=100, figsize=(4, 2));
```

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-27 13 35 46](https://user-images.githubusercontent.com/79494088/131072350-98bdd432-8330-4b26-9323-1ce83ee0b964.png)

```py
# ì ë‹¹í•œ ì§€ì—­ì˜ ë°© 3ê°œì¸ ì§‘ ê°€ê²©
predict(3, 1, -121.35, 47.55)
'''
í‰ê· ê°€ê²©: $525,265 
ì˜ˆì¸¡ê°€ê²©: $382,123. 
(bedrooms, 3.0)     -25124.723574
(bathrooms, 1.0)   -142083.969321
(long, -121.35)     -21022.116137
(lat, 47.55)         45088.940133
'''
```

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-27 13 36 31](https://user-images.githubusercontent.com/79494088/131072410-7c965543-30ab-40a5-86d3-6482e8a0a56a.png)

```py
# ê°™ì€ ì§€ì—­ì— ë°© 2ê°œ ì§‘ ê°€ê²©ì„ ì˜ˆì¸¡í•´ ë³´ë©´, ì§€ì—­(lat) ìˆ˜ì¹˜ê°€ ê°™ìŒì—ë„ ì˜í–¥ì€ ë‹¬ë¼ì§
predict(2, 1, -122.35, 47.55)
'''
í‰ê· ê°€ê²©: $525,265 
ì˜ˆì¸¡ê°€ê²©: $281,714. 
(bedrooms, 2.0)     -45592.182391
(bathrooms, 1.0)   -118088.150603
(long, -122.35)     -62003.221763
(lat, 47.55)        -17867.074728
'''
```

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-27 13 37 15](https://user-images.githubusercontent.com/79494088/131072460-79fea275-9e06-4696-9941-af7fb64e9fd2.png)

```py
# ê°™ì€ ì§€ì—­ ë°© 1ê°œì¸ ì§‘ ê°€ê²©
predict(1, 1, -122.35, 47.55)
'''
í‰ê· ê°€ê²©: $525,265 
ì˜ˆì¸¡ê°€ê²©: $277,940. 
(bedrooms, 1.0)     -54303.671169
(bathrooms, 1.0)   -120596.652648
(long, -122.35)     -60103.932089
(lat, 47.55)        -12320.268028
'''
```

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-27 13 37 52](https://user-images.githubusercontent.com/79494088/131072497-3e1b371c-06cd-41ec-bc34-57023a787767.png)

```py
# SHAP plotìœ¼ë¡œ ê° íŠ¹ì„±ì´ ì–´ë–¤ ê°’ ë²”ìœ„ì—ì„œ ì˜í–¥ì„ ì£¼ëŠ”ì§€ í™•ì¸
# 100ê°œ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œì— ëŒ€í•´ì„œ ê° íŠ¹ì„±ë“¤ì˜ ì˜í–¥ í™•ì¸
# ìƒ˜í”Œ ìˆ˜ë¥¼ ë„ˆë¬´ í¬ê²Œ ì¢ìœ¼ë©´ ê³„ì‚°ì´ ì˜¤ë˜ê±¸ë¦¬ë‹ˆ ì£¼ì˜
shap_values = explainer.shap_values(X_test.iloc[:100])
shap.force_plot(explainer.expected_value, shap_values, X_test.iloc[:100])
```

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-27 09 57 06](https://user-images.githubusercontent.com/79494088/131072639-fe305913-c075-44ae-839a-ae06b5e88eaf.png)

```py
shap_values = explainer.shap_values(X_test.iloc[:300])
shap.summary_plot(shap_values, X_test.iloc[:300])
```

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-27 13 40 15](https://user-images.githubusercontent.com/79494088/131072693-1bf84242-0e3e-427b-a0db-9a1ca85fa4dd.png)

```py
shap.summary_plot(shap_values, X_test.iloc[:300], plot_type="violin")
```

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-27 13 40 27](https://user-images.githubusercontent.com/79494088/131072706-d598d2ea-ef7f-4967-87d6-8eb10c50357c.png)

```py
shap.summary_plot(shap_values, X_test.iloc[:300], plot_type="bar")
```

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-27 13 40 38](https://user-images.githubusercontent.com/79494088/131072722-049ec81a-f89d-4753-ad8e-0a03874d6e8a.png)

## SHAP value ë¶„ë¥˜ ë¬¸ì œì— ì ìš©

```py
# Lending Club ë°ì´í„° ì‚¬ìš©
# ëŒ€ì¶œ ìƒíƒœê°€ 'charged off'(ìƒê°) ì¸ì§€ 'fully paid'(ì™„ë‚©) ì¸ì§€ ì˜ˆì¸¡í•˜ëŠ” ë¬¸ì œ
import pandas as pd

# Kaggle ë°ì´í„°ì…‹ì—ì„œ 10% ìƒ˜í”Œë§ëœ ë°ì´í„°ì…ë‹ˆë‹¤.
## Source: https://www.kaggle.com/wordsforthewise/lending-club
## 10% of expired loans (loan_status: ['Fully Paid' and 'Charged Off'])
## grades A-D
## term ' 36 months'
df = pd.read_csv('https://ds-lecture-data.s3.ap-northeast-2.amazonaws.com/lending_club/lending_club_sampled.csv', index_col=0)


# 2-class íƒ€ê²Ÿ ('Fully Paid' or 'Charged Off')
target = 'loan_status'
X = df.drop(columns=target)
y = df[target]


# ë°ì´í„°ì…‹ ë¶„ë¦¬
from sklearn.model_selection import train_test_split

X_train_val, X_test, y_train_val, y_test = train_test_split(
    X, y, test_size=10000
    , stratify=y
    , random_state=2)

X_train, X_val, y_train, y_val = train_test_split(
    X_train_val, y_train_val, test_size=10000
    , stratify=y_train_val
    , random_state=42)

print('X_train shape', X_train.shape)
print('y_train shape', y_train.shape)
print('X_val shape', X_val.shape)
print('y_val shape', y_val.shape)
print('X_test shape', X_test.shape)
print('y_test shape', y_test.shape)
'''
X_train shape (77591, 150)
y_train shape (77591,)
X_val shape (10000, 150)
y_val shape (10000,)
X_test shape (10000, 150)
y_test shape (10000,)
'''


# test idsë¥¼ ì €ì¥í•˜ê³  SHAPë¶„ì„ì‹œ ì‚¬ìš©
test_id = X_test['id']


def wrangle(X):
    X = X.copy()

    # to datetime
    X['issue_d'] = pd.to_datetime(X['issue_d'], infer_datetime_format=True)
    
    # ê°œì„¤ ë‚ ì§œ - ìµœì´ˆ ì‹ ìš© ê°œì„¤
    X['earliest_cr_line'] = pd.to_datetime(X['earliest_cr_line'], infer_datetime_format=True)
    X['earliest_cr_line'] = X['issue_d'] - X['earliest_cr_line']
    X['earliest_cr_line'] = X['earliest_cr_line'].dt.days

    # Engineer issue_d_year
    X['issue_d_year'] = X['issue_d'].dt.year
    
    # Engineer issue_d_year
    X['issue_d_month'] = X['issue_d'].dt.month
            
    # non-digit ë¬¸ì ì¹˜í™˜ -> '', floatë³€í™˜
    X['emp_length'] = X['emp_length'].str.replace(r'\D','').astype(float)
        
    # Get length of free text fields
    X['title'] = X['title'].str.len()
    X['desc'] = X['desc'].str.len()
    X['emp_title'] = X['emp_title'].str.len()
    
    # sub_grade ìˆ«ì ì¹˜í™˜
    sub_grade_ranks = {'A1': 1.1, 'A2': 1.2, 'A3': 1.3, 'A4': 1.4, 'A5': 1.5, 
                       'B1': 2.1, 'B2': 2.2, 'B3': 2.3, 'B4': 2.4, 'B5': 2.5, 
                       'C1': 3.1, 'C2': 3.2, 'C3': 3.3, 'C4': 3.4, 'C5': 3.5, 
                       'D1': 4.1, 'D2': 4.2, 'D3': 4.3, 'D4': 4.4, 'D5': 4.5}
    X['sub_grade'] = X['sub_grade'].map(sub_grade_ranks)
    
    # í¬ê²Œ ì˜ë¯¸ ì—†ëŠ” íŠ¹ì„± ì‚­ì œ
    X = X.drop(columns='id')        # Always unique
    X = X.drop(columns='url')       # Always unique
    X = X.drop(columns='grade')     # Duplicative of sub_grade
    X = X.drop(columns='zip_code')  # High cardinality
    X = X.drop(columns='issue_d')   # date
    
    # drop null > 70%
    null_frac = X.isnull().mean().sort_values(ascending=False)
    X = X.drop(columns = sorted(list(null_frac[null_frac > 0.7].index)))
    
    # Keep list (https://www.kaggle.com/pileatedperch/predicting-charge-off-from-initial-listing-data)
    # ì ì¬ì ì¸ íˆ¬ììì—ê²Œë§Œ ì œê³µë˜ëŠ” íŠ¹ì„±ìœ¼ë¡œë§Œ ì œí•œí•©ë‹ˆë‹¤
    keep_list = keep_list = [
        'addr_state', 'annual_inc', 'application_type', 'dti', 'earliest_cr_line', 'emp_length'
        , 'emp_title', 'fico_range_high', 'fico_range_low', 'grade', 'home_ownership', 'id'
        , 'initial_list_status', 'installment', 'int_rate', 'issue_d', 'loan_amnt', 'loan_status'
        , 'mort_acc', 'open_acc', 'pub_rec', 'pub_rec_bankruptcies', 'purpose', 'revol_bal'
        , 'revol_util', 'sub_grade', 'term', 'title', 'total_acc', 'verification_status', 'zip_code']
    drop_list = [col for col in X.columns if col not in keep_list]
    X = X.drop(labels=drop_list, axis=1)
        
    # Reset index
    X = X.reset_index(drop=True)
    
    return X

X_train = wrangle(X_train)
X_val   = wrangle(X_val)
X_test  = wrangle(X_test)

print('X_train shape', X_train.shape)
print('X_val shape', X_val.shape)
print('X_test shape', X_test.shape)
'''
X_train shape (77591, 26)
X_val shape (10000, 26)
X_test shape (10000, 26)
'''


# í´ë˜ìŠ¤ì˜ ë¹„ìœ¨
y_train.value_counts(normalize=True)
'''
Fully Paid     0.847328
Charged Off    0.152672
Name: loan_status, dtype: float64
'''


ratio = 0.15/0.84
ratio
'''
0.17857142857142858
'''


from category_encoders import OrdinalEncoder
from sklearn.impute import SimpleImputer
from sklearn.pipeline import make_pipeline
from xgboost import XGBClassifier

processor = make_pipeline(
    OrdinalEncoder(), 
    SimpleImputer(strategy='median')
)

X_train_processed = processor.fit_transform(X_train)
X_val_processed = processor.transform(X_val)

eval_set = [(X_train_processed, y_train), 
            (X_val_processed, y_val)]

# XGBoost ë¶„ë¥˜ê¸°ë¥¼ í•™ìŠµ
# í´ë˜ìŠ¤ ë¹„ìœ¨ì„ ë§ì¶”ê¸° ìœ„í•´ scale_pos_weight= #C harged Off / # Fully Paid
model = XGBClassifier(n_estimators=1000, verbosity=0, n_jobs=-1, scale_pos_weight=ratio)
model.fit(X_train_processed, y_train, eval_set=eval_set, eval_metric='auc', 
          early_stopping_rounds=10)
'''
[0]	validation_0-auc:0.68092	validation_1-auc:0.66370
Multiple eval metrics have been passed: 'validation_1-auc' will be used for early stopping.

Will train until validation_1-auc hasn't improved in 10 rounds.
[1]	validation_0-auc:0.68988	validation_1-auc:0.67363
[2]	validation_0-auc:0.69617	validation_1-auc:0.67690
[3]	validation_0-auc:0.70074	validation_1-auc:0.67876
.
.
.
[35]	validation_0-auc:0.77165	validation_1-auc:0.68821
[36]	validation_0-auc:0.77249	validation_1-auc:0.68820
[37]	validation_0-auc:0.77412	validation_1-auc:0.68758
Stopping. Best iteration:
[27]	validation_0-auc:0.75906	validation_1-auc:0.68906

XGBClassifier(base_score=0.5, booster=None, colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
              importance_type='gain', interaction_constraints=None,
              learning_rate=0.300000012, max_delta_step=0, max_depth=6,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              n_estimators=1000, n_jobs=-1, num_parallel_tree=1, random_state=0,
              reg_alpha=0, reg_lambda=1, scale_pos_weight=0.17857142857142858,
              subsample=1, tree_method=None, validate_parameters=False,
              verbosity=0)
'''


from sklearn.metrics import roc_auc_score
X_test_processed = processor.transform(X_test)
X_val_processed = processor.transform(X_val)
class_index = 1
y_pred_proba = model.predict_proba(X_test_processed)[:, class_index]
print(f'Test AUC for class "{model.classes_[class_index]}":')
print(roc_auc_score(y_test, y_pred_proba)) # ë²”ìœ„ëŠ” 0-1, ìˆ˜ì¹˜ëŠ” ë†’ì„ ìˆ˜ë¡ ì¢‹ìŠµë‹ˆë‹¤
'''
Test AUC for class "Fully Paid":
0.6839267781606986
'''


# Confution matrixë¥¼ í™•ì¸í•´ ë´…ì‹œë‹¤
from sklearn.metrics import classification_report
y_test_pred = model.predict(X_test_processed)
print(classification_report(y_test, y_test_pred))
'''
              precision    recall  f1-score   support

 Charged Off       0.24      0.66      0.35      1527
  Fully Paid       0.91      0.61      0.73      8473

    accuracy                           0.62     10000
   macro avg       0.57      0.64      0.54     10000
weighted avg       0.81      0.62      0.67     10000
'''


# ì˜ˆì¸¡ê°’ ì‹¤ì œê°’ ë¹„êµ
df_p = pd.DataFrame({
    'id': test_id, 
    'pred_proba': y_pred_proba, # ì˜ˆì¸¡í™•ë¥  
    'status_group': y_test # ì‹¤ì œê°’
})

df_p = df_p.merge(
     df[['id','issue_d','sub_grade','total_pymnt','funded_amnt', 'term','int_rate']],
     how='left'
)


df_p.head()
```

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-27 13 45 59](https://user-images.githubusercontent.com/79494088/131073151-0beb3f56-2d75-4f47-be05-b73e4722bc37.png)

```py
fully_paid = df_p['status_group'] == 'Fully Paid'
charged_off = ~fully_paid
right = (fully_paid) == (df_p['pred_proba'] > 0.50)
wrong = ~right


# ëŒ€ì¶œì€ Fully Paid, ì˜ˆì¸¡ì´ ë§ëŠ” ê²½ìš°
df_p[fully_paid & right].sample(n=10, random_state=1).sort_values(by='pred_proba')
```

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-27 13 46 58](https://user-images.githubusercontent.com/79494088/131073260-c90ad962-44f5-44e7-893b-bcd730acc2db.png)

```py
# í…ŒìŠ¤íŠ¸ì…‹ì—ì„œ ì¸ë±ìŠ¤ 1 ìƒ˜í”Œì˜ ì˜ˆì¸¡
# ìš°ì„  ëª¨ë“  íŠ¹ì„± ìˆ˜ì¹˜ë¥¼ ë³¸ë‹¤
row = X_test.iloc[[3160]]
row
```

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-27 13 47 49](https://user-images.githubusercontent.com/79494088/131073316-22da930d-c564-4aee-bd07-4d1b0983326a.png)

```py
# SHAP ê·¸ë˜í”„ë¡œ ì˜ˆì¸¡ ì„¤ëª…
## UnicodeDecoderError ë°œìƒì‹œ xgboost 1.1-> 1.0 ë‹¤ìš´ê·¸ë ˆì´ë“œ (conda install -c conda-forge xgboost=1.0)
import xgboost
import shap

explainer = shap.TreeExplainer(model)
row_processed = processor.transform(row)
shap_values = explainer.shap_values(row_processed)

shap.initjs()
shap.force_plot(
    base_value=explainer.expected_value, 
    shap_values=shap_values, 
    features=row, 
    link='logit' # SHAP valueë¥¼ í™•ë¥ ë¡œ ë³€í™˜
)
```

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-27 13 48 39](https://user-images.githubusercontent.com/79494088/131073371-3960ded4-e41c-4271-8ace-47285d0cb47e.png)

```py
# ì˜ˆì¸¡ì„ SHAP ê·¸ë˜í”„ë¥¼ í†µí•´ ì„¤ëª…í•˜ëŠ” í•¨ìˆ˜
feature_names = row.columns
feature_values = row.values[0]
shaps = pd.Series(shap_values[0], zip(feature_names, feature_values))


pros = shaps.sort_values(ascending=False)[:3].index
cons = shaps.sort_values(ascending=True)[:3].index


print('fully paid ì˜ˆì¸¡ì— ëŒ€í•œ Positive ìš”ì¸ Top 3 ì…ë‹ˆë‹¤:')
for i, pro in enumerate(pros, start=1):
    feature_name, feature_value = pro
    print(f'{i}. {feature_name} : {feature_value}')

print('\n')
print('Negative ìš”ì¸ Top 3 ì…ë‹ˆë‹¤:')
for i, con in enumerate(cons, start=1):
    feature_name, feature_value = con
    print(f'{i}. {feature_name} : {feature_value}')
'''
fully paid ì˜ˆì¸¡ì— ëŒ€í•œ Positive ìš”ì¸ Top 3 ì…ë‹ˆë‹¤:
1. sub_grade : 1.1
2. int_rate : 5.42
3. fico_range_low : 745.0


Negative ìš”ì¸ Top 3 ì…ë‹ˆë‹¤:
1. open_acc : 16.0
2. purpose : major_purchase
3. addr_state : PA
'''


def explain(row_number):
    positive_class = 'Fully Paid'
    positive_class_index = 1

    # row ê°’ì„ ë³€í™˜í•©ë‹ˆë‹¤
    row = X_test.iloc[[row_number]]
    row_processed = processor.transform(row)

    # ì˜ˆì¸¡í•˜ê³  ì˜ˆì¸¡í™•ë¥ ì„ ì–»ìŠµë‹ˆë‹¤ 
    pred = model.predict(row_processed)[0]
    pred_proba = model.predict_proba(row_processed)[0, positive_class_index]
    pred_proba *= 100
    if pred != positive_class:
        pred_proba = 100 - pred_proba

    # ì˜ˆì¸¡ê²°ê³¼ì™€ í™•ë¥ ê°’ì„ ì–»ìŠµë‹ˆë‹¤
    print(f'ì´ ëŒ€ì¶œì— ëŒ€í•œ ì˜ˆì¸¡ê²°ê³¼ëŠ” {pred} ìœ¼ë¡œ, í™•ë¥ ì€ {pred_proba:.0f}% ì…ë‹ˆë‹¤.')
    
    # SHAPë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤
    shap_values = explainer.shap_values(row_processed)

    # Fully Paidì— ëŒ€í•œ top 3 pros, consë¥¼ ì–»ìŠµë‹ˆë‹¤
    feature_names = row.columns
    feature_values = row.values[0]
    shaps = pd.Series(shap_values[0], zip(feature_names, feature_values))
    pros = shaps.sort_values(ascending=False)[:3].index
    cons = shaps.sort_values(ascending=True)[:3].index

    # ì˜ˆì¸¡ì— ê°€ì¥ ì˜í–¥ì„ ì¤€ top3
    print('\n')
    print('Positive ì˜í–¥ì„ ê°€ì¥ ë§ì´ ì£¼ëŠ” 3ê°€ì§€ ìš”ì¸ ì…ë‹ˆë‹¤:')
    
    evidence = pros if pred == positive_class else cons
    for i, info in enumerate(evidence, start=1):
        feature_name, feature_value = info
        print(f'{i}. {feature_name} : {feature_value}')

    # ì˜ˆì¸¡ì— ê°€ì¥ ë°˜ëŒ€ì ì¸ ì˜í–¥ì„ ì¤€ ìš”ì¸ top1
    print('\n')
    print('Negative ì˜í–¥ì„ ê°€ì¥ ë§ì´ ì£¼ëŠ” 3ê°€ì§€ ìš”ì¸ ì…ë‹ˆë‹¤:')
    
    evidence = cons if pred == positive_class else pros
    for i, info in enumerate(evidence, start=1):
        feature_name, feature_value = info
        print(f'{i}. {feature_name} : {feature_value}')

    # SHAP
    shap.initjs()
    return shap.force_plot(
        base_value=explainer.expected_value, 
        shap_values=shap_values, 
        features=row, 
        link='logit'
    )


explain(3160)
'''
ì´ ëŒ€ì¶œì— ëŒ€í•œ ì˜ˆì¸¡ê²°ê³¼ëŠ” Fully Paid ìœ¼ë¡œ, í™•ë¥ ì€ 90% ì…ë‹ˆë‹¤.


Positive ì˜í–¥ì„ ê°€ì¥ ë§ì´ ì£¼ëŠ” 3ê°€ì§€ ìš”ì¸ ì…ë‹ˆë‹¤:
1. sub_grade : 1.1
2. int_rate : 5.42
3. fico_range_low : 745.0


Negative ì˜í–¥ì„ ê°€ì¥ ë§ì´ ì£¼ëŠ” 3ê°€ì§€ ìš”ì¸ ì…ë‹ˆë‹¤:
1. open_acc : 16.0
2. purpose : major_purchase
3. addr_state : PA
'''
```

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-27 13 50 15](https://user-images.githubusercontent.com/79494088/131073510-c7cb8167-9ea9-4d11-97c3-e1d71a0106c4.png)

```py
# ëŒ€ì¶œ ê²°ê³¼ëŠ” Fully Paid, ì˜ˆì¸¡ì´ ì˜ëª»ëœ ê²½ìš°
df_p[fully_paid & wrong].sample(n=10, random_state=1).sort_values(by='pred_proba')
```

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-27 13 51 12](https://user-images.githubusercontent.com/79494088/131073597-1bb9fff0-759f-4ec0-a6ab-3de3e297cc2e.png)

```py
explain(3559)
'''
ì´ ëŒ€ì¶œì— ëŒ€í•œ ì˜ˆì¸¡ê²°ê³¼ëŠ” Charged Off ìœ¼ë¡œ, í™•ë¥ ì€ 83% ì…ë‹ˆë‹¤.


Positive ì˜í–¥ì„ ê°€ì¥ ë§ì´ ì£¼ëŠ” 3ê°€ì§€ ìš”ì¸ ì…ë‹ˆë‹¤:
1. sub_grade : 4.4
2. installment : 1300.55
3. int_rate : 19.99


Negative ì˜í–¥ì„ ê°€ì¥ ë§ì´ ì£¼ëŠ” 3ê°€ì§€ ìš”ì¸ ì…ë‹ˆë‹¤:
1. home_ownership : MORTGAGE
2. emp_length : 10.0
3. revol_bal : 21434.0
'''
```

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-27 13 51 41](https://user-images.githubusercontent.com/79494088/131073637-75dac146-c353-4abe-80f7-24bfb69a92d2.png)

```py
# ëŒ€ì¶œ ê²°ê³¼ëŠ” Charged Off, ì˜ˆì¸¡ì´ ì˜ëª»ëœ ê²½ìš°
df_p[charged_off & wrong].sample(n=10, random_state=1).sort_values(by='pred_proba')
```

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-27 13 52 20](https://user-images.githubusercontent.com/79494088/131073691-cb5dd81b-c01d-494a-9194-a9bc91aecc71.png)

```py
explain(7896)
'''
ì´ ëŒ€ì¶œì— ëŒ€í•œ ì˜ˆì¸¡ê²°ê³¼ëŠ” Fully Paid ìœ¼ë¡œ, í™•ë¥ ì€ 91% ì…ë‹ˆë‹¤.


Positive ì˜í–¥ì„ ê°€ì¥ ë§ì´ ì£¼ëŠ” 3ê°€ì§€ ìš”ì¸ ì…ë‹ˆë‹¤:
1. sub_grade : 1.3
2. int_rate : 6.89
3. dti : 5.72


Negative ì˜í–¥ì„ ê°€ì¥ ë§ì´ ì£¼ëŠ” 3ê°€ì§€ ìš”ì¸ ì…ë‹ˆë‹¤:
1. annual_inc : 31000.0
2. emp_length : 3.0
3. earliest_cr_line : 8705
'''
```

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-08-27 13 52 57](https://user-images.githubusercontent.com/79494088/131073752-0d39a3bd-b5fd-4492-bca0-ed49947b5c8e.png)

### Feature Importances, PDP, SHAPì˜ íŠ¹ì§• êµ¬ë¶„

- ì„œë¡œ ê´€ë ¨ì´ ìˆëŠ” ëª¨ë“  íŠ¹ì„±ë“¤ì— ëŒ€í•œ ì „ì—­ì ì¸(Global) ì„¤ëª…
  - Feature Importances
  - Drop-Column Importances
  - Permutaton Importances
- íƒ€ê²Ÿê³¼ ê´€ë ¨ì´ ìˆëŠ” ê°œë³„ íŠ¹ì„±ë“¤ì— ëŒ€í•œ ì „ì—­ì ì¸ ì„¤ëª…
  - Partial Dependence plots
  - ê°œë³„ ê´€ì¸¡ì¹˜ì— ëŒ€í•œ ì§€ì—­ì ì¸(local) ì„¤ëª…
- Shapley Values